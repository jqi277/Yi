# fastapi_app.py  (runtime v3.9.0, analysis logic v3.9.0Â·æ˜“ç»æ¨å¯¼ç‰ˆ)
# 3.9.0ï¼šæ˜“ç»â€œæ¨å¯¼ç‰ˆâ€
# - ä¸‰åˆè±¡ï¼šä¸»/è¾…/åŸº + äº”è¡Œç”Ÿå…‹/åŒæ°”/ç›¸å¹¶ â†’ åªåšâ€œäººç‰©ç”»åƒâ€ï¼Œä¸å¤¹å¸¦å»ºè®®
# - äº‹ä¸š/æ„Ÿæƒ…ï¼šæŒ‰å¦è±¡çŸ¥è¯†åº“æ¨å¯¼â€œè¿‘æœŸçŠ¶æ€/è¿‘æœŸå»ºè®®â€ï¼›ä¸»=é£æ ¼ï¼Œè¾…=å¤–éƒ¨ï¼Œäºº=åŸºç¡€
# - å»ºç«‹å¦è±¡çŸ¥è¯†åº“ï¼ˆæ¯å¦ï¼šäººæ ¼å…³é”®è¯ã€äº‹ä¸šçŠ¶æ€/å»ºè®®ã€æ„Ÿæƒ…çŠ¶æ€/å»ºè®®ã€äº”è¡Œï¼‰
# - å»æ¨¡æ¿åŒ–ï¼šæ‰€æœ‰æ–‡æœ¬ä»å¦ä¹‰+ç”Ÿå…‹æ¨å¯¼ç”Ÿæˆï¼Œè€Œéé€šç”¨å£å·
import os, base64, json, logging, traceback, re
from typing import Dict, Any, List, Tuple

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse, HTMLResponse, Response
from fastapi.middleware.cors import CORSMiddleware

from openai import OpenAI

RUNTIME_VERSION = "3.9.0"
ANALYSIS_VERSION = os.getenv("ANALYSIS_VERSION", "390").strip()
SCHEMA_ID = "selfy.v3"
DEBUG = str(os.getenv("DEBUG","0")).strip() in ("1","true","True","YES","yes")

logging.basicConfig(level=logging.DEBUG if DEBUG else logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger("selfy-ai")

app = FastAPI(title="Selfy AI - YiJing Analysis API (æ¨å¯¼ç‰ˆ)", version=RUNTIME_VERSION)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

try:
    client = OpenAI()
except Exception as e:
    logger.error("OpenAI client init failed: %s", e); client=None

# ---- å¸¸é‡ä¸å·¥å…· ----
BAGUA_SYMBOLS = {"è‰®":"å±±","ç¦»":"ç«","å…‘":"æ³½","ä¹¾":"å¤©","å¤":"åœ°","éœ‡":"é›·","å·½":"é£","å":"æ°´"}

def _to_data_url(content: bytes, content_type: str) -> str:
    return f"data:{content_type};base64,{base64.b64encode(content).decode('utf-8')}"

def _build_tools_schema() -> List[Dict[str, Any]]:
    return [{
      "type":"function",
      "function":{
        "name":"submit_analysis_v3",
        "description":"Return end-user facing JSON for Selfy AI YiJing analysis.",
        "parameters":{
          "type":"object",
          "properties":{
            "summary":{"type":"string"},
            "archetype":{"type":"string"},
            "confidence":{"type":"number"},
            "sections":{"type":"object","properties":{"å§¿æ€":{"type":"string"},"ç¥æƒ…":{"type":"string"},"é¢ç›¸":{"type":"string"}},"required":["å§¿æ€","ç¥æƒ…","é¢ç›¸"],"additionalProperties":False},
            "domains":{"type":"array","items":{"type":"string"}},
            "meta":{"type":"object","additionalProperties":True}
          },
          "required":["summary","archetype","confidence","sections","domains"],
          "additionalProperties":False
        }
      }
    }]

def _json_hint() -> str:
    return ("åªä»¥ JSON object è¿”å›ï¼ˆå¿…é¡» JSONï¼‰ã€‚ç¤ºä¾‹:{\"summary\":\"â€¦\",\"archetype\":\"â€¦\",\"confidence\":0.9,"
            "\"sections\":{\"å§¿æ€\":\"â€¦\",\"ç¥æƒ…\":\"â€¦\",\"é¢ç›¸\":\"â€¦\"},"
            "\"domains\":[\"é‡‘é’±ä¸äº‹ä¸š\",\"é…å¶ä¸æ„Ÿæƒ…\"],"
            "\"meta\":{\"triple_analysis\":{\"å§¿æ€\":{\"è¯´æ˜\":\"â€¦\",\"å¦è±¡\":\"è‰®\",\"è§£è¯»\":\"â€¦\",\"æ€§æ ¼å€¾å‘\":\"â€¦\"},\"ç¥æƒ…\":{â€¦},\"é¢å®¹\":{â€¦},\"ç»„åˆæ„å¢ƒ\":\"â€¦\",\"æ€»ç»“\":\"â€¦\"},"
            "\"face_parts\":{\"çœ‰\":{\"ç‰¹å¾\":\"â€¦\",\"å¦è±¡\":\"â€¦\",\"è§£è¯»\":\"â€¦\"},\"çœ¼\":{â€¦},\"é¼»\":{â€¦},\"å˜´\":{â€¦},\"é¢§/ä¸‹å·´\":{â€¦}}}}")

def _prompt_for_image_v390():
    sys = (
      "ä½ æ˜¯ Selfy AI çš„æ˜“ç»è§‚ç›¸åŠ©æ‰‹ï¼ˆv3.9.0 æ¨å¯¼åŸºåº•ï¼‰ã€‚"
      "ä¸¥æ ¼æŒ‰â€œä¸‰è±¡å››æ®µå¼â€åˆ†æï¼šã€å§¿æ€/ç¥æƒ…/é¢å®¹ã€‘ä¸‰éƒ¨åˆ†ã€‚æ¯éƒ¨åˆ†å¿…é¡»åŒ…å«ï¼š"
      "1) è¯´æ˜ï¼š1å¥ï¼Œå®¢è§‚æç»˜å¤–è§‚/åŠ¨ä½œ/æ°”è´¨ï¼›"
      "2) å¦è±¡ï¼šä»…å†™ä¸€ä¸ªå¦åï¼ˆè‰®/ç¦»/å…‘/ä¹¾/å¤/éœ‡/å·½/åï¼‰ï¼›"
      "3) è§£è¯»ï¼š1â€“2å¥ï¼ŒåŸºäºå¦è±¡ä¸è§‚å¯Ÿåšå«ä¹‰é˜é‡Šï¼›"
      "4) æ€§æ ¼å€¾å‘ï¼š1â€“2å¥ï¼Œç‹¬ç«‹æˆæ®µï¼Œä¸è¦ä¸â€œè§£è¯»â€é‡å¤æªè¾ã€‚"
      "ç„¶åç»™å‡ºç®€è¦æ€»ç»“ã€äººè®¾æ ‡ç­¾ï¼Œå¹¶åœ¨ meta.face_parts ä¸­è¡¥å……äº”å®˜ç»†èŠ‚ï¼ˆçœ‰/çœ¼/é¼»/å˜´/é¢§/ä¸‹å·´ä»»äº”é¡¹ï¼‰ã€‚"
      "domains ä»…ä» ['é‡‘é’±ä¸äº‹ä¸š','é…å¶ä¸æ„Ÿæƒ…'] é€‰æ‹©ï¼Œmeta.domains_detail ä¸­å¯ç»™å‡ºå„ 60â€“90 å­—æ–‡æœ¬ï¼ˆåç«¯ä¼šå†åšæ¨å¯¼èåˆï¼‰ã€‚"
      "å°†ç»“æœé€šè¿‡ submit_analysis_v3 å·¥å…·è¿”å›ï¼Œå¹¶"+_json_hint()+"ã€‚è¯­è¨€ï¼šä¸­æ–‡ã€‚"
    )
    user = "è¯·æŒ‰è§„èŒƒåˆ†æå›¾ç‰‡ï¼Œä¸¥æ ¼é€šè¿‡å‡½æ•°è¿”å› JSONï¼ˆä¸è¦è¾“å‡ºè‡ªç”±æ–‡æœ¬ï¼‰ã€‚"
    return [{"role":"system","content":sys},{"role":"user","content":user}]

# ---- æ–‡æœ¬æ¸…ç† ----
DOMAIN_LEADS = r"(åœ¨(é‡‘é’±ä¸äº‹ä¸š|é…å¶ä¸æ„Ÿæƒ…|äº‹ä¸š|æ„Ÿæƒ…)(æ–¹é¢|ä¸­|é‡Œ)?|ç›®å‰|è¿‘æœŸ|å½“ä¸‹)"
_STOPWORDS = r"(å§¿æ€|ç¥æƒ…|é¢å®¹|æ•´ä½“|æ°”è´¨|å½¢è±¡|ç»™äººä»¥|ä¸€ç§|ä»¥åŠ|å¹¶ä¸”|è€Œä¸”|æ›´æ˜¾|æ˜¾å¾—|å±•ç°å‡º|æµéœ²å‡º|é€éœ²å‡º)"

def _depronoun(s: str) -> str:
    if not isinstance(s, str): return s
    s = s.strip()
    s = re.sub(r"^(ä»–|å¥¹|TA|ä½ |å¯¹æ–¹|å…¶)(çš„)?[ï¼Œã€ï¼š ]*", "", s)
    s = re.sub(r"^(åœ¨(äº‹ä¸š|æ„Ÿæƒ…|ç”Ÿæ´»)[ä¸Šä¸­]|ç›®å‰|è¿‘æœŸ)[ï¼Œã€ï¼š ]*", "", s)
    return s

def _neutralize(s: str) -> str:
    if not isinstance(s, str): return s
    s = s.strip()
    s = re.sub(r"(ä»–|å¥¹|TA|å¯¹æ–¹|å…¶)(çš„)?", "", s)
    s = re.sub(DOMAIN_LEADS + r"[ï¼Œã€ï¼š ]*", "", s)
    s = re.sub(r"(å¯èƒ½|æˆ–è®¸|ä¹Ÿè®¸)[ï¼Œã€ ]*", "", s)
    s = re.sub(r"[ï¼›;]+", "ï¼›", s)
    s = re.sub(r"[ï¼Œ,]{2,}", "ï¼Œ", s)
    return s.strip("ï¼›ï¼Œã€‚ ")

def _canon_key(s: str) -> str:
    if not isinstance(s, str): return ""
    k = re.sub(_STOPWORDS, "", s)
    k = re.sub(r"[çš„åœ°å¾—]", "", k)
    k = re.sub(r"\s+", "", k)
    return k

def _dedupe_smart(s: str) -> str:
    if not isinstance(s, str): return s
    s = s.strip("ã€‚ï¼›ï¼Œ,; ")
    sentences = re.split(r"[ã€‚ï¼ï¼Ÿ]", s)
    clean_sentences = []
    for sen in sentences:
        sen = sen.strip("ï¼Œ,;ï¼› ")
        if not sen: continue
        parts = re.split(r"[ï¼Œ,ï¼›;]", sen)
        seen_keys, kept = set(), []
        for p in parts:
            t = p.strip()
            if not t: continue
            ck = _canon_key(t)
            if ck and ck not in seen_keys:
                seen_keys.add(ck); kept.append(t)
        clean_sentences.append("ï¼Œ".join(kept))
    return "ã€‚".join(clean_sentences) + ("ã€‚" if clean_sentences else "")

def _strip_domain_lead(s: str) -> str:
    if not isinstance(s, str): return s
    s = re.sub("^" + DOMAIN_LEADS + r"[ï¼Œã€ï¼š ]*", "", s.strip())
    s = re.sub(r"^ä¸Š[ï¼Œã€ï¼š ]*", "", s)  # è£æ‰â€œä¸Šï¼Œâ€ä¹‹ç±»æ®‹ç•™
    return s

# ---- æ˜“ç»çŸ¥è¯†åº“ ----
HEX_SUMMARY = {  # ç”¨äºè½»é‡æç¤º
    "ä¹¾":"è‡ªä¿¡Â·ä¸»å¯¼Â·æœæ–­",
    "å¤":"åŒ…å®¹Â·ç¨³å®šÂ·æ‰¿è½½",
    "éœ‡":"è¡ŒåŠ¨Â·çªç ´Â·èµ·åŠ¿",
    "å·½":"åè°ƒÂ·æ¸—é€Â·è¯´æœ",
    "å":"è°¨æ…Â·æ¢æ·±Â·æ±‚è¯",
    "ç¦»":"æ¸…æ™°Â·è¡¨è¾¾Â·æ´å¯Ÿ",
    "è‰®":"æ­¢å®šÂ·è¾¹ç•ŒÂ·ç¨³å®ˆ",
    "å…‘":"äº²å’ŒÂ·äº¤æµÂ·æ‚¦äºº"
}

WUXING = {
    "ä¹¾":{"element":"é‡‘","polarity":"é˜³","virtue":"åˆšå¥è‡ªå¼ºã€å¾¡é¢†å±€é¢"},
    "å…‘":{"element":"é‡‘","polarity":"é˜´","virtue":"å’Œæ‚¦äº²å’Œã€ä»¥ä¹æ„Ÿäºº"},
    "ç¦»":{"element":"ç«","polarity":"é˜´","virtue":"æ˜è¾¨æ´å¯Ÿã€æ“…äºè¡¨è¾¾"},
    "éœ‡":{"element":"æœ¨","polarity":"é˜³","virtue":"å‘åŠ¨èµ·åŠ¿ã€æ•¢äºçªç ´"},
    "å·½":{"element":"æœ¨","polarity":"é˜´","virtue":"æ¸—é€åè°ƒã€å–„è°‹åˆä¼—"},
    "å":{"element":"æ°´","polarity":"é˜³","virtue":"å®¡æ…æ¢æ·±ã€å±…å®‰è¯†å±"},
    "è‰®":{"element":"åœŸ","polarity":"é˜³","virtue":"æ­¢å®šæœ‰åº¦ã€å®ˆæ­£ç«‹ç•Œ"},
    "å¤":{"element":"åœŸ","polarity":"é˜´","virtue":"åšå¾·è½½ç‰©ã€å†…æ•›æ‰¿è½½"}
}

SHENG = {"æœ¨":"ç«","ç«":"åœŸ","åœŸ":"é‡‘","é‡‘":"æ°´","æ°´":"æœ¨"}
KE    = {"æœ¨":"åœŸ","åœŸ":"æ°´","æ°´":"ç«","ç«":"é‡‘","é‡‘":"æœ¨"}

# æ¯å¦åœ¨äº‹ä¸š/æ„Ÿæƒ…ä¸­çš„â€œçŠ¶æ€/å»ºè®®â€ï¼ˆåŸºäºè±¡è¾æ„æ¶µï¼Œç®€åŒ–ä¸ºå¯è½åœ°çŸ­å¥ï¼‰
HEX_DOMAIN_KB: Dict[str, Dict[str, Dict[str,str]]] = {
    "ä¹¾":{
        "persona":"åˆšå¥è‡ªå¼ºã€ä¸»å¯¼æ¡†æ¶",
        "career":{"state":"è¡Œäº‹æœæ–­ï¼Œç›®æ ‡å¯¼å‘ï¼Œå–„äºå®šæ–¹å‘ä¸æ ‡å‡†","advice":"å–è±¡â€œåˆ©è§å¤§äººâ€ï¼šå‘ä¸Šè¿çº¿ã€å€ŸåŠ¿èµ„æºï¼›åˆ†è§£ç›®æ ‡ï¼ŒèŠ‚å¥ç¨³å¥æ¨è¿›"},
        "love":{"state":"è¡¨è¾¾ç›´æ¥ï¼Œé‡æ‰¿è¯ºä¸ä¿æŠ¤æ„Ÿ","advice":"å–è±¡â€œå¤©è¡Œå¥â€ï¼šä»¥è¯šç›¸å¾…ï¼Œå…ˆå®šè¾¹ç•Œä¸èŠ‚å¥ï¼Œå†æ·±å…¥å…³ç³»"}
    },
    "å¤":{
        "persona":"åšå¾·è½½ç‰©ã€ç¨³å®šæ‰¿è½½",
        "career":{"state":"ç¨³æ‰ç¨³æ‰“ï¼Œé‡æµç¨‹ä¸é…åˆï¼Œèƒ½è½åœ°æ‰§è¡Œ","advice":"å–è±¡â€œåšå¾·è½½ç‰©â€ï¼šæŒ‰éƒ¨å°±ç­ï¼Œå…ˆç¨³åå¹¿ï¼›æŠŠè§„åˆ™è½æˆæ˜ç¡®çš„æ­¥éª¤"},
        "love":{"state":"åŒ…å®¹è€å¿ƒï¼Œé‡é™ªä¼´ä¸ä¿¡ä»»","advice":"ä»¥â€œåœ°åŠ¿å¤â€ä¸ºèŒƒï¼šå¤šå€¾å¬å°‘åˆ¤æ–­ï¼Œæ‰¿æ¥å¯¹æ–¹éœ€æ±‚ï¼Œç¨³å®šå…³ç³»åº•ç›˜"}
    },
    "éœ‡":{
        "persona":"èµ·åŠ¿çªç ´ã€æ•¢ä¸ºå…ˆ",
        "career":{"state":"è¡ŒåŠ¨åŠ›å¼ºï¼Œå–„äºå¼€å¯é¡¹ç›®å¹¶å¸¦åŠ¨èŠ‚å¥","advice":"å–è±¡â€œé›·åŠ¨ä¸‡ç‰©â€ï¼šå¿«é€Ÿè¯•é”™ï¼Œå°æ­¥å¿«è·‘ï¼›ä»¥è¡ŒåŠ¨æ‹‰åŠ¨èµ„æºèšé›†"},
        "love":{"state":"å…³é”®å¤„èƒ½ä¸»åŠ¨é è¿‘ï¼Œå¸¦åŠ¨äº’åŠ¨æ°›å›´","advice":"ä»¥â€œåŠ¨â€åŒ–â€œé™â€ï¼šåˆ¶é€ æ­£å‘äº’åŠ¨ï¼Œç”¨çœŸå®è¡ŒåŠ¨è¡¨è¾¾åœ¨æ„"}
    },
    "å·½":{
        "persona":"æ¸—é€åè°ƒã€æŸ”ä¸­æœ‰æ®",
        "career":{"state":"å–„ååŒä¸è¯´æœï¼Œèƒ½æŠŠäººæ‹‰åˆ°åŒä¸€è½¨é“","advice":"å–è±¡â€œå…¥è€Œä¸äº‰â€ï¼šå…ˆèåé¢†ï¼Œå˜æ¸…åˆ©å®³ï¼Œè®©å…±è¯†è‡ªç„¶å½¢æˆ"},
        "love":{"state":"ä½“è´´åˆ†å¯¸æ„Ÿå¼ºï¼Œæ“…åŒ–è§£å°çŸ›ç›¾","advice":"ä»¥â€œå’Œâ€ä¸ºä¸»ï¼šå¤šç¡®è®¤ã€å¤šå¤è¿°ï¼Œé™ä½è¯¯è§£ï¼Œç¨³ä¸­åŠ æ·±"}
    },
    "ç¦»":{
        "persona":"æ˜æ™°è¡¨è¾¾ã€æ´å¯Ÿåˆ†æ˜",
        "career":{"state":"ä¿¡æ¯é€æ˜ï¼Œé€»è¾‘æ¸…æ¥šï¼Œæ“…æ€»ç»“ä¸å‘ˆç°","advice":"å–è±¡â€œæ—¥ä¸­è§æ˜â€ï¼šå…ˆè®²æ¸…ç¼˜ç”±ä¸æ ‡å‡†ï¼Œå†è¿›å…¥æ‰§è¡Œä¸éªŒæ”¶"},
        "love":{"state":"æ²Ÿé€šç›´ç™½ï¼Œé‡æƒ…ç»ªè¡¨è¾¾ä¸ä»ªå¼æ„Ÿ","advice":"ä¿æŒâ€œæ˜è€Œä¸ç¼â€ï¼šå¦è¯šä½†ä¸é€¼è¿«ï¼Œç»™å¯¹æ–¹æ¶ˆåŒ–ç©ºé—´"}
    },
    "å":{
        "persona":"å±…å®‰è¯†å±ã€å…ˆè¯åè¡Œ",
        "career":{"state":"å®¡æ…å‘¨å¯†ï¼Œå–„é£é™©è¯†åˆ«ä¸é¢„æ¡ˆ","advice":"å–è±¡â€œä¹ åä¸ºé™©â€ï¼šå…ˆæ ¸å¯¹å…³é”®æ•°æ®ä¸å‡è®¾ï¼Œç•™å‡ºA/Bæ–¹æ¡ˆ"},
        "love":{"state":"åœ¨æ„å®‰å…¨æ„Ÿï¼Œæƒ…ç»ªèµ·ä¼éœ€è¢«çœ‹è§","advice":"ä»¥â€œå®è¯â€æ¢â€œå®‰å¿ƒâ€ï¼šå°‘çŒœå¤šé—®ï¼Œå»ºç«‹ç¨³å®šçš„ç¡®è®¤æœºåˆ¶"}
    },
    "è‰®":{
        "persona":"æ­¢å½“å…¶æ—¶ã€è¾¹ç•Œåˆ†æ˜",
        "career":{"state":"èƒ½æ§èŠ‚å¥ï¼Œå®ˆè¾¹ç•Œï¼Œæ¨è¿›æœ‰ç« æ³•","advice":"å–è±¡â€œè‰®å…¶èƒŒâ€ï¼šå…ˆå®šèŒƒå›´ä¸ä¼˜å…ˆçº§ï¼Œå†é€æ­¥æ‰©å±•"},
        "love":{"state":"å°Šé‡è¾¹ç•Œï¼Œå…³ç³»æ¨è¿›å¾ªåºæ¸è¿›","advice":"ç¨³æ­¥é è¿‘ï¼šç»™å½¼æ­¤ç‹¬å¤„ä¸å‘¼å¸æ„Ÿï¼ŒèŠ‚å¥ç•¥æ…¢åæ›´ç¨³"}
    },
    "å…‘":{
        "persona":"ä»¥æ‚¦ä¸ºå’Œã€äº²å’Œæ²Ÿé€š",
        "career":{"state":"äº²å’ŒåŠ›å¼ºï¼Œå–„æ²Ÿé€šä¿ƒæˆä¸å®¢æˆ·å…³ç³»","advice":"å–è±¡â€œè¯´ä»¥æˆäº‹â€ï¼šæŠŠåˆ©ç›Šç‚¹è®²æ¸…ï¼Œå…ˆæ˜“åéš¾ï¼Œä¿ƒæˆåœ¨å…±èµ¢"},
        "love":{"state":"è¡¨è¾¾æ„Ÿå—è‡ªç„¶ï¼Œäº’åŠ¨è½»æ¾æœ‰è¶£","advice":"ä»¥â€œæ‚¦â€å…»â€œæ·±â€ï¼šç”¨æ—¥å¸¸çš„å°ç¡®è®¤ä¸å…³æ€€ï¼ŒæŒç»­åŠ æ¸©"}
    }
}

# ---- ç”Ÿå…‹å…³ç³» ----
def _rel(a: str, b: str) -> str:
    if not a or not b: return ""
    if a == b: return "åŒæ°”ç›¸æ±‚"
    if SHENG.get(a) == b: return "ç›¸ç”Ÿ"
    if KE.get(a) == b: return "ç›¸å…‹"
    return "ç›¸å¹¶"

# ---- ä¸‰åˆè±¡ï¼ˆçº¯äººç‰©ç”»åƒï¼‰ ----
def _style_by_main_plain(h: str) -> str:
    if h in ("ä¹¾","éœ‡"): return "è¡Œäº‹èŠ‚å¥åä¸»åŠ¨"
    if h in ("å¤","è‰®"): return "è¡Œäº‹èŠ‚å¥åç¨³å¥"
    if h in ("ç¦»",):     return "é£æ ¼é‡è¡¨è¾¾ä¸æ¸…æ™°"
    if h in ("å…‘",):     return "é£æ ¼é‡å…³ç³»ä¸äº²å’Œ"
    if h in ("å·½",):     return "é£æ ¼é‡åè°ƒä¸æ¸—é€"
    if h in ("å",):     return "é£æ ¼åè°¨æ…ä¸æ±‚è¯"
    return "é£æ ¼å¹³è¡¡"

def _relation_plain(rel: str, pos: str) -> str:
    # pos: "mf" ä¸»-è¾…ï¼›"bm" åŸº-ä¸»  â€”â€” å…¨éƒ¨æ”¹ä¸ºâ€œæè¿°æ€§â€ï¼Œä¸ä¸‹æŒ‡ä»¤
    if pos == "mf":
        if rel == "ç›¸ç”Ÿ": return "ä¸»è¾…åŒå‘ï¼Œé…åˆé¡ºç•…"
        if rel == "ç›¸å…‹": return "ä¸»è¾…ç›¸åˆ¶ï¼Œæ¨è¿›æ—¶æ˜“æœ‰æ‹‰æ‰¯"
        if rel == "åŒæ°”ç›¸æ±‚": return "ä¸»è¾…åŒé¢‘ï¼Œæ‰§è¡Œå¹²è„†"
        return "ä¸»è¾…å„å®ˆä¸€éš…ï¼Œå–èˆæƒè¡¡æ›´æ˜¾é‡è¦"
    else:
        if rel == "ç›¸ç”Ÿ": return "æ ¹åŸºä¸ç›®æ ‡é¡ºæµï¼Œåº•ç›˜ç»™åŠ›"
        if rel == "ç›¸å…‹": return "å†…åœ¨ç»éªŒä¸ç›®æ ‡ç›¸æ‹§ï¼Œå¿ƒåŠ›æœ‰åˆ†é…"
        if rel == "åŒæ°”ç›¸æ±‚": return "å†…å¤–ä¸€è‡´ï¼Œè¡¨è¾¾ä¸è¡ŒåŠ¨ä¸æ‰“æ¶"
        return "èµ„æºå–å‘ä¸ç›®æ ‡å„æœ‰ä¾§é‡ï¼Œéœ€è¦å…¼å®¹å¹¶è¡Œ"

def _synthesize_combo_portrait(hexes: List[str]) -> str:
    zh, sh, bh = (hexes + ["", "", ""])[:3]
    keys = [h for h in [zh, sh, bh] if h]
    if not keys: return ""

    def vw(h, key): 
        return (WUXING.get(h) or {}).get(key, "")

    lead_parts = []
    for role, h in (("ä¸»", zh), ("è¾…", sh), ("åŸº", bh)):
        if not h: continue
        ele = vw(h,"element"); pol = vw(h,"polarity"); vir = vw(h,"virtue")
        sym = BAGUA_SYMBOLS.get(h,"")
        lead_parts.append(f"{role}{h}ï¼ˆ{sym}ï¼‰ï¼Œå±{ele}ä¸º{pol}ï¼Œ{vir}")
    lead = "ï¼›".join(lead_parts) + "ã€‚"

    rel1 = _rel(vw(zh,"element"), vw(sh,"element")) if zh and sh else ""
    rel2 = _rel(vw(bh,"element"), vw(zh,"element")) if bh and zh else ""
    rel_texts = []
    if rel1: rel_texts.append(_relation_plain(rel1, "mf"))
    if rel2: rel_texts.append(_relation_plain(rel2, "bm"))
    style = _style_by_main_plain(zh) if zh else "é£æ ¼å¹³è¡¡"

    tail = "ï¼›".join([t for t in rel_texts if t])
    tail = (tail + ("ï¼›" if tail else "") + style) if style else tail
    out = f"ä¸‰è±¡ç›¸åˆï¼š{lead}{tail}ã€‚"
    return _dedupe_smart(out)

# ---- ä¸‰åˆ†è±¡åˆå¥ & ä¸“ä¸šæç¤ºï¼ˆä¿æŒ v3.8 çš„ä¼˜åŒ–ï¼Œä½†å»å»ºè®®åŒ–ï¼‰ ----
def _combine_sentence(desc: str, interp: str) -> str:
    if not desc and not interp: return ""
    desc  = _neutralize(_depronoun((desc or "").strip().rstrip("ï¼›;ã€‚")))
    interp = _neutralize(_depronoun((interp or "").strip().lstrip("â€”â€”").lstrip("- ").strip().rstrip("ï¼›;ã€‚")))
    interp = re.sub(r"^(è¿™ç§|æ­¤ç±»|è¿™ç±»|å…¶|è¿™ç§å§¿æ€|è¿™ç§ç¥æƒ…|è¿™ç§é¢å®¹)[ï¼Œã€ï¼š ]*", "", interp)
    s = f"{desc}ï¼Œ{interp}" if (desc and interp) else (desc or interp)
    s = re.sub(r"[ï¼›;]+", "ï¼›", s)
    s = re.sub(r"ï¼Œï¼Œ+", "ï¼Œ", s)
    return _dedupe_smart(s)

def _collect_traits_and_merge(ta: Dict[str,Any]) -> Tuple[List[str], Dict[str,Any]]:
    traits, new_ta = [], {}
    for key in ["å§¿æ€","ç¥æƒ…","é¢å®¹"]:
        o = (ta.get(key) or {}).copy()
        tend = (o.get("æ€§æ ¼å€¾å‘") or "").strip().rstrip("ï¼›;ã€‚")
        if tend: traits.append(tend)
        desc = (o.get("è¯´æ˜") or "")
        inter = (o.get("è§£è¯»") or "")
        merged = _combine_sentence(desc, inter)
        hexname = (o.get("å¦è±¡") or "").strip()
        pro = ""
        if hexname in HEX_SUMMARY:
            kw = HEX_SUMMARY[hexname].split("Â·")[1] if "Â·" in HEX_SUMMARY[hexname] else HEX_SUMMARY[hexname]
            pro = f"ã€{hexname}Â·{kw}ã€‘"
        if pro and merged:
            merged = f"{pro} {merged}"
        o["è¯´æ˜"] = desc.strip().rstrip("ï¼›;ã€‚")
        o["è§£è¯»"] = merged.strip()
        o["æ€§æ ¼å€¾å‘"] = ""  # ä¸‰åˆè±¡çš„äººæ ¼å€¾å‘åªåœ¨å†…éƒ¨åˆæˆ
        new_ta[key] = o
    for k in ta.keys():
        if k not in new_ta:
            new_ta[k] = ta[k]
    return traits, new_ta

# ---- é¢†åŸŸæ¨å¯¼ï¼šçŠ¶æ€ & å»ºè®® ----
def _rel_to_env_phrase(rel: str, domain: str, pos: str) -> str:
    # pos: "mf" å¤–éƒ¨/äººé™…ï¼›"bm" èµ„æº/æ ¹åŸº
    if domain == "äº‹ä¸š":
        if pos == "mf":
            return {"ç›¸ç”Ÿ":"å¤–éƒ¨åŠ©æ¨ï¼ŒåŒäº‹/å®¢æˆ·æ›´æ˜“é…åˆ",
                    "ç›¸å…‹":"å¤–éƒ¨ç‰µæ‰¯ï¼ŒååŒæˆæœ¬ä¸Šå‡",
                    "åŒæ°”ç›¸æ±‚":"å¤–éƒ¨åŒé¢‘ï¼Œä¿¡æ¯ä¼ è¾¾æ›´é¡º",
                    "ç›¸å¹¶":"ä¾§é‡ä¸åŒï¼Œéœ€åœ¨ç›®æ ‡ä¸èµ„æºé—´å–èˆ"}.get(rel,"")
        else:
            return {"ç›¸ç”Ÿ":"åº•ç›˜ç»™åŠ›ï¼Œèµ„æºä¸èŠ‚å¥é¡ºæµ",
                    "ç›¸å…‹":"æ—§ç»éªŒç‰µåˆ¶ï¼Œèµ„æºä¸ç›®æ ‡æœ‰æ‹§",
                    "åŒæ°”ç›¸æ±‚":"å†…å¤–ä¸€è‡´ï¼Œæ‰§è¡Œé˜»åŠ›å°",
                    "ç›¸å¹¶":"èµ„æºæ–¹å‘ä¸ç›®æ ‡å„æœ‰ä¾§é‡"}.get(rel,"")
    else:  # æ„Ÿæƒ…
        if pos == "mf":
            return {"ç›¸ç”Ÿ":"å¯¹æ–¹/å…³ç³»æ˜“å›åº”ï¼Œäº’åŠ¨æ›´é¡º",
                    "ç›¸å…‹":"å½¼æ­¤èŠ‚å¥ä¸ä¸€ï¼Œæ˜“è¯¯è¯»",
                    "åŒæ°”ç›¸æ±‚":"åŒé¢‘æ²Ÿé€šï¼Œé»˜å¥‘æ„Ÿå¼º",
                    "ç›¸å¹¶":"å…³æ³¨ç‚¹ä¸åŒï¼Œéœ€è¦æ›´å¤šç†è§£"}.get(rel,"")
        else:
            return {"ç›¸ç”Ÿ":"å®‰å…¨æ„Ÿå……æ²›ï¼Œå…³ç³»åº•è‰²ç¨³å®š",
                    "ç›¸å…‹":"æ—§æƒ…ç»ª/æ—§æ¨¡å¼ç‰µæ‰¯å½“ä¸‹",
                    "åŒæ°”ç›¸æ±‚":"ä»·å€¼è§‚ä¸€è‡´ï¼Œæ‰¿è¯ºæ˜“å…‘ç°",
                    "ç›¸å¹¶":"ç°å®æ¡ä»¶ä¸æœŸå¾…ä¸å®Œå…¨é‡åˆ"}.get(rel,"")

def _domain_status_by_kb(hexes: List[str], domain: str) -> str:
    zh, sh, bh = (hexes + ["", "", ""])[:3]
    def e(h): return (WUXING.get(h) or {}).get("element","")
    s_main = HEX_DOMAIN_KB.get(zh,{}).get("career" if domain=="äº‹ä¸š" else "love",{}).get("state","") if zh else ""
    s_fu   = HEX_DOMAIN_KB.get(sh,{}).get("career" if domain=="äº‹ä¸š" else "love",{}).get("state","") if sh else ""
    s_base = HEX_DOMAIN_KB.get(bh,{}).get("career" if domain=="äº‹ä¸š" else "love",{}).get("state","") if bh else ""

    r_mf = _rel(e(zh), e(sh)) if zh and sh else ""
    r_bm = _rel(e(bh), e(zh)) if bh and zh else ""

    env_phrase = _domain_status_by_kb._cache.setdefault((domain,"mf",r_mf), _rel_to_env_phrase(r_mf, domain, "mf")) if r_mf else ""
    base_phrase= _domain_status_by_kb._cache.setdefault((domain,"bm",r_bm), _rel_to_env_phrase(r_bm, domain, "bm")) if r_bm else ""

    parts = []
    if s_main: parts.append(f"ä¸»{zh}ï¼š{s_main}")
    if s_fu:   parts.append(f"è¾…{sh}ï¼š{s_fu}")
    if env_phrase: parts.append(env_phrase)
    if s_base: parts.append(f"åŸº{bh}ï¼š{s_base}")
    if base_phrase: parts.append(base_phrase)

    text = "ï¼›".join([p for p in parts if p])
    return _dedupe_smart(text)
_domain_status_by_kb._cache = {}

def _domain_advice_by_kb(hexes: List[str], domain: str) -> str:
    zh, sh, bh = (hexes + ["", "", ""])[:3]
    s = []
    key = "career" if domain=="äº‹ä¸š" else "love"
    if zh: s.append(HEX_DOMAIN_KB.get(zh,{}).get(key,{}).get("advice",""))
    if sh: s.append(HEX_DOMAIN_KB.get(sh,{}).get(key,{}).get("advice",""))
    if bh: s.append(HEX_DOMAIN_KB.get(bh,{}).get(key,{}).get("advice",""))

    # å…³ç³»ä¿®æ­£ï¼ˆå…è®¸ä¸‹æŒ‡ä»¤ï¼Œå› ä¸ºå±äºâ€œå»ºè®®â€ï¼‰
    def e(h): return (WUXING.get(h) or {}).get("element","")
    r_mf = _rel(e(zh), e(sh)) if zh and sh else ""
    r_bm = _rel(e(bh), e(zh)) if bh and zh else ""

    if domain=="äº‹ä¸š":
        if r_mf=="ç›¸å…‹": s.append("å…ˆæŠŠè§’è‰²ä¸æœŸå¾…è®²æ¸…ï¼Œå†å®šæ¨è¿›èŠ‚å¥")
        if r_mf=="ç›¸ç”Ÿ": s.append("è¯†åˆ«åŠ©æ¨ç‚¹ï¼Œå€ŸåŠ›å‘åŠ›")
        if r_bm=="ç›¸å…‹": s.append("åŒºåˆ†æ—§ç»éªŒä¸å½“ä¸‹ç›®æ ‡ï¼Œé¿å…è€—æ•£")
        if r_bm=="ç›¸ç”Ÿ": s.append("æŠŠæˆç†Ÿåšæ³•æ ‡å‡†åŒ–ï¼Œå½¢æˆSOP")
        if r_mf=="åŒæ°”ç›¸æ±‚": s.append("è®¾ç½®â€œå”±åè°ƒâ€ç¯èŠ‚ï¼Œé¿å…ç›²ç‚¹")
    else:
        if r_mf=="ç›¸å…‹": s.append("æ˜ç¡®ç•Œé™ä¸èŠ‚å¥ï¼Œå‡å°‘è¯¯ä¼š")
        if r_mf=="ç›¸ç”Ÿ": s.append("é¡ºåŠ¿å¤šåˆ›é€ æ­£å‘äº’åŠ¨")
        if r_bm=="ç›¸å…‹": s.append("ä¸è¿‡å»å’Œè§£ï¼Œé¿å…æ—§æ¨¡å¼å½±å“å½“ä¸‹")
        if r_bm=="ç›¸ç”Ÿ": s.append("æŠŠç¨³å®šçš„å¥½ä¹ æƒ¯ä¿ç•™ä¸‹æ¥")
        if r_mf=="åŒæ°”ç›¸æ±‚": s.append("ä¿ç•™ç©ºé—´ä¸æ–°é²œæ„Ÿ")

    tips = [ _neutralize(_depronoun(t)).strip("ã€‚ï¼› ") for t in s if t ]
    tips = [t for i,t in enumerate(tips) if t and t not in tips[:i]]
    return _dedupe_smart(("ï¼›".join(tips[:3]) + "ã€‚") if tips else "")

# ---- èåˆè¾“å‡º ----
def _merge_status_and_detail(status: str, detail: str) -> str:
    # v3.9ï¼šä¿ç•™ä¸€å°å¥æ¥è‡ªæ¨¡å‹çš„æè¿°ä½œä¸ºâ€œè‰²å½©â€ï¼Œä½†ä¸è¦†ç›–KBæ¨å¯¼
    detail_first = detail.split("ã€‚")[0].strip() if detail else ""
    detail_first = _neutralize(_strip_domain_lead(detail_first))
    status = _neutralize(_strip_domain_lead(status or ""))
    parts = [p for p in [status, detail_first] if p]
    text = "ï¼›".join(parts).rstrip("ï¼›")
    return _dedupe_smart(text)

def _coerce_output(data: Dict[str,Any]) -> Dict[str,Any]:
    out = dict(data)
    meta = out.get("meta") or {}
    if not isinstance(meta, dict): meta = {}
    out["meta"] = meta

    ta = meta.get("triple_analysis") or {}
    traits, ta = _collect_traits_and_merge(ta)
    meta["triple_analysis"] = ta

    hexes = [(ta.get("å§¿æ€") or {}).get("å¦è±¡",""),
             (ta.get("ç¥æƒ…") or {}).get("å¦è±¡",""),
             (ta.get("é¢å®¹") or {}).get("å¦è±¡","")]
    combo_title = " + ".join([h for h in hexes if h])
    meta["combo_title"] = combo_title

    # ä¸‰åˆè±¡ï¼šæ”¹ä¸ºâ€œäººç‰©ç”»åƒâ€
    overview = _synthesize_combo_portrait(hexes)
    if not overview:
        overview = (ta.get("æ€»ç»“") or out.get("summary","")).strip().rstrip("ï¼›;")
    meta["overview_card"] = {"title": f"ğŸ”® å¦è±¡ç»„åˆï¼š{combo_title}" if combo_title else "ğŸ”® å¦è±¡ç»„åˆ",
                             "summary": overview}

    # headline
    try:
        out["confidence"] = float(out.get("confidence",0.0))
    except Exception:
        out["confidence"] = 0.0
    arch = (out.get("archetype") or "").strip()
    out["archetype"] = arch
    meta["headline"] = {"tag": arch or "äººæ ¼ç”»åƒ", "confidence": out["confidence"]}

    # é¢†åŸŸï¼šçŠ¶æ€ & å»ºè®®ï¼ˆå…¨éƒ¨èµ°KB + ç”Ÿå…‹æ¨å¯¼ï¼‰
    kb_status = {
        "äº‹ä¸š": _domain_status_by_kb(hexes, "äº‹ä¸š"),
        "æ„Ÿæƒ…": _domain_status_by_kb(hexes, "æ„Ÿæƒ…"),
    }
    dd = (meta.get("domains_detail") or {})  # ä»…å–è‰²å½©
    merged_status = {
        "äº‹ä¸š": _merge_status_and_detail(kb_status.get("äº‹ä¸š",""), dd.get("é‡‘é’±ä¸äº‹ä¸š","")),
        "æ„Ÿæƒ…": _merge_status_and_detail(kb_status.get("æ„Ÿæƒ…",""), dd.get("é…å¶ä¸æ„Ÿæƒ…","")),
    }
    meta["domains_status"] = merged_status
    meta["domains_suggestion"] = {
        "äº‹ä¸š": _domain_advice_by_kb(hexes, "äº‹ä¸š"),
        "æ„Ÿæƒ…": _domain_advice_by_kb(hexes, "æ„Ÿæƒ…"),
    }

    def _clean(s):
        if not isinstance(s, str): return s
        s = s.replace("â€”â€”", "ï¼Œ")
        s = re.sub(r"[ï¼›;]+", "ï¼›", s)
        s = re.sub(r"ï¼›([ã€‚ï¼])", r"\1", s)
        s = re.sub(r"([ã€‚ï¼ï¼Ÿ])ï¼›", r"\1", s)
        s = _depronoun(s)
        s = _neutralize(s)
        return _dedupe_smart(s)

    out["summary"] = _clean(out.get("summary",""))
    out["archetype"] = _clean(out.get("archetype",""))

    def _deep_clean(x):
        if isinstance(x, dict):
            return {k:_deep_clean(v) for k,v in x.items()}
        if isinstance(x, list):
            return [_deep_clean(v) for v in x]
        return _clean(x)

    out["meta"] = _deep_clean(meta)
    return out

# ---- HTTP è·¯ç”± ----
@app.get("/health")
def health(): return {"status":"ok"}

@app.get("/", include_in_schema=False)
def root():
    return HTMLResponse("<h3>Selfy AI</h3><a href='/docs'>/docs</a> Â· <a href='/mobile'>/mobile</a>")

@app.head("/", include_in_schema=False)
def root_head():
    return Response(status_code=200)

@app.get("/version")
def version(): return {"runtime":RUNTIME_VERSION,"analysis":ANALYSIS_VERSION,"schema":SCHEMA_ID,"debug":DEBUG}

@app.get("/mobile", include_in_schema=False)
def mobile():
    path = os.path.join(os.path.dirname(__file__), "index_mobile.html")
    try:
        html = open(path, "r", encoding="utf-8").read()
    except Exception as e:
        return HTMLResponse(f"<pre>index_mobile.html not found: {e}</pre>", status_code=500)
    return HTMLResponse(html)

def _call_openai(messages):
    if client is None:
        raise RuntimeError("OpenAI client not initialized")
    return client.chat.completions.create(
        model="gpt-4o",
        temperature=0.4,
        tools=_build_tools_schema(),
        tool_choice={"type":"function","function":{"name":"submit_analysis_v3"}},
        response_format={"type":"json_object"},
        messages=messages
    )

def _call_gpt_tool_with_image(data_url: str) -> Dict[str,Any]:
    messages = _prompt_for_image_v390()
    messages[-1]["content"] = [
        {"type":"text","text":messages[-1]["content"]},
        {"type":"image_url","image_url":{"url":data_url}}
    ]
    resp = _call_openai(messages)
    choice = resp.choices[0]
    tool_calls = getattr(choice.message, "tool_calls", None)
    if tool_calls:
        args = json.loads(tool_calls[0].function.arguments)
    else:
        content = getattr(choice.message, "content", None)
        if isinstance(content, str) and content.strip().startswith("{"):
            args = json.loads(content)
        else:
            raise RuntimeError("Model did not return tool_calls.")
    return {"tool_args": args, "oai_raw": resp if DEBUG else None}

@app.post("/upload")
async def upload(file: UploadFile = File(...)):
    try:
        if not file: raise HTTPException(400,"No file")
        ct = file.content_type or ""
        if not ct.startswith("image/"): raise HTTPException(415,f"Unsupported content type: {ct}")
        raw = await file.read()
        if not raw: raise HTTPException(400,"Empty file")
        if len(raw) > 15*1024*1024: raise HTTPException(413,"File too large (>15MB)")

        data_url = _to_data_url(raw, ct)
        logger.info("[UPLOAD] %s %dB %s", file.filename, len(raw), ct)

        result = _call_gpt_tool_with_image(data_url)
        tool_args = result["tool_args"]
        final_out = _coerce_output(tool_args)

        if DEBUG:
            meta = final_out.setdefault("meta",{}).setdefault("debug",{})
            meta["file_info"]={"filename":file.filename,"content_type":ct,"size":len(raw)}
            try:
                meta["oai_choice_finish_reason"]=result["oai_raw"].choices[0].finish_reason
            except Exception:
                meta["oai_choice_finish_reason"]="n/a"

        return JSONResponse(content=final_out, status_code=200)
    except HTTPException as he:
        if DEBUG:
            return JSONResponse(status_code=he.status_code, content={"error":he.detail,"debug":{"trace":traceback.format_exc()}})
        raise
    except Exception as e:
        logging.exception("upload failed: %s", e)
        body={"error":"Internal Server Error"}
        if DEBUG: body["debug"]={"message":str(e),"trace":traceback.format_exc()}
        return JSONResponse(status_code=500, content=body)
