# fastapi_app.py  (runtime v3.9.0 Â· YiJing inference)
# - Tri-image (å§¿æ€/ç¥æƒ…/é¢ç›¸) stays intact from upstream (model v3.7.2 style)
# - NEW: å¦è±¡ç»„åˆ(ä¸‰åˆè±¡) = äººç‰©ç”»åƒï¼ˆä¸»/è¾…/åŸº + äº”è¡Œç”Ÿå…‹ï¼Œä¸¥ç¦å»ºè®®å£å»ï¼‰
# - NEW: äº‹ä¸š/æ„Ÿæƒ… = è¿‘æœŸçŠ¶æ€(bullets) + è¿‘æœŸå»ºè®®(bullets)ï¼Œå®Œå…¨æŒ‰è¯åº“ä¸ç”Ÿå…‹æ¨å¯¼
# - Lexicon hot-load: yijing_lexicon.json in same folder; reload on every request
# - Render-compatible; no external state required
import os, json, re, base64, logging, traceback
from typing import Any, Dict, List, Tuple

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse, HTMLResponse
from fastapi.middleware.cors import CORSMiddleware

from openai import OpenAI

RUNTIME_VERSION = "3.9.0"
ANALYSIS_VERSION = os.getenv("ANALYSIS_VERSION", "372")  # model prompt profile
SCHEMA_ID = "selfy.v3"
DEBUG = str(os.getenv("DEBUG","0")).strip() in ("1","true","True","YES","yes")

logging.basicConfig(level=logging.DEBUG if DEBUG else logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger("selfy-ai")

app = FastAPI(title="Selfy AI â€” YiJing Inference API", version=RUNTIME_VERSION)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

try:
    client = OpenAI()
except Exception as e:
    logger.error("OpenAI client init failed: %s", e); client=None

# --- Utils ---
BAGUA_SYMBOLS = {"è‰®":"å±±","ç¦»":"ç«","å…‘":"æ³½","ä¹¾":"å¤©","å¤":"åœ°","éœ‡":"é›·","å·½":"é£","å":"æ°´"}

def _to_data_url(content: bytes, content_type: str) -> str:
    return f"data:{content_type};base64,{base64.b64encode(content).decode('utf-8')}"

# --- Lexicon ---
def load_lexicon() -> Dict[str, Any]:
    """Hot-load lexicon every call. Must be placed next to this file."""
    path = os.path.join(os.path.dirname(__file__), "yijing_lexicon.json")
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.warning("Lexicon missing or invalid, using minimal fallback: %s", e)
        # Minimal fallback for safety
        return {
            "persona": {"ä¹¾":"åˆšå¥è‡ªå¼º","å¤":"åšå¾·è½½ç‰©","ç¦»":"æ˜è¾¨è¡¨è¾¾","å…‘":"äº²å’Œäº¤æµ","éœ‡":"è¡ŒåŠ¨å¼€å±€","å·½":"åè°ƒå…¥å¾®","å":"è°¨æ…æ±‚è¯","è‰®":"æ­¢å®šç¨³å®ˆ"},
            "domains": {"career":{},"love":{}},
            "wuxing":{"ä¹¾":"é‡‘","å…‘":"é‡‘","ç¦»":"ç«","éœ‡":"æœ¨","å·½":"æœ¨","å":"æ°´","è‰®":"åœŸ","å¤":"åœŸ"},
            "sheng":{"æœ¨":"ç«","ç«":"åœŸ","åœŸ":"é‡‘","é‡‘":"æ°´","æ°´":"æœ¨"},
            "ke":{"æœ¨":"åœŸ","åœŸ":"æ°´","æ°´":"ç«","ç«":"é‡‘","é‡‘":"æœ¨"},
            "fuse":{"ç”Ÿ":"ç‰¹è´¨äº’è¡¥ï¼Œæ›´å®¹æ˜“å½¢æˆåˆåŠ›","å…‹":"é£æ ¼æœ‰å†²çªï¼Œéœ€è¦å…ˆå¯¹é½æ–¹å¼å†æ¨è¿›","æ¯”":"é£æ ¼ä¸€è‡´ï¼Œä¼˜åŠ¿è¢«æ”¾å¤§ï¼ŒåŒæ—¶è¦ç•™æ„ç›²åŒº","å¹¶":"å„æœ‰ä¾§é‡ï¼Œåˆ†å·¥æ¸…æ™°ï¼Œäº’ä¸å¹²æ‰°"},
            "rel_influence":{"career":{},"love":{}}
        }

# --- OpenAI tool schema ---
def _build_tools_schema():
    return [{
      "type":"function",
      "function":{
        "name":"submit_analysis_v3",
        "description":"Return end-user facing JSON for Selfy AI YiJing analysis.",
        "parameters":{
          "type":"object",
          "properties":{
            "summary":{"type":"string"},
            "archetype":{"type":"string"},
            "confidence":{"type":"number"},
            "sections":{"type":"object","properties":{"å§¿æ€":{"type":"string"},"ç¥æƒ…":{"type":"string"},"é¢ç›¸":{"type":"string"}},"required":["å§¿æ€","ç¥æƒ…","é¢ç›¸"],"additionalProperties":False},
            "domains":{"type":"array","items":{"type":"string"}},
            "meta":{"type":"object","additionalProperties":True}
          },
          "required":["summary","archetype","confidence","sections","domains"],
          "additionalProperties":False
        }
      }
    }]

# --- OpenAI prompt (æŠŠâ€œé¢å®¹â€=>â€œé¢ç›¸â€ï¼Œå¹¶è¦æ±‚è¿”å› meta.triple_analysis) ---
def _prompt_for_image_v372():
    sys = (
      "ä½ æ˜¯ Selfy AI çš„æ˜“ç»è§‚ç›¸åŠ©æ‰‹ï¼ˆv3.7.2 é£æ ¼ï¼‰ã€‚"
      "ä¸¥æ ¼æŒ‰â€œä¸‰è±¡å››æ®µå¼â€åˆ†æï¼šã€å§¿æ€/ç¥æƒ…/é¢ç›¸ã€‘ä¸‰éƒ¨åˆ†ã€‚æ¯éƒ¨åˆ†åŒ…å«ï¼š"
      "1) è¯´æ˜ï¼š1å¥ï¼Œå®¢è§‚æç»˜ï¼›2) å¦è±¡ï¼šä»…å†™ä¸€ä¸ªå…«å¦åï¼›3) è§£è¯»ï¼š1â€“2å¥ï¼›4) æ€§æ ¼å€¾å‘ï¼š1â€“2å¥ã€‚"
      "éšåï¼š5) å¦è±¡ç»„åˆï¼ˆ90â€“150å­—ï¼‰ï¼›6) æ€»ç»“æ€§æ ¼å°è±¡ï¼ˆ20â€“40å­—ï¼‰ï¼›7) archetypeï¼ˆ2â€“5å­—ï¼‰ã€‚"
      "åŒæ—¶ï¼š"
      "Â· åœ¨ meta.face_parts ä¸­ç»™ã€çœ‰/çœ¼/é¼»/å˜´/é¢§/ä¸‹å·´ã€‘ï¼ˆè¦†ç›–5é¡¹ï¼‰ç‰¹å¾ä¸è§£è¯»ï¼›"
      "Â· åœ¨ meta.triple_analysis ä¸­åˆ†åˆ«ç»™ã€å§¿æ€/ç¥æƒ…/é¢ç›¸ã€‘çš„ {è¯´æ˜, å¦è±¡, è§£è¯», æ€§æ ¼å€¾å‘}ã€‚"
      "domains ä»…ä» ['é‡‘é’±ä¸äº‹ä¸š','é…å¶ä¸æ„Ÿæƒ…'] é€‰æ‹©ï¼›åœ¨ meta.domains_detail ä¸­å„å†™ 60â€“90 å­—å»ºè®®ã€‚"
      "é€šè¿‡ submit_analysis_v3 å·¥å…·ä»¥ JSON è¿”å›ï¼Œä¸è¦è¾“å‡ºè‡ªç”±æ–‡æœ¬ã€‚"
    )
    user = "è¯·æŒ‰ 3.7.2 é£æ ¼å¯¹å›¾åƒè¿›è¡Œåˆ†æï¼Œå¹¶ä¸¥æ ¼ä»¥å·¥å…·è¿”å› JSONã€‚"
    return [{"role":"system","content":sys},{"role":"user","content":user}]

def _call_oai(messages):
    if client is None:
        raise RuntimeError("OpenAI client not initialized")
    return client.chat.completions.create(
        model="gpt-4o",
        temperature=0.4,
        tools=_build_tools_schema(),
        tool_choice={"type":"function","function":{"name":"submit_analysis_v3"}},
        response_format={"type":"json_object"},
        messages=messages
    )

# ---------- HOTFIX: robust extraction for OpenAI responses ----------
def _looks_like_json(s: str) -> bool:
    s = (s or "").strip()
    return s.startswith("{") and s.endswith("}")

def extract_selfy_payload(resp) -> dict:
    """
    Try all possible shapes:
    1) tool_calls[0].function.arguments  (modern tools)
    2) function_call.arguments            (older)
    3) message.content as JSON string     (json_object fallback)
    Return {} if none works.
    """
    payload = {}

    try:
        # 1) tool_calls
        msg = resp.choices[0].message
        if getattr(msg, "tool_calls", None):
            for tc in msg.tool_calls:
                fn = getattr(tc, "function", None)
                if fn and getattr(fn, "name", "") == "submit_analysis_v3":
                    args = getattr(fn, "arguments", "") or ""
                    if args and _looks_like_json(args):
                        payload = json.loads(args)
                        break

        # 2) function_call (older)
        if not payload and getattr(msg, "function_call", None):
            fc = msg.function_call
            args = getattr(fc, "arguments", "") or ""
            if args and _looks_like_json(args):
                payload = json.loads(args)

        # 3) message.content as JSON
        if not payload:
            content = getattr(msg, "content", "") or ""
            if content and _looks_like_json(content):
                payload = json.loads(content)

    except Exception as e:
        logging.error("[PARSE] exception: %s", e, exc_info=True)

    # æœ€åå…œåº•ï¼šç¡®ä¿å…³é”®å­—æ®µå­˜åœ¨ï¼Œé¿å…å‰ç«¯ç©ºç™½
    if payload and isinstance(payload, dict):
        payload.setdefault("summary", "")
        payload.setdefault("archetype", "")
        payload.setdefault("confidence", 0.0)
        payload.setdefault("sections", {"å§¿æ€": "", "ç¥æƒ…": "", "é¢ç›¸": ""})
        payload.setdefault("domains", [])
        payload.setdefault("meta", {})

    if DEBUG:
        try:
            logging.debug("[PARSE] raw kind: tool_calls=%s, function_call=%s, content_len=%s",
                          bool(getattr(resp.choices[0].message, "tool_calls", None)),
                          bool(getattr(resp.choices[0].message, "function_call", None)),
                          len((getattr(resp.choices[0].message, "content", "") or "")))
            logging.debug("[PARSE] payload keys: %s", list(payload.keys()) if payload else [])
        except Exception:
            pass

    return payload
# ---------- /HOTFIX ----------


# --- Text cleaners ---
DOMAIN_LEADS = r"(åœ¨(é‡‘é’±ä¸äº‹ä¸š|é…å¶ä¸æ„Ÿæƒ…|äº‹ä¸š|æ„Ÿæƒ…)(æ–¹é¢|ä¸­|é‡Œ)?|ç›®å‰|è¿‘æœŸ|å½“ä¸‹)"
# --- Text cleanersï¼ˆåŠ å…¥â€œé¢ç›¸â€åˆ°åœç”¨è¯ï¼‰ ---
_STOPWORDS = r"(å§¿æ€|ç¥æƒ…|é¢ç›¸|é¢å®¹|æ•´ä½“|æ°”è´¨|å½¢è±¡|ç»™äººä»¥|ä¸€ç§|ä»¥åŠ|å¹¶ä¸”|è€Œä¸”|æ›´æ˜¾|æ˜¾å¾—|å±•ç°å‡º|æµéœ²å‡º|é€éœ²å‡º)"

def _depronoun(s:str)->str:
    if not isinstance(s,str): return s
    s = re.sub(r"^(ä»–|å¥¹|TA|ä½ |å¯¹æ–¹|å…¶)(çš„)?[ï¼Œã€ï¼š ]*", "", s.strip())
    s = re.sub(r"^(åœ¨(äº‹ä¸š|æ„Ÿæƒ…|ç”Ÿæ´»)[ä¸Šä¸­]|ç›®å‰|è¿‘æœŸ)[ï¼Œã€ï¼š ]*", "", s)
    return s
def _neutralize(s:str)->str:
    if not isinstance(s,str): return s
    s = re.sub(r"(ä»–|å¥¹|TA|å¯¹æ–¹|å…¶)(çš„)?", "", s.strip())
    s = re.sub(DOMAIN_LEADS + r"[ï¼Œã€ï¼š ]*", "", s)
    s = re.sub(r"(å¯èƒ½|æˆ–è®¸|ä¹Ÿè®¸)[ï¼Œã€ ]*", "", s)
    s = re.sub(r"[ï¼Œ,]{2,}", "ï¼Œ", s)
    s = re.sub(r"[ï¼›;]{2,}", "ï¼›", s)
    return s.strip("ï¼›ï¼Œã€‚ ")
def _canon_key(s:str)->str:
    if not isinstance(s,str): return ""
    k = re.sub(_STOPWORDS, "", s)
    k = re.sub(r"[çš„åœ°å¾—]", "", k); k = re.sub(r"\s+", "", k)
    return k
def _dedupe_smart(s:str)->str:
    if not isinstance(s,str): return s
    s = s.strip("ã€‚ï¼›ï¼Œ,; ")
    sentences = re.split(r"[ã€‚ï¼ï¼Ÿ]", s)
    out = []
    for sen in sentences:
        sen = sen.strip("ï¼Œ,;ï¼› ")
        if not sen: continue
        parts = re.split(r"[ï¼Œ,ï¼›;]", sen)
        seen, kept = set(), []
        for p in parts:
            t = p.strip(); 
            if not t: continue
            ck = _canon_key(t)
            if ck and ck not in seen:
                seen.add(ck); kept.append(t)
        out.append("ï¼Œ".join(kept))
    return "ã€‚".join(out) + ("ã€‚" if out else "")

# --- YiJing inference helpers ---
def _rel(el_a:str, el_b:str, sheng:Dict[str,str], ke:Dict[str,str])->str:
    if not el_a or not el_b: return "å¹¶"
    if el_a == el_b: return "æ¯”"
    if sheng.get(el_a) == el_b: return "ç”Ÿ"
    if ke.get(el_a) == el_b: return "å…‹"
    return "å¹¶"

def _synthesize_combo(lex:Dict[str,Any], h1:str, h2:str, h3:str)->str:
    """ä¸‰åˆè±¡ï¼šäººç‰©ç”»åƒï¼ˆæ— å»ºè®®ï¼‰"""
    persona = lex.get("persona",{})
    wuxing  = lex.get("wuxing",{})
    sheng   = lex.get("sheng",{})
    ke      = lex.get("ke",{})
    fuse    = lex.get("fuse",{})
    parts = []
    for role,h in (("ä¸»",h1),("è¾…",h2),("åŸº",h3)):
        if not h: continue
        sym = BAGUA_SYMBOLS.get(h,"")
        per = persona.get(h,"")
        el  = wuxing.get(h,"")
        p = f"{role}{h}ï¼ˆ{sym}ï¼Œ{el}ï¼‰ï¼Œ{per}" if per else f"{role}{h}ï¼ˆ{sym}ï¼Œ{el}ï¼‰"
        parts.append(p)
    # relations
    r1 = _rel(wuxing.get(h1,""), wuxing.get(h2,""), sheng, ke) if h1 and h2 else ""
    r2 = _rel(wuxing.get(h3,""), wuxing.get(h1,""), sheng, ke) if h3 and h1 else ""
    r_texts = []
    if r1: r_texts.append(f"ä¸»ä¸è¾…ï¼š{fuse.get(r1,'')}")
    if r2: r_texts.append(f"åŸºä¸ä¸»ï¼š{fuse.get(r2,'')}")
    out = "ï¼›".join(parts) + "ã€‚" + " ".join([t for t in r_texts if t])
    # å»â€œè¿™ç±»äºº/è¿™ç§äººâ€ç±»æŒ‡ä»£ï¼šæœ¬æ®µåªé™ˆè¿°ç‰¹è´¨
    return _dedupe_smart(_neutralize(_depronoun(out)))

def _collect_bullets(source: Dict[str,Any], keys: List[str], limit:int=4)->List[str]:
    seen, bullets = set(), []
    for k in keys:
        arr = (source.get(k) or {}).get("state") if isinstance(source.get(k), dict) else None
        if not arr: continue
        for item in arr:
            t = _dedupe_smart(_neutralize(_depronoun(str(item)))).strip("ã€‚")
            ck = _canon_key(t)
            if ck and ck not in seen:
                seen.add(ck); bullets.append("Â· " + t)
            if len(bullets) >= limit: break
        if len(bullets) >= limit: break
    return bullets

def _collect_advices(source: Dict[str,Any], keys: List[str], limit:int=3)->List[str]:
    seen, bullets = set(), []
    for k in keys:
        arr = (source.get(k) or {}).get("advice") if isinstance(source.get(k), dict) else None
        if not arr: continue
        for item in arr:
            t = _dedupe_smart(_neutralize(_depronoun(str(item)))).strip("ã€‚")
            ck = _canon_key(t)
            if ck and ck not in seen:
                seen.add(ck); bullets.append("Â· " + t)
            if len(bullets) >= limit: break
        if len(bullets) >= limit: break
    return bullets

def _relation_influence(lex:Dict[str,Any], domain:str, el_main:str, el_fu:str, el_base:str)->List[str]:
    sheng, ke = lex.get("sheng",{}), lex.get("ke",{})
    rel_text = lex.get("rel_influence",{}).get(domain,{})
    bullets = []
    if el_main and el_fu:
        r1 = _rel(el_main, el_fu, sheng, ke)
        if rel_text.get(r1): bullets.append("Â· " + rel_text[r1])
    if el_base and el_main:
        r2 = _rel(el_base, el_main, sheng, ke)
        if rel_text.get(r2): bullets.append("Â· " + rel_text[r2])
    # de-dup
    seen, out = set(), []
    for b in bullets:
        ck = _canon_key(b)
        if ck not in seen:
            seen.add(ck); out.append(b)
    return out[:2]

def _make_domains(lex:Dict[str,Any], h1:str, h2:str, h3:str)->Dict[str,Dict[str,List[str]]]:
    doms = {"career":{}, "love":{}}
    el = lex.get("wuxing",{})
    keys = [k for k in [h1,h2,h3] if k]
    for domain in ["career","love"]:
        source = lex.get("domains",{}).get(domain,{}) or {}
        status = _collect_bullets(source, keys, limit=4)
        status += _relation_influence(lex, domain, el.get(h1,""), el.get(h2,""), el.get(h3,""))
        # re-trim to <=4
        status = status[:4]
        advice = _collect_advices(source, keys, limit=3)
        doms[domain] = {"status": status, "advice": advice}
    return doms

# --- Post-processing of model output ---
def _combine_sentence(desc:str, interp:str)->str:
    if not desc and not interp: return ""
    desc  = _neutralize(_depronoun((desc or "").strip().rstrip("ï¼›;ã€‚")))
    interp = _neutralize(_depronoun((interp or "").strip().lstrip("â€”â€”").lstrip("- ").strip().rstrip("ï¼›;ã€‚")))
    interp = re.sub(r"^(è¿™ç§|æ­¤ç±»|è¿™ç±»|å…¶|è¿™ç§å§¿æ€|è¿™ç§ç¥æƒ…|è¿™ç§é¢ç›¸)[ï¼Œã€ï¼š ]*", "", interp)
    s = f"{desc}ï¼Œ{interp}" if (desc and interp) else (desc or interp)
    s = re.sub(r"[ï¼›;]+", "ï¼›", s); s = re.sub(r"ï¼Œï¼Œ+", "ï¼Œ", s)
    return _dedupe_smart(s)

def _coerce_output(tool_args: Dict[str,Any])->Dict[str,Any]:
    out = dict(tool_args)
    meta = out.get("meta") or {}
    if not isinstance(meta, dict): meta = {}
    out["meta"] = meta

    # Merge triple analysis parts
    ta = meta.get("triple_analysis") or {}
    def _apply(o):
        desc = (o.get("è¯´æ˜") or ""); inter = (o.get("è§£è¯»") or "")
        merged = _combine_sentence(desc, inter)
        o["è§£è¯»"] = merged; return o
    for k in ["å§¿æ€","ç¥æƒ…","é¢ç›¸"]:
        if isinstance(ta.get(k), dict): ta[k] = _apply(ta[k])
    meta["triple_analysis"] = ta

    # Trio hexes
    h1 = (ta.get("å§¿æ€") or {}).get("å¦è±¡","") or ""
    h2 = (ta.get("ç¥æƒ…") or {}).get("å¦è±¡","") or ""
    h3 = (ta.get("é¢ç›¸") or {}).get("å¦è±¡","") or "" 

    # Load lexicon and synthesize
    lex = load_lexicon()
    combo_title = " + ".join([h for h in [h1,h2,h3] if h])
    combo_summary = _synthesize_combo(lex, h1, h2, h3)
    meta["overview_card"] = {"title": f"ğŸ”® å¦è±¡ç»„åˆï¼š{combo_title}" if combo_title else "ğŸ”® å¦è±¡ç»„åˆ",
                             "summary": combo_summary}

    doms = _make_domains(lex, h1, h2, h3)
    meta["domains_status"] = {"äº‹ä¸š": "\n".join(doms["career"]["status"]),
                              "æ„Ÿæƒ…": "\n".join(doms["love"]["status"])}
    meta["domains_suggestion"] = {"äº‹ä¸š": "\n".join(doms["career"]["advice"]),
                                  "æ„Ÿæƒ…": "\n".join(doms["love"]["advice"])}

    # Headline
    try:
        conf = float(out.get("confidence",0.0))
    except Exception: conf = 0.0
    meta["headline"] = {"tag": (out.get("archetype") or "").strip(), "confidence": conf}

    # Top-level clean
    def _clean(s):
        if not isinstance(s,str): return s
        s = s.replace("â€”â€”","ï¼Œ")
        s = re.sub(r"[ï¼›;]+","ï¼›", s)
        s = re.sub(r"ï¼›([ã€‚ï¼])", r"\1", s)
        s = re.sub(r"([ã€‚ï¼ï¼Ÿ])ï¼›", r"\1", s)
        s = _depronoun(s); s = _neutralize(s)
        return _dedupe_smart(s)
    out["summary"] = _clean(out.get("summary",""))
    out["archetype"] = _clean(out.get("archetype",""))
    # deep clean meta
    def _deep(x):
        if isinstance(x, dict): return {k:_deep(v) for k,v in x.items()}
        if isinstance(x, list): return [_deep(v) for v in x]
        return _clean(x)
    out["meta"] = _deep(meta)
    return out

# --- Routes ---
@app.get("/health")
def health(): return {"status":"ok"}

@app.get("/", include_in_schema=False)
def root():
    return HTMLResponse("<h3>Selfy AI</h3><a href='/docs'>/docs</a> Â· <a href='/mobile'>/mobile</a>")

@app.get("/version")
def version(): return {"runtime":RUNTIME_VERSION,"analysis":ANALYSIS_VERSION,"schema":SCHEMA_ID,"debug":DEBUG}

@app.get("/mobile", include_in_schema=False)
def mobile():
    path = os.path.join(os.path.dirname(__file__), "index_mobile.html")
    try:
        html = open(path, "r", encoding="utf-8").read()
    except Exception as e:
        return HTMLResponse(f"<pre>index_mobile.html not found: {e}</pre>", status_code=500)
    return HTMLResponse(html)

@app.post("/upload")
async def upload(file: UploadFile = File(...)):
    try:
        if client is None:
            raise HTTPException(503, "OpenAI client not initialized")
        ct = file.content_type or ""
        if not ct.startswith("image/"): raise HTTPException(415,f"Unsupported content type: {ct}")
        raw = await file.read()
        if not raw: raise HTTPException(400,"Empty file")
        if len(raw) > 15*1024*1024: raise HTTPException(413,"File too large (>15MB)")

        data_url = _to_data_url(raw, ct)
        logger.info("[UPLOAD] %s %dB %s", file.filename, len(raw), ct)

        # 1) ç»„è£…å¤šæ¨¡æ€æ¶ˆæ¯ï¼ˆæ–‡æœ¬ + å›¾ç‰‡ï¼‰
        msgs = _prompt_for_image_v372()
        msgs[-1]["content"] = [
            {"type":"text","text":msgs[-1]["content"]},
            {"type":"image_url","image_url":{"url":data_url}}
        ]

        # 2) è°ƒç”¨ OpenAIï¼ˆç”¨æ­£å¼å‚æ•°ï¼Œè€Œä¸æ˜¯ [...] å ä½ç¬¦ï¼‰
        resp = client.chat.completions.create(
            model="gpt-4o",
            messages=msgs,
            response_format={"type": "json_object"},
            tools=_build_tools_schema(),
            tool_choice={"type": "function", "function": {"name": "submit_analysis_v3"}},
            temperature=0.4,
        )

        # 3) è§£æ + åˆæˆåå¤„ç†
        raw_payload = extract_selfy_payload(resp)
        if not raw_payload:
            logging.error("[UPLOAD] Empty payload after extraction")
            return JSONResponse({"error": "empty_payload", "tip": "enable DEBUG=1 to log raw shapes"}, status_code=502)

        final_payload = _coerce_output(raw_payload)
        return JSONResponse(final_payload)

    except HTTPException as he:
        if DEBUG:
            return JSONResponse(status_code=he.status_code, content={"error":he.detail,"debug":{"trace":traceback.format_exc()}})
        raise
    except Exception as e:
        logging.exception("upload failed: %s", e)
        body={"error":"Internal Server Error"}
        if DEBUG: body["debug"]={"message":str(e),"trace":traceback.format_exc()}
        return JSONResponse(status_code=500, content=body)
