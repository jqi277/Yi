# fastapi_app.py  (runtime v3.8.1, analysis logic v3.7.2, humanized phrasing + ä¸»/è¾…/åŸº synthesis â€¢ Yijing combo independent)
import os, base64, json, logging, traceback, re, math
from typing import Dict, Any, List

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse, HTMLResponse, Response
from fastapi.middleware.cors import CORSMiddleware

from openai import OpenAI

RUNTIME_VERSION = "3.8.1"
ANALYSIS_VERSION = os.getenv("ANALYSIS_VERSION", "372").strip()  # default 372
SCHEMA_ID = "selfy.v3"
DEBUG = str(os.getenv("DEBUG","0")).strip() in ("1","true","True","YES","yes")

logging.basicConfig(level=logging.DEBUG if DEBUG else logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger("selfy-ai")

app = FastAPI(title="Selfy AI - YiJing Analysis API", version=RUNTIME_VERSION)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

# OpenAI client
try:
    client = OpenAI()
except Exception as e:
    logger.error("OpenAI client init failed: %s", e); client=None

BAGUA_SYMBOLS = {"è‰®":"å±±","ç¦»":"ç«","å…‘":"æ³½","ä¹¾":"å¤©","å¤":"åœ°","éœ‡":"é›·","å·½":"é£","å":"æ°´"}

# ---------------- helpers ----------------
def _to_data_url(content: bytes, content_type: str) -> str:
    return f"data:{content_type};base64,{base64.b64encode(content).decode('utf-8')}"

def _build_tools_schema() -> List[Dict[str, Any]]:
    return [{
      "type":"function",
      "function":{
        "name":"submit_analysis_v3",
        "description":"Return end-user facing JSON for Selfy AI YiJing analysis.",
        "parameters":{
          "type":"object",
          "properties":{
            "summary":{"type":"string"},
            "archetype":{"type":"string"},
            "confidence":{"type":"number"},
            "sections":{"type":"object","properties":{"å§¿æ€":{"type":"string"},"ç¥æƒ…":{"type":"string"},"é¢ç›¸":{"type":"string"}},"required":["å§¿æ€","ç¥æƒ…","é¢ç›¸"],"additionalProperties":False},
            "domains":{"type":"array","items":{"type":"string"}},
            "meta":{"type":"object","additionalProperties":True}
          },
          "required":["summary","archetype","confidence","sections","domains"],
          "additionalProperties":False
        }
      }
    }]

def _json_hint() -> str:
    return ("åªä»¥ JSON object è¿”å›ï¼ˆå¿…é¡» JSONï¼‰ã€‚ç¤ºä¾‹:{\"summary\":\"â€¦\",\"archetype\":\"â€¦\",\"confidence\":0.9,"
            "\"sections\":{\"å§¿æ€\":\"â€¦\",\"ç¥æƒ…\":\"â€¦\",\"é¢ç›¸\":\"â€¦\"},"
            "\"domains\":[\"é‡‘é’±ä¸äº‹ä¸š\",\"é…å¶ä¸æ„Ÿæƒ…\"],"
            "\"meta\":{\"triple_analysis\":{\"å§¿æ€\":{\"è¯´æ˜\":\"â€¦\",\"å¦è±¡\":\"è‰®\",\"è§£è¯»\":\"â€¦\",\"æ€§æ ¼å€¾å‘\":\"â€¦\"},\"ç¥æƒ…\":{â€¦},\"é¢å®¹\":{â€¦},\"ç»„åˆæ„å¢ƒ\":\"â€¦\",\"æ€»ç»“\":\"â€¦\"},"
            "\"face_parts\":{\"çœ‰\":{\"ç‰¹å¾\":\"â€¦\",\"å¦è±¡\":\"â€¦\",\"è§£è¯»\":\"â€¦\"},\"çœ¼\":{â€¦},\"é¼»\":{â€¦},\"å˜´\":{â€¦},\"é¢§/ä¸‹å·´\":{â€¦}},"
            "\"domains_detail\":{\"é‡‘é’±ä¸äº‹ä¸š\":\"â€¦(60â€“90å­—)\",\"é…å¶ä¸æ„Ÿæƒ…\":\"â€¦(60â€“90å­—)\"}}}")

def _prompt_for_image_v372():
    # è¯´æ˜ï¼šä¿ç•™ 3.7.2 çš„æ ¸å¿ƒåˆ¤å®šé€»è¾‘ï¼Œä¸åŠ¨åˆ†æï¼Œåªè§„èŒƒè¾“å‡ºç»“æ„
    sys = (
      "ä½ æ˜¯ Selfy AI çš„æ˜“ç»è§‚ç›¸åŠ©æ‰‹ï¼ˆv3.7.2 é£æ ¼ï¼‰ã€‚"
      "ä¸¥æ ¼æŒ‰â€œä¸‰è±¡å››æ®µå¼â€åˆ†æï¼šã€å§¿æ€/ç¥æƒ…/é¢å®¹ã€‘ä¸‰éƒ¨åˆ†ã€‚æ¯éƒ¨åˆ†å¿…é¡»åŒ…å«ï¼š"
      "1) è¯´æ˜ï¼š1å¥ï¼Œå®¢è§‚æç»˜å¤–è§‚/åŠ¨ä½œ/æ°”è´¨ï¼›"
      "2) å¦è±¡ï¼šä»…å†™ä¸€ä¸ªå¦åï¼ˆè‰®/ç¦»/å…‘/ä¹¾/å¤/éœ‡/å·½/åï¼‰ï¼›"
      "3) è§£è¯»ï¼š1â€“2å¥ï¼ŒåŸºäºå¦è±¡ä¸è§‚å¯Ÿåšå«ä¹‰é˜é‡Šï¼›"
      "4) æ€§æ ¼å€¾å‘ï¼š1â€“2å¥ï¼Œç‹¬ç«‹æˆæ®µï¼Œä¸è¦ä¸â€œè§£è¯»â€é‡å¤æªè¾ã€‚"
      "ç„¶åç»™å‡ºï¼š"
      "5) å¦è±¡ç»„åˆï¼šæ ‡é¢˜=ä¸‰è±¡å¦åç›¸åŠ ï¼ˆå¦‚â€œè‰® + ç¦» + å…‘â€ï¼‰ï¼Œæ­£æ–‡ 90â€“150 å­—ï¼ˆå¯ä¸ä¸‰è±¡ç»“è®ºé€‚åº¦é‡åˆï¼‰ï¼›"
      "6) æ€»ç»“æ€§æ ¼å°è±¡ï¼š20â€“40å­—ï¼Œé¿å…æ¨¡æ¿åŒ–ï¼›"
      "7) äººæ ¼æ ‡ç­¾ archetypeï¼š2â€“5å­—ä¸­æ–‡ï¼Œå¦‚â€œå¤–å†·å†…çƒ­/ä¸»å¯¼å‹/è°¨æ…å‹â€ã€‚"
      "é¢ç›¸éœ€æ‹†æˆäº”å®˜ï¼šåœ¨ meta.face_parts ä¸­ï¼Œç»™ã€çœ‰/çœ¼/é¼»/å˜´/é¢§/ä¸‹å·´ã€‘ï¼ˆä»»é€‰5é¡¹è¦†ç›–ï¼‰å„å†™â€œç‰¹å¾ï¼ˆå¤–è§‚ï¼‰â€ä¸â€œè§£è¯»ï¼ˆåŸºäºæ˜“ç»ï¼‰â€ã€‚"
      "domains ä»…ä» ['é‡‘é’±ä¸äº‹ä¸š','é…å¶ä¸æ„Ÿæƒ…'] é€‰æ‹©ï¼›åœ¨ meta.domains_detail ä¸­åˆ†åˆ«å†™ 60â€“90 å­—å»ºè®®æ–‡æœ¬ã€‚"
      "å°†ç»“æœé€šè¿‡ submit_analysis_v3 å·¥å…·è¿”å›ï¼Œå¹¶"+_json_hint()+"ã€‚è¯­è¨€ï¼šä¸­æ–‡ã€‚æœ¬æ¶ˆæ¯å«â€œJSONâ€ä»¥æ»¡è¶³ API è¦æ±‚ã€‚"
    )
    user = "è¯·æŒ‰ 3.7.2 é£æ ¼åˆ†æå›¾ç‰‡ï¼Œä¸¥æ ¼é€šè¿‡å‡½æ•°è¿”å› JSONï¼ˆä¸è¦è¾“å‡ºè‡ªç”±æ–‡æœ¬ï¼‰ã€‚"
    return [{"role":"system","content":sys},{"role":"user","content":user}]

# ===== æ–‡æœ¬åå¤„ç†ï¼šå»ä»£è¯ / å»å¤è¯» / äººè¯åŒ– =====

DOMAIN_LEADS = r"(åœ¨(é‡‘é’±ä¸äº‹ä¸š|é…å¶ä¸æ„Ÿæƒ…|äº‹ä¸š|æ„Ÿæƒ…)(æ–¹é¢|ä¸­|é‡Œ)?|ç›®å‰|è¿‘æœŸ|å½“ä¸‹)"

def _depronoun(s: str) -> str:
    """å»æ‰â€œä»–/å¥¹/TA/ä½ /å…¶/å¯¹æ–¹/åœ¨â€¦ä¸Š/ä¸­/ç›®å‰/è¿‘æœŸâ€ç­‰å£å¤´èµ·å¥ï¼Œä½¿è¯­å¥å®¢è§‚ä¸­æ€§"""
    if not isinstance(s, str): return s
    s = s.strip()
    s = re.sub(r"^(ä»–|å¥¹|TA|ä½ |å¯¹æ–¹|å…¶)(çš„)?[ï¼Œã€ï¼š ]*", "", s)
    s = re.sub(r"^(åœ¨(äº‹ä¸š|æ„Ÿæƒ…|ç”Ÿæ´»)[ä¸Šä¸­]|ç›®å‰|è¿‘æœŸ)[ï¼Œã€ï¼š ]*", "", s)
    return s

def _neutralize(s: str) -> str:
    """å…¨å±€å»äººç§°ä»£è¯/åœºæ™¯å£å¤´è¯­/å¼±åŒ–è¯ï¼›æŠŠ'å¥¹å¯èƒ½/ä»–ä¼š'ç­‰æ”¹ä¸ºä¸­æ€§åˆ¤æ–­"""
    if not isinstance(s, str): return s
    s = s.strip()
    s = re.sub(r"(ä»–|å¥¹|TA|å¯¹æ–¹|å…¶)(çš„)?", "", s)
    s = re.sub(DOMAIN_LEADS + r"[ï¼Œã€ï¼š ]*", "", s)
    s = re.sub(r"(å¯èƒ½|æˆ–è®¸|ä¹Ÿè®¸)[ï¼Œã€ ]*", "", s)
    s = re.sub(r"[ï¼›;]+", "ï¼›", s)
    s = re.sub(r"[ï¼Œ,]{2,}", "ï¼Œ", s)
    return s.strip("ï¼›ï¼Œã€‚ ")

def _dedupe_phrase(s: str) -> str:
    """ä»¥é€—å·/å¥å·åˆ‡åˆ†åšæœ‰åºå»é‡ï¼Œé¿å…â€œå¤è¯»æœºâ€"""
    if not isinstance(s, str): return s
    parts = re.split(r"[ï¼Œ,ã€‚\.]", s)
    seen, kept = set(), []
    for p in parts:
        t = p.strip()
        if not t: continue
        if t not in seen:
            seen.add(t)
            kept.append(t)
    out = "ï¼Œ".join(kept)
    out = re.sub(r"(ï¼Œ){2,}", "ï¼Œ", out).strip("ï¼Œ")
    return out

def _strip_domain_lead(s: str) -> str:
    """ä¸“é—¨å»æ‰é¢†åŸŸå£å¤´åŒ–å¼•å¯¼è¯­ï¼šåœ¨é‡‘é’±ä¸äº‹ä¸šæ–¹é¢â€¦ / åœ¨æ„Ÿæƒ…ä¸­â€¦"""
    if not isinstance(s, str): return s
    return re.sub("^" + DOMAIN_LEADS + r"[ï¼Œã€ï¼š ]*", "", s.strip())

def _first_clause(s: str, maxlen: int = 20) -> str:
    """å–ä¸€ä¸ªå®Œæ•´å­å¥ï¼ˆåˆ°ç¬¬ä¸€ä¸ªæ ‡ç‚¹ä¸ºæ­¢ï¼‰ï¼Œæœ€é•¿ maxlenï¼Œé¿å…åŠå¥è¢«æˆªæ–­"""
    if not isinstance(s, str): return s
    s = s.strip()
    m = re.split(r"[ã€‚ï¼›ï¼›;ï¼Œ,]", s, maxsplit=1)
    head = (m[0] or "").strip()
    if len(head) > maxlen:
        head = head[:maxlen].rstrip("ï¼Œ,ã€‚ï¼›ï¼› ")
    return head

# ----- OpenAI è°ƒç”¨ -----

def _inflate_dotted_keys(obj: Dict[str, Any]) -> Dict[str, Any]:
    if not isinstance(obj, dict): return obj
    out: Dict[str, Any] = {}
    for k,v in obj.items():
        if "." not in k:
            out[k] = _inflate_dotted_keys(v) if isinstance(v, dict) else v
    for k,v in obj.items():
        if isinstance(k, str) and "." in k:
            head, tail = k.split(".", 1)
            base = out.setdefault(head, {})
            if not isinstance(base, dict): base = {}; out[head] = base
            cur = base
            parts = tail.split(".")
            for i, p in enumerate(parts):
                if i == len(parts)-1:
                    cur[p] = v
                else:
                    cur = cur.setdefault(p, {})
    for k in list(out.keys()):
        if isinstance(out[k], dict):
            out[k] = _inflate_dotted_keys(out[k])
    return out


def _call_openai(messages):
    return client.chat.completions.create(
        model="gpt-4o",
        temperature=0.4,
        tools=_build_tools_schema(),
        tool_choice={"type":"function","function":{"name":"submit_analysis_v3"}},
        response_format={"type":"json_object"},
        messages=messages
    )

# ---------- Synthesis helpers ----------
# æ³¨ï¼šä»¥ä¸‹è¯ä¹‰ç”¨äºâ€œç»„åˆæ¨å¯¼â€çš„è¯­ä¹‰åº“ï¼ˆéæ¨¡æ¿ï¼‰ã€‚
HEX_SUMMARY = {
    "ä¹¾":"è‡ªä¿¡Â·é¢†å¯¼Â·æœæ–­",     # ä¹¾ä¸ºå¤©ï¼šåˆšå¥ã€è‡ªå¼ºã€ä¸»å¯¼
    "å¤":"åŒ…å®¹Â·ç¨³å®šÂ·æ‰¿è½½",     # å¤ä¸ºåœ°ï¼šåšå¾·ã€æŸ”é¡ºã€æ‰¿è½½
    "éœ‡":"è¡ŒåŠ¨Â·çªç ´Â·èµ·åŠ¿",     # éœ‡ä¸ºé›·ï¼šå‘åŠ¨ã€å¼€æ‹“ã€å†³æ–­
    "å·½":"åè°ƒÂ·æ¸—é€Â·è¯´æœ",     # å·½ä¸ºé£ï¼šå…¥è€Œä¸äº‰ã€è°ƒå’Œã€å½±å“
    "å":"è°¨æ…Â·æ¢æ·±Â·æ™ºè°‹",     # åä¸ºæ°´ï¼šé™©ä¸­æ±‚ã€è°‹ç•¥ã€æ±‚è¯
    "ç¦»":"æ˜æ™°Â·è¡¨è¾¾Â·æ´å¯Ÿ",     # ç¦»ä¸ºç«ï¼šæ˜è¾¨ã€æ´å¯Ÿã€è¡¨è¾¾
    "è‰®":"æ­¢å®šÂ·è¾¹ç•ŒÂ·ç¨³é‡",     # è‰®ä¸ºå±±ï¼šå½“æ­¢åˆ™æ­¢ã€ç«‹ç•Œã€ç¨³å®ˆ
    "å…‘":"äº²å’ŒÂ·äº¤æµÂ·æ‚¦äºº"      # å…‘ä¸ºæ³½ï¼šè¯´ä¹Ÿã€å’Œæ‚¦ã€æ²Ÿé€š
}

# äº”è¡Œ&é˜´é˜³ï¼ˆç”¨äºå¦é—´å…³ç³»ï¼‰
WUXING = {
    "ä¹¾":{"element":"é‡‘","polarity":"é˜³","virtue":"åˆšå¥è‡ªå¼ºã€å¾¡é¢†å±€é¢"},
    "å…‘":{"element":"é‡‘","polarity":"é˜´","virtue":"å’Œæ‚¦äº²å’Œã€ä»¥ä¹æ„Ÿäºº"},
    "ç¦»":{"element":"ç«","polarity":"é˜´","virtue":"æ˜è¾¨æ´å¯Ÿã€æ“…äºè¡¨è¾¾"},
    "éœ‡":{"element":"æœ¨","polarity":"é˜³","virtue":"å‘åŠ¨èµ·åŠ¿ã€æ•¢äºçªç ´"},
    "å·½":{"element":"æœ¨","polarity":"é˜´","virtue":"æ¸—é€åè°ƒã€å–„è°‹åˆä¼—"},
    "å":{"element":"æ°´","polarity":"é˜³","virtue":"å®¡æ…æ¢æ·±ã€å±…å®‰è¯†å±"},
    "è‰®":{"element":"åœŸ","polarity":"é˜³","virtue":"æ­¢å®šæœ‰åº¦ã€å®ˆæ­£ç«‹ç•Œ"},
    "å¤":{"element":"åœŸ","polarity":"é˜´","virtue":"åšå¾·è½½ç‰©ã€å†…æ•›æ‰¿è½½"}
}

# äº”è¡Œç›¸ç”Ÿ/ç›¸å…‹è¡¨
SHENG = {"æœ¨":"ç«","ç«":"åœŸ","åœŸ":"é‡‘","é‡‘":"æ°´","æ°´":"æœ¨"}
KE    = {"æœ¨":"åœŸ","åœŸ":"æ°´","æ°´":"ç«","ç«":"é‡‘","é‡‘":"æœ¨"}

def _rel(a: str, b: str) -> str:
    """è¿”å› aâ†’b çš„å…³ç³»è¯ï¼šç›¸ç”Ÿ/ç›¸å…‹/åŒæ°”"""
    if not a or not b: return ""
    if a == b: return "åŒæ°”ç›¸æ±‚"
    if SHENG.get(a) == b: return "ç›¸ç”Ÿ"
    if KE.get(a) == b: return "ç›¸å…‹"
    return "ç›¸å¹¶"

def _style_by_main(h: str) -> str:
    """ä¸»å¦å®šæ•´ä½“è¡Œäº‹é£æ ¼ï¼ˆæ€»ç»“å¥ç”¨ï¼‰"""
    if h in ("ä¹¾","éœ‡"): return "æ•´ä½“åè¿›å–ä¸ä¸»å¯¼ï¼Œå®œæŠŠæ¡æ–¹å‘ã€ä¸»åŠ¨å¼€å±€"
    if h in ("å¤","è‰®"): return "æ•´ä½“åç¨³å®ˆä¸æ‰¿è½½ï¼Œå®œåšç§¯è–„å‘ã€ç¨³ä¸­æ±‚è¿›"
    if h in ("ç¦»",):     return "æ•´ä½“åæ˜è¾¨ä¸è¡¨è¾¾ï¼Œå®œå…¬å¼€å¤ç›˜ã€ä»¥æ¸…æ™°ä¿ƒæ¨è¿›"
    if h in ("å…‘",):     return "æ•´ä½“åäº²å’Œä¸æ²Ÿé€šï¼Œå®œä»¥åˆä¼—ä¹‹åŠ›è¾¾æˆç›®æ ‡"
    if h in ("å·½",):     return "æ•´ä½“ååè°ƒä¸æ¸—é€ï¼Œå®œæŸ”æ€§æ¨è¿›ã€æ¶¦ç‰©æ— å£°"
    if h in ("å",):     return "æ•´ä½“åå®¡æ…ä¸è°‹ç•¥ï¼Œå®œå…ˆæ±‚è¯ååˆ¤æ–­ã€æ­¥æ­¥ä¸ºè¥"
    return "è¡Œäº‹é£æ ¼å¹³è¡¡ï¼Œå®œé¡ºåŠ¿è€Œä¸º"

def _synthesize_combo(hexes: List[str], ta: Dict[str,Any], traits: List[str]) -> str:
    """
    æ˜“ç»å¼â€œä¸»/è¾…/åŸºâ€ç‹¬ç«‹æ¨å¯¼ï¼ˆä¸å¤ç”¨ä¸‰åˆ†è±¡æ–‡æœ¬ï¼‰ï¼š
    - ä¸»ï¼ˆå§¿æ€ï¼‰å®šå¤§åŠ¿ï¼›è¾…ï¼ˆç¥æƒ…ï¼‰è°ƒå…¶ç”¨ï¼›åŸºï¼ˆé¢å®¹ï¼‰å®šå…¶æ ¹ã€‚
    - å¼•å…¥äº”è¡Œä¸é˜´é˜³ï¼Œåˆ†æä¸»-è¾…ã€åŸº-ä¸»çš„å…³ç³»ï¼ˆç›¸ç”Ÿ/ç›¸å…‹/åŒæ°”/ç›¸å¹¶ï¼‰ã€‚
    - è¾“å‡ºä¸ºä¸€æ®µæ€»ç»“æ€§å æ–­ï¼šä¸æ‹¼ä¸‰åˆ†è±¡åŸå¥ã€‚
    """
    zh, sh, bh = (hexes + ["", "", ""])[:3]
    keys = [h for h in [zh, sh, bh] if h]
    if not keys: return ""

    def vw(h, key): 
        return (WUXING.get(h) or {}).get(key, "")
    def hs(h): 
        return HEX_SUMMARY.get(h, "")

    # 1) å¦å¾·å¼€ç¯‡ï¼šä¸»/è¾…/åŸº + äº”è¡Œ/å¾·æ€§
    parts = []
    for role, h in (("ä¸»", zh), ("è¾…", sh), ("åŸº", bh)):
        if not h: continue
        ele = vw(h,"element"); pol = vw(h,"polarity"); vir = vw(h,"virtue")
        sym = BAGUA_SYMBOLS.get(h,"")
        seg = f"{role}{h}ï¼ˆ{sym}ï¼‰ï¼Œå±{ele}ä¸º{pol}ï¼Œ{vir}"
        parts.append(seg)

    lead = "ï¼›".join(parts) + "ã€‚"

    # 2) å¦é—´å…³ç³»ï¼šä¸»-è¾…ã€åŸº-ä¸»
    rel1 = _rel(vw(zh,"element"), vw(sh,"element")) if zh and sh else ""
    rel2 = _rel(vw(bh,"element"), vw(zh,"element")) if bh and zh else ""

    rel_texts = []
    if rel1:
        if rel1 == "ç›¸ç”Ÿ": rel_texts.append("ä¸»è¾…ç›¸ç”Ÿï¼Œæ°”åŠ¿é¡ºç•…ï¼Œç”¨åˆšå¾—æŸ”")
        elif rel1 == "ç›¸å…‹": rel_texts.append("ä¸»è¾…ç›¸å…‹ï¼Œå®œè°ƒå’Œæ”¶æ”¾ï¼Œä»¥å…ç”¨åŠ›è¿‡åº¦")
        elif rel1 == "åŒæ°”ç›¸æ±‚": rel_texts.append("ä¸»è¾…åŒæ°”ï¼Œé£æ ¼çº¯ç²¹ä½†éœ€é˜²åæ‰§")
        else: rel_texts.append("ä¸»è¾…ç›¸å¹¶ï¼Œå„è¡Œå…¶åŠ¿ï¼Œéœ€ä»¥èŠ‚å¥ç»Ÿæ‘„")
    if rel2:
        if rel2 == "ç›¸ç”Ÿ": rel_texts.append("åŸºç”Ÿä¸»ï¼Œæ ¹åŸºä¾›ç»™ï¼ŒååŠ²å……è¶³")
        elif rel2 == "ç›¸å…‹": rel_texts.append("åŸºå…‹ä¸»ï¼Œå†…åœ¨æ‹‰æ‰¯ï¼Œå®œå…ˆç¨³æ ¹å†è¡ŒåŠ¿")
        elif rel2 == "åŒæ°”ç›¸æ±‚": rel_texts.append("åŸºä¸ä¸»åŒæ°”ï¼Œæ‰€å®ˆæ‰€ä¸ºä¸€è‡´")
        else: rel_texts.append("åŸºä¸ä¸»ç›¸å¹¶ï¼Œå®œä»¥è§„åˆ™çº¦æŸä»¥æˆåº")

    rel_para = (" ".join(rel_texts) + "ã€‚") if rel_texts else ""

    # 3) å¦æ„æ€»æ‹¬ï¼šå‚è€ƒä¸»å¦é£æ ¼æ”¶æŸ
    style = _style_by_main(zh) if zh else "é£æ ¼å¹³è¡¡ï¼Œå®œé¡ºåŠ¿è€Œä¸º"

    # 4) æ”¶æ•›æˆä¸€æ®µç‹¬ç«‹æ€»ç»“
    out = f"ä¸‰è±¡ç›¸åˆï¼š{lead}{rel_para}{style}ã€‚"
    out = re.sub(r"[ï¼›;]+", "ï¼›", out)
    out = _dedupe_phrase(out)
    return out

def _insight_for_domains(hexes: List[str]) -> Dict[str, str]:
    """åŸºäºå¦è±¡ç»™â€œè¿‘æœŸçŠ¶æ€â€çš„è¦ç‚¹ï¼Œå¼±æ¨¡æ¿ã€å¼ºå¦æ„ï¼ˆæ›´åƒå³æ—¶è¡¨è¾¾ï¼‰"""
    s = set([h for h in hexes if h])
    biz = []
    if "ä¹¾" in s or "éœ‡" in s: biz.append("æ¨è¿›æœ‰åŠ›ã€èŠ‚å¥å‘å‰")
    if "ç¦»" in s: biz.append("è¡¨è¾¾æ¸…æ¥šã€å¤ç›˜åˆ°ä½")
    if "å…‘" in s or "å·½" in s: biz.append("å–„è°ˆåˆ¤ååŒã€èƒ½å¸¦åŠ¨äºº")
    if "å¤" in s or "è‰®" in s: biz.append("è½åœ°ç¨³ã€è¾¹ç•Œæ˜ã€æŠ—å¹²æ‰°")
    if "å" in s: biz.append("é£é™©æ„è¯†å¼ºã€æ–¹æ¡ˆç•™åæ‰‹")

    love = []
    if "å…‘" in s: love.append("æ°›å›´è½»æ¾ã€äº’åŠ¨è‡ªç„¶")
    if "ç¦»" in s: love.append("å–„è¡¨è¾¾æƒ³æ³•ã€å…±æƒ…åˆ°ä½")
    if "å¤" in s: love.append("é‡æ‰¿è¯ºä¸ç…§é¡¾")
    if "å" in s: love.append("åœ¨æ„å®‰å…¨æ„Ÿã€è¾ƒæ•æ„Ÿ")
    if "éœ‡" in s or "ä¹¾" in s: love.append("å…³é”®æ—¶ä¼šä¸»åŠ¨")
    if "è‰®" in s: love.append("ä¿æŒåˆ†å¯¸ä¸ç¨³å®š")
    return {"äº‹ä¸š": "ï¼›".join(biz), "æ„Ÿæƒ…": "ï¼›".join(love)}

def _merge_status_and_detail(status: str, detail: str) -> str:
    """åˆå¹¶â€œçŠ¶æ€è¦ç‚¹ + æ¨¡å‹æ®µé¦–å¥â€ï¼Œå½»åº•å»ä»£è¯/å»é¢†åŸŸå¼•å¯¼è¯­/å»å¤è¯»"""
    detail_first = detail.split("ã€‚")[0].strip() if detail else ""
    detail_first = _neutralize(_strip_domain_lead(detail_first))
    status = _neutralize(_strip_domain_lead(status or ""))
    parts = [p for p in [status, detail_first] if p]
    text = "ï¼›".join(parts).rstrip("ï¼›")
    return _dedupe_phrase(text)

def _imperative_suggestion(detail: str, hexes: List[str], domain: str) -> str:
    """
    ä»¥å¦è±¡å¯¼å‘ç”Ÿæˆâ€œå¯æ‰§è¡Œå»ºè®®â€ï¼Œé¿å…åƒç¯‡ä¸€å¾‹ï¼›è¾“å‡ºç”¨ä¸­æ€§å®¢è§‚è¡¨è¾¾ã€‚
    """
    if not detail: detail = ""
    s = set([h for h in hexes if h])
    tips = []
    if domain == "äº‹ä¸š":
        if "ä¹¾" in s or "éœ‡" in s: tips.append("æŠŠé˜¶æ®µç›®æ ‡æ‹‰æ¸…æ¥šï¼Œä»Šå¤©å°±æ¨è¿›ä¸€å°æ­¥")
        if "ç¦»" in s: tips.append("æŠŠå¤ç›˜å…¬å¼€å‡ºæ¥ï¼Œç”¨æ•°æ®è¯´è¯")
        if "å…‘" in s or "å·½" in s: tips.append("çº¦ä¸€åœºå…³é”®ååŒï¼Œå…ˆæ¢ä½å†è°ˆç›®æ ‡")
        if "å¤" in s or "è‰®" in s: tips.append("å®šè¾¹ç•Œä¸èŠ‚å¥ï¼Œä¸æŠ¢ä¸æ‹–")
        if "å" in s: tips.append("åˆ—å‡ºå‰ä¸‰ä¸ªé£é™©ï¼Œå‡†å¤‡Bè®¡åˆ’")
    else:
        if "å…‘" in s: tips.append("ç”¨è½»æ¾è¯­æ°”å›åº”ï¼ŒåŠæ—¶ç»™åé¦ˆ")
        if "å¤" in s: tips.append("æŠŠåœ¨æ„çš„äº‹è¯´æ¸…æ¥šï¼Œå¹¶å…‘ç°æ‰¿è¯º")
        if "ç¦»" in s: tips.append("ç›´è¯´çœŸå®æƒ³æ³•ï¼Œä¹Ÿè¯´æ¸…ç•Œé™")
        if "éœ‡" in s or "ä¹¾" in s: tips.append("é‡è¦èŠ‚ç‚¹åˆ«çŠ¹è±«ï¼Œä¸»åŠ¨ä¸€ç‚¹")
        if "å" in s: tips.append("åˆ«å…ˆå…¥ä¸ºä¸»ï¼Œå¤šæ±‚è¯å†åˆ¤æ–­")
        if "è‰®" in s: tips.append("å°Šé‡å½¼æ­¤èŠ‚å¥ï¼Œä¿ç•™å„è‡ªç©ºé—´")

    base = _neutralize(_strip_domain_lead(detail.strip())).rstrip("ï¼›")
    add = "ï¼›".join(tips[:3])
    out = (base + ("ã€‚å»ºè®®ï¼š" if base else "å»ºè®®ï¼š") + add + "ã€‚") if add else base
    out = re.sub(r"[ï¼›;]+", "ï¼›", out)
    return _dedupe_phrase(out)

def _collect_traits_and_merge(ta: Dict[str,Any]) -> (List[str], Dict[str,Any]):
    """æ”¶é›†ä¸‰è±¡é‡Œçš„'æ€§æ ¼å€¾å‘'ï¼Œå¹¶æŠŠæ¯è±¡çš„â€˜è¯´æ˜+è§£è¯»â€™åˆå¹¶ä¸ºä¸€å¥"""
    traits = []
    new_ta = {}
    for key in ["å§¿æ€","ç¥æƒ…","é¢å®¹"]:
        o = (ta.get(key) or {}).copy()
        tend = (o.get("æ€§æ ¼å€¾å‘") or "").strip().rstrip("ï¼›;ã€‚")
        if tend: traits.append(tend)
        desc = (o.get("è¯´æ˜") or "")
        inter = (o.get("è§£è¯»") or "")
        merged = _combine_sentence(desc, inter)
        o["è¯´æ˜"] = desc.strip().rstrip("ï¼›;ã€‚")
        o["è§£è¯»"] = merged.strip()
        o["æ€§æ ¼å€¾å‘"] = ""
        new_ta[key] = o
    for k in ta.keys():
        if k not in new_ta:
            new_ta[k] = ta[k]
    return traits, new_ta

def _combine_sentence(desc: str, interp: str) -> str:
    """åˆå¹¶â€˜è¯´æ˜ + è§£è¯»â€™ï¼Œå¼ºå»ä»£è¯/å»å¤è¯»ï¼Œè®©å¥å­æ›´åƒäººè¯´è¯"""
    if not desc and not interp: return ""
    desc = _neutralize(_depronoun((desc or "").strip().rstrip("ï¼›;ã€‚")))
    interp = _neutralize(_depronoun((interp or "").strip().lstrip("â€”â€”").lstrip("- ").strip().rstrip("ï¼›;ã€‚")))
    interp = re.sub(r"^(è¿™ç§|æ­¤ç±»|è¿™ç±»|å…¶|è¿™ç§å§¿æ€|è¿™ç§ç¥æƒ…|è¿™ç§é¢å®¹)[ï¼Œã€ï¼š ]*", "", interp)
    s = f"{desc}ï¼Œ{interp}ã€‚" if (desc and interp) else f"{desc or interp}ã€‚"
    s = re.sub(r"[ï¼›;]+", "ï¼Œ", s)
    s = re.sub(r"ï¼Œï¼Œ+", "ï¼Œ", s)
    s = _dedupe_phrase(s)
    return s

def _coerce_output(data: Dict[str,Any]) -> Dict[str,Any]:
    out = dict(data)
    meta = out.get("meta") or {}
    if not isinstance(meta, dict): meta = {}
    out["meta"] = meta

    ta = meta.get("triple_analysis") or {}
    traits, ta = _collect_traits_and_merge(ta)
    meta["triple_analysis"] = ta

    hexes = [(ta.get("å§¿æ€") or {}).get("å¦è±¡",""),
             (ta.get("ç¥æƒ…") or {}).get("å¦è±¡",""),
             (ta.get("é¢å®¹") or {}).get("å¦è±¡","")]
    combo_title = " + ".join([h for h in hexes if h])
    meta["combo_title"] = combo_title

    synthesized = _synthesize_combo(hexes, ta, traits)
    one = (ta.get("æ€»ç»“") or out.get("summary",""))
    overview = (synthesized or one).strip().rstrip("ï¼›;")
    meta["overview_card"] = {"title": f"ğŸ”® å¦è±¡ç»„åˆï¼š{combo_title}" if combo_title else "ğŸ”® å¦è±¡ç»„åˆ",
                             "summary": overview}

    try:
        out["confidence"] = float(out.get("confidence",0.0))
    except Exception:
        out["confidence"] = 0.0
    arch = (out.get("archetype") or "").strip()
    meta["headline"] = {"tag": arch, "confidence": out["confidence"]}

    dd = meta.get("domains_detail") or {}
    status = _insight_for_domains(hexes)
    merged_status = {
        "äº‹ä¸š": _merge_status_and_detail(status.get("äº‹ä¸š",""), dd.get("é‡‘é’±ä¸äº‹ä¸š","")),
        "æ„Ÿæƒ…": _merge_status_and_detail(status.get("æ„Ÿæƒ…",""), dd.get("é…å¶ä¸æ„Ÿæƒ…","")),
    }
    meta["domains_status"] = merged_status
    meta["domains_suggestion"] = {
        "äº‹ä¸š": _imperative_suggestion(dd.get("é‡‘é’±ä¸äº‹ä¸š",""), hexes, "äº‹ä¸š"),
        "æ„Ÿæƒ…": _imperative_suggestion(dd.get("é…å¶ä¸æ„Ÿæƒ…",""), hexes, "æ„Ÿæƒ…")
    }

    def _clean(s):
        if not isinstance(s, str): return s
        s = s.replace("â€”â€”", "ï¼Œ")
        s = re.sub(r"[ï¼›;]+", "ï¼›", s)
        s = re.sub(r"ï¼›([ã€‚ï¼])", r"\1", s)
        s = re.sub(r"([ã€‚ï¼ï¼Ÿ])ï¼›", r"\1", s)
        s = _depronoun(s)
        s = _neutralize(s)
        s = _dedupe_phrase(s)
        return s

    out["summary"] = _clean(out.get("summary",""))
    out["archetype"] = _clean(out.get("archetype",""))

    def _deep_clean(x):
        if isinstance(x, dict):
            return {k:_deep_clean(v) for k,v in x.items()}
        if isinstance(x, list):
            return [_deep_clean(v) for v in x]
        return _clean(x)

    out["meta"] = _deep_clean(meta)
    return out

# ---------------- routes ----------------
@app.get("/health")
def health(): return {"status":"ok"}

@app.get("/", include_in_schema=False)
def root():
    return HTMLResponse("<h3>Selfy AI</h3><a href='/docs'>/docs</a> Â· <a href='/mobile'>/mobile</a>")

@app.head("/", include_in_schema=False)
def root_head():
    return Response(status_code=200)

@app.get("/version")
def version(): return {"runtime":RUNTIME_VERSION,"analysis":ANALYSIS_VERSION,"schema":SCHEMA_ID,"debug":DEBUG}

@app.get("/mobile", include_in_schema=False)
def mobile():
    path = os.path.join(os.path.dirname(__file__), "index_mobile.html")
    try:
        html = open(path, "r", encoding="utf-8").read()
    except Exception as e:
        return HTMLResponse(f"<pre>index_mobile.html not found: {e}</pre>", status_code=500)
    return HTMLResponse(html)

def _call_gpt(messages):
    if client is None:
        raise RuntimeError("OpenAI client not initialized")
    return client.chat.completions.create(
        model="gpt-4o",
        temperature=0.4,
        tools=_build_tools_schema(),
        tool_choice={"type":"function","function":{"name":"submit_analysis_v3"}},
        response_format={"type":"json_object"},
        messages=messages
    )

def _call_gpt_tool_with_image(data_url: str) -> Dict[str,Any]:
    messages = _prompt_for_image_v372()
    messages[-1]["content"] = [
        {"type":"text","text":messages[-1]["content"]},
        {"type":"image_url","image_url":{"url":data_url}}
    ]
    resp = _call_gpt(messages)
    choice = resp.choices[0]
    tool_calls = getattr(choice.message, "tool_calls", None)
    if tool_calls:
        args = json.loads(tool_calls[0].function.arguments)
    else:
        content = getattr(choice.message, "content", None)
        if isinstance(content, str) and content.strip().startswith("{"):
            args = json.loads(content)
        else:
            raise RuntimeError("Model did not return tool_calls.")
    return {"tool_args": args, "oai_raw": resp if DEBUG else None}

@app.post("/upload")
async def upload(file: UploadFile = File(...)):
    try:
        if not file: raise HTTPException(400,"No file")
        ct = file.content_type or ""
        if not ct.startswith("image/"): raise HTTPException(415,f"Unsupported content type: {ct}")
        raw = await file.read()
        if not raw: raise HTTPException(400,"Empty file")
        if len(raw) > 15*1024*1024: raise HTTPException(413,"File too large (>15MB)")

        data_url = _to_data_url(raw, ct)
        logger.info("[UPLOAD] %s %dB %s", file.filename, len(raw), ct)

        result = _call_gpt_tool_with_image(data_url)
        tool_args = result["tool_args"]
        final_out = _coerce_output(tool_args)

        if DEBUG:
            meta = final_out.setdefault("meta",{}).setdefault("debug",{})
            meta["file_info"]={"filename":file.filename,"content_type":ct,"size":len(raw)}
            try:
                meta["oai_choice_finish_reason"]=result["oai_raw"].choices[0].finish_reason
            except Exception:
                meta["oai_choice_finish_reason"]="n/a"

        return JSONResponse(content=final_out, status_code=200)
    except HTTPException as he:
        if DEBUG:
            return JSONResponse(status_code=he.status_code, content={"error":he.detail,"debug":{"trace":traceback.format_exc()}})
        raise
    except Exception as e:
        logging.exception("upload failed: %s", e)
        body={"error":"Internal Server Error"}
        if DEBUG: body["debug"]={"message":str(e),"trace":traceback.format_exc()}
        return JSONResponse(status_code=500, content=body)
