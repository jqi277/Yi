# fastapi_app.py  (runtime v3.7.10, analysis logic v3.7.2 + post-processing)
import os, base64, json, logging, traceback, re
from typing import Dict, Any, List

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse, HTMLResponse, Response
from fastapi.middleware.cors import CORSMiddleware

from openai import OpenAI

RUNTIME_VERSION = "3.7.10"
ANALYSIS_VERSION = os.getenv("ANALYSIS_VERSION", "372").strip()
SCHEMA_ID = "selfy.v3"
DEBUG = str(os.getenv("DEBUG","0")).strip() in ("1","true","True","YES","yes")

logging.basicConfig(level=logging.DEBUG if DEBUG else logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger("selfy-ai")

app = FastAPI(title="Selfy AI - YiJing Analysis API", version=RUNTIME_VERSION)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

try:
    client = OpenAI()
except Exception as e:
    logger.error("OpenAI client init failed: %s", e); client=None

BAGUA_SYMBOLS = {"è‰®":"å±±","ç¦»":"ç«","å…‘":"æ³½","ä¹¾":"å¤©","å¤":"åœ°","éœ‡":"é›·","å·½":"é£","å":"æ°´"}

# ---------------- helpers ----------------
def _to_data_url(content: bytes, content_type: str) -> str:
    return f"data:{content_type};base64,{base64.b64encode(content).decode('utf-8')}"

def _build_tools_schema() -> List[Dict[str, Any]]:
    return [{
      "type":"function",
      "function":{
        "name":"submit_analysis_v3",
        "description":"Return end-user facing JSON for Selfy AI YiJing analysis.",
        "parameters":{
          "type":"object",
          "properties":{
            "summary":{"type":"string"},
            "archetype":{"type":"string"},
            "confidence":{"type":"number"},
            "sections":{"type":"object","properties":{"å§¿æ€":{"type":"string"},"ç¥æƒ…":{"type":"string"},"é¢ç›¸":{"type":"string"}},"required":["å§¿æ€","ç¥æƒ…","é¢ç›¸"],"additionalProperties":False},
            "domains":{"type":"array","items":{"type":"string"}},
            "meta":{"type":"object","additionalProperties":True}
          },
          "required":["summary","archetype","confidence","sections","domains"],
          "additionalProperties":False
        }
      }
    }]

def _json_hint() -> str:
    return ("åªä»¥ JSON object è¿”å›ï¼ˆå¿…é¡» JSONï¼‰ã€‚ç¤ºä¾‹:{\"summary\":\"â€¦\",\"archetype\":\"â€¦\",\"confidence\":0.9,"
            "\"sections\":{\"å§¿æ€\":\"â€¦\",\"ç¥æƒ…\":\"â€¦\",\"é¢ç›¸\":\"â€¦\"},"
            "\"domains\":[\"é‡‘é’±ä¸äº‹ä¸š\",\"é…å¶ä¸æ„Ÿæƒ…\"],"
            "\"meta\":{\"triple_analysis\":{\"å§¿æ€\":{\"è¯´æ˜\":\"â€¦\",\"å¦è±¡\":\"è‰®\",\"è§£è¯»\":\"â€¦\",\"æ€§æ ¼å€¾å‘\":\"â€¦\"},\"ç¥æƒ…\":{â€¦},\"é¢å®¹\":{â€¦},\"ç»„åˆæ„å¢ƒ\":\"â€¦\",\"æ€»ç»“\":\"â€¦\"},"
            "\"face_parts\":{\"çœ‰\":{\"ç‰¹å¾\":\"â€¦\",\"å¦è±¡\":\"â€¦\",\"è§£è¯»\":\"â€¦\"},\"çœ¼\":{â€¦},\"é¼»\":{â€¦},\"å˜´\":{â€¦},\"é¢§/ä¸‹å·´\":{â€¦}},"
            "\"domains_detail\":{\"é‡‘é’±ä¸äº‹ä¸š\":\"â€¦(60â€“90å­—)\",\"é…å¶ä¸æ„Ÿæƒ…\":\"â€¦(60â€“90å­—)\"}}}")

def _prompt_for_image_v372():
    sys = (
      "ä½ æ˜¯ Selfy AI çš„æ˜“ç»è§‚ç›¸åŠ©æ‰‹ï¼ˆv3.7.2 é£æ ¼ï¼‰ã€‚"
      "ä¸¥æ ¼æŒ‰â€œä¸‰è±¡å››æ®µå¼â€åˆ†æï¼šã€å§¿æ€/ç¥æƒ…/é¢å®¹ã€‘ä¸‰éƒ¨åˆ†ã€‚æ¯éƒ¨åˆ†å¿…é¡»åŒ…å«ï¼š"
      "1) è¯´æ˜ï¼š1å¥ï¼Œå®¢è§‚æç»˜å¤–è§‚/åŠ¨ä½œ/æ°”è´¨ï¼›"
      "2) å¦è±¡ï¼šä»…å†™ä¸€ä¸ªå¦åï¼ˆè‰®/ç¦»/å…‘/ä¹¾/å¤/éœ‡/å·½/åï¼‰ï¼›"
      "3) è§£è¯»ï¼š1â€“2å¥ï¼ŒåŸºäºå¦è±¡ä¸è§‚å¯Ÿåšå«ä¹‰é˜é‡Šï¼›"
      "4) æ€§æ ¼å€¾å‘ï¼š1â€“2å¥ï¼Œç‹¬ç«‹æˆæ®µï¼Œä¸è¦ä¸â€œè§£è¯»â€é‡å¤æªè¾ã€‚"
      "ç„¶åç»™å‡ºï¼š"
      "5) å¦è±¡ç»„åˆï¼šæ ‡é¢˜=ä¸‰è±¡å¦åç›¸åŠ ï¼ˆå¦‚â€œè‰® + ç¦» + å…‘â€ï¼‰ï¼Œæ­£æ–‡ 90â€“150 å­—ï¼ˆå¯ä¸ä¸‰è±¡ç»“è®ºé€‚åº¦é‡åˆï¼‰ï¼›"
      "6) æ€»ç»“æ€§æ ¼å°è±¡ï¼š20â€“40å­—ï¼Œé¿å…æ¨¡æ¿åŒ–ï¼›"
      "7) äººæ ¼æ ‡ç­¾ archetypeï¼š2â€“5å­—ä¸­æ–‡ï¼Œå¦‚â€œå¤–å†·å†…çƒ­/ä¸»å¯¼å‹/è°¨æ…å‹â€ã€‚"
      "é¢ç›¸éœ€æ‹†æˆäº”å®˜ï¼šåœ¨ meta.face_parts ä¸­ï¼Œç»™ã€çœ‰/çœ¼/é¼»/å˜´/é¢§/ä¸‹å·´ã€‘ï¼ˆä»»é€‰5é¡¹è¦†ç›–ï¼‰å„å†™â€œç‰¹å¾ï¼ˆå¤–è§‚ï¼‰â€ä¸â€œè§£è¯»ï¼ˆåŸºäºæ˜“ç»ï¼‰â€ã€‚"
      "domains ä»…ä» ['é‡‘é’±ä¸äº‹ä¸š','é…å¶ä¸æ„Ÿæƒ…'] é€‰æ‹©ï¼›åœ¨ meta.domains_detail ä¸­åˆ†åˆ«å†™ 60â€“90 å­—å»ºè®®æ–‡æœ¬ã€‚"
      "å°†ç»“æœé€šè¿‡ submit_analysis_v3 å·¥å…·è¿”å›ï¼Œå¹¶"+_json_hint()+"ã€‚è¯­è¨€ï¼šä¸­æ–‡ã€‚æœ¬æ¶ˆæ¯å«â€œJSONâ€ä»¥æ»¡è¶³ API è¦æ±‚ã€‚"
    )
    user = "è¯·æŒ‰ 3.7.2 é£æ ¼åˆ†æå›¾ç‰‡ï¼Œä¸¥æ ¼é€šè¿‡å‡½æ•°è¿”å› JSONï¼ˆä¸è¦è¾“å‡ºè‡ªç”±æ–‡æœ¬ï¼‰ã€‚"
    return [{"role":"system","content":sys},{"role":"user","content":user}]

def _call_openai(messages):
    return client.chat.completions.create(
        model="gpt-4o",
        temperature=0.4,
        tools=_build_tools_schema(),
        tool_choice={"type":"function","function":{"name":"submit_analysis_v3"}},
        response_format={"type":"json_object"},
        messages=messages
    )

# ---------------- text post-processing ----------------
def _clean_punct(s: str) -> str:
    if not s: return s
    s = s.replace("â€”â€”", "ï¼Œ")
    s = re.sub(r"[ï¼›;]+\s*", "ï¼›", s)          # å‹ç¼©è¿ç»­åˆ†å·
    s = re.sub(r"[ã€‚\.]{2,}", "ã€‚", s)        # å‹ç¼©è¿ç»­å¥å·
    s = re.sub(r"\s*ï¼›\s*", "ï¼›", s)          # åˆ†å·ä¸¤ä¾§ç©ºæ ¼
    s = re.sub(r"(ï¼Œ|ã€)\\1+", r"\\1", s)     # å‹ç¼©è¿ç»­é€—å·/é¡¿å·
    s = re.sub(r"ï¼›\s*$", "ã€‚", s)            # å¥å°¾åˆ†å·â†’å¥å·
    return s.strip()

def _dedup_sentence_fragments(s: str) -> str:
    if not s: return s
    parts = re.split(r"[ã€‚ï¼›]", s)
    seen = set(); filtered = []
    for p in parts:
        p = p.strip()
        if not p: continue
        if p in seen: 
            continue
        seen.add(p); filtered.append(p)
    s = "ã€‚".join(filtered) + "ã€‚"
    clauses = [c.strip() for c in re.split(r"[ï¼Œ,]", s) if c.strip()]
    uniq = []; seen2 = set()
    for c in clauses:
        if c not in seen2:
            seen2.add(c); uniq.append(c)
    return "ï¼Œ".join(uniq).rstrip("ï¼Œ").replace("ã€‚ã€‚","ã€‚").rstrip("ï¼›") + "ã€‚"

def _merge_observe_and_interp(desc: str, interp: str) -> str:
    desc = (desc or "").strip().rstrip("ï¼›ã€‚")
    interp = (interp or "").strip().rstrip("ï¼›ã€‚")
    if not desc and not interp: return ""
    s = f"{desc}ï¼Œ{interp}" if desc and interp else (desc or interp)
    return _clean_punct(_dedup_sentence_fragments(s))

def _hex_meaning(h: str) -> List[str]:
    m = {
        "ä¹¾": ["è‡ªä¿¡","ä¸»å¯¼","æœæ–­"],
        "å¤": ["åŒ…å®¹","ç¨³é‡","æ‰¿è½½"],
        "å…‘": ["äº²å’Œ","äº¤æµ","æ‚¦äºº"],
        "ç¦»": ["æ˜æ™°","è¡¨è¾¾","æ´å¯Ÿ"],
        "è‰®": ["ç¨³å®ˆ","è¾¹ç•Œ","å®šåŠ›"],
        "éœ‡": ["è¡ŒåŠ¨","çªç ´","å¯åŠ¨"],
        "å·½": ["åå•†","æ¸—é€","æ•´åˆ"],
        "å": ["è°¨æ…","æ·±åº¦","é£é™©æ„è¯†"],
    }
    return m.get(h, [])

def _compose_combo_summary(hexes: List[str], traits: List[str]) -> str:
    kws = []
    for h in hexes:
        kws += _hex_meaning(h)
    # å»é‡ä¿åº
    seen = set(); kws2 = []
    for k in kws:
        if k not in seen:
            seen.add(k); kws2.append(k)
    head = "ã€".join(kws2[:6]) or "ç»¼åˆå‡è¡¡"
    tail = "ï¼›".join([t for t in traits if t][:2])  # å°‘é‡èåˆ
    sent = f"æ•´ä½“æ°”è´¨ä»¥{head}ä¸ºä¸»"
    if tail:
        sent += f"ï¼›{tail}"
    sent += "ã€‚"
    return _clean_punct(sent)

def _merge_status_and_detail(status: str, detail: str) -> str:
    first = ""
    if detail:
        first = detail.split("ã€‚")[0].strip()
        if re.match(r"^(å»ºè®®|å¯|å®œ|å°½é‡|é¿å…|ä¼˜å…ˆ|ä¸å¦¨|å°è¯•|ä¿æŒ|åŠ å¼º|æ˜ç¡®|åˆ¶å®š|å¤šä¸|å¤šç”¨|æå‰|å…³æ³¨|ç•™æ„|ä¸»åŠ¨)", first):
            first = ""
    chunks = [ch for ch in [status, first] if ch]
    s = "ï¼›".join(chunks)
    return _clean_punct(_dedup_sentence_fragments(s))

def _imperative_suggestion(detail: str) -> str:
    if not detail: return ""
    sents = [x.strip() for x in re.split(r"[ã€‚;ï¼›]", detail) if x.strip()]
    sugg = []
    for s in sents:
        if re.match(r"^(å»ºè®®|å¯|å®œ|å°½é‡|é¿å…|ä¼˜å…ˆ|ä¸å¦¨|å°è¯•|ä¿æŒ|åŠ å¼º|æ˜ç¡®|åˆ¶å®š|å¤šä¸|å¤šç”¨|æå‰|å…³æ³¨|ç•™æ„|ä¸»åŠ¨)", s):
            sugg.append(s)
    if not sugg:
        s = detail.strip()
        s = re.sub(r"(å¯ä»¥è€ƒè™‘|å¯ä»¥|é€‚åˆ|èƒ½å¤Ÿ|å¯èƒ½ä¼š|éœ€è¦|åº”å½“|æœ‰åŠ©äº)", "å»ºè®®", s)
        sugg = [s]
    text = "ï¼›".join(sugg)
    return _clean_punct(_dedup_sentence_fragments(text))

def _sanitize_block_text(s: str) -> str:
    return _clean_punct(_dedup_sentence_fragments(s or ""))

def _inflate_dotted_keys(obj: Dict[str, Any]) -> Dict[str, Any]:
    if not isinstance(obj, dict): return obj
    out: Dict[str, Any] = {}
    for k,v in obj.items():
        if "." not in k:
            out[k] = _inflate_dotted_keys(v) if isinstance(v, dict) else v
    for k,v in obj.items():
        if isinstance(k, str) and "." in k:
            head, tail = k.split(".", 1)
            base = out.setdefault(head, {})
            if not isinstance(base, dict): base = {}; out[head] = base
            cur = base
            parts = tail.split(".")
            for i, p in enumerate(parts):
                if i == len(parts)-1:
                    cur[p] = v
                else:
                    cur = cur.setdefault(p, {})
    for k in list(out.keys()):
        if isinstance(out[k], dict):
            out[k] = _inflate_dotted_keys(out[k])
    return out

# ---------------- main coercer ----------------
def _coerce_output_v372(data: Dict[str,Any]) -> Dict[str,Any]:
    data = _inflate_dotted_keys(data)
    out = dict(data)
    meta = out.get("meta") or {}
    if not isinstance(meta, dict): meta = {}
    out["meta"] = meta

    ta = meta.get("triple_analysis") or {}
    # åˆå¹¶è¯´æ˜+è§£è¯»ä¸ºä¸€å¥ï¼Œå¹¶æ¸…ç†é‡å¤æ ‡ç‚¹ï¼›åŒæ—¶æ”¶é›†â€œæ€§æ ¼å€¾å‘â€
    traits = []
    for key in ["å§¿æ€","ç¥æƒ…","é¢å®¹"]:
        block = (ta.get(key) or {}).copy()
        if not isinstance(block, dict): block = {}
        desc = block.get("è¯´æ˜","")
        interp = block.get("è§£è¯»","")
        merged = _merge_observe_and_interp(desc, interp)
        tend = (block.get("æ€§æ ¼å€¾å‘") or "").strip()
        if tend: traits.append(tend)
        block["è§£è¯»"] = merged
        ta[key] = block
    meta["triple_analysis"] = ta

    # ç»„åˆå¦
    hexes = [(ta.get("å§¿æ€") or {}).get("å¦è±¡",""),
             (ta.get("ç¥æƒ…") or {}).get("å¦è±¡",""),
             (ta.get("é¢å®¹") or {}).get("å¦è±¡","")]
    combo_title = " + ".join([h for h in hexes if h]) if any(hexes) else ""
    if combo_title:
        meta["combo_title"] = combo_title

    raw_summary = (ta.get("æ€»ç»“") or out.get("summary","")).strip()
    combo_summary = _compose_combo_summary(hexes, traits)
    if raw_summary:
        raw_summary = _sanitize_block_text(raw_summary)
        if re.search(r"(å§¿æ€|ç¥æƒ…|é¢å®¹|åŒæ‰‹|ç›®å…‰|äº”å®˜|çœ‰|çœ¼|é¼»|å˜´|ä¸‹å·´)", raw_summary):
            overview_text = combo_summary
        else:
            overview_text = _sanitize_block_text(raw_summary + "ï¼›" + combo_summary)
    else:
        overview_text = combo_summary
    meta["overview_card"] = {
        "title": f"ğŸ”® å¦è±¡ç»„åˆï¼š{combo_title}" if combo_title else "ğŸ”® å¦è±¡ç»„åˆ",
        "summary": overview_text
    }

    # ä¸­æ–‡ archetype å…œåº•
    arch = (out.get("archetype") or "").strip()
    if arch and not any('\u4e00' <= ch <= '\u9fff' for ch in arch):
        s = set([h for h in hexes if h])
        if "ä¹¾" in s and "å…‘" in s: arch = "ä¸»å¯¼Â·äº²å’Œå‹"
        elif "ä¹¾" in s and "ç¦»" in s: arch = "ä¸»å¯¼Â·è¡¨è¾¾å‹"
        elif "è‰®" in s and "å¤" in s: arch = "ç¨³é‡Â·åŒ…å®¹å‹"
        elif "å" in s and "ç¦»" in s: arch = "è°¨æ…Â·è¡¨è¾¾å‹"
        elif "éœ‡" in s and "å…‘" in s: arch = "è¡ŒåŠ¨Â·äº²å’Œå‹"
        else: arch = "ç»¼åˆå‹"
        out["archetype"] = arch

    # äº‹ä¸š/æ„Ÿæƒ…ï¼šçŠ¶æ€ä¸å»ºè®®
    def_map = meta.get("domains_detail") or {}
    status_map = _insight_for_domains(hexes)
    merged_status = {
        "äº‹ä¸š": _merge_status_and_detail(status_map.get("äº‹ä¸š",""), def_map.get("é‡‘é’±ä¸äº‹ä¸š","")),
        "æ„Ÿæƒ…": _merge_status_and_detail(status_map.get("æ„Ÿæƒ…",""), def_map.get("é…å¶ä¸æ„Ÿæƒ…","")),
    }
    meta["domains_status"] = {
        "äº‹ä¸š": _sanitize_block_text(merged_status["äº‹ä¸š"]),
        "æ„Ÿæƒ…": _sanitize_block_text(merged_status["æ„Ÿæƒ…"]),
    }
    meta["domains_suggestion"] = {
        "äº‹ä¸š": _sanitize_block_text(_imperative_suggestion(def_map.get("é‡‘é’±ä¸äº‹ä¸š",""))),
        "æ„Ÿæƒ…": _sanitize_block_text(_imperative_suggestion(def_map.get("é…å¶ä¸æ„Ÿæƒ…",""))),
    }

    # äº”å®˜ç»†èŠ‚å…œåº•æ¸…ç†ï¼šåˆ†å·â†’å¥å·
    fp = meta.get("face_parts") or {}
    for part in fp:
        for sub in ["ç‰¹å¾","è§£è¯»"]:
            if isinstance(fp[part], dict) and fp[part].get(sub):
                fp[part][sub] = _clean_punct(fp[part][sub]).replace("ï¼›","ã€‚")
    meta["face_parts"] = fp

    # å¯ä¿¡åº¦å…œåº•
    try:
        out["confidence"] = float(out.get("confidence",0.0))
    except Exception:
        out["confidence"] = 0.0
    meta["headline"] = {"tag": out.get("archetype",""), "confidence": out["confidence"]}

    out["meta"] = meta
    return out

# ---------------- routes ----------------
@app.get("/health")
def health(): return {"status":"ok"}

@app.get("/", include_in_schema=False)
def root():
    return HTMLResponse("<h3>Selfy AI</h3><a href='/docs'>/docs</a> Â· <a href='/mobile'>/mobile</a>")

@app.head("/", include_in_schema=False)
def root_head():
    return Response(status_code=200)

@app.get("/version")
def version(): return {"runtime":RUNTIME_VERSION,"analysis":ANALYSIS_VERSION,"schema":SCHEMA_ID,"debug":DEBUG}

@app.get("/mobile", include_in_schema=False)
def mobile():
    path = os.path.join(os.path.dirname(__file__), "index_mobile.html")
    try:
        html = open(path, "r", encoding="utf-8").read()
    except Exception as e:
        return HTMLResponse(f"<pre>index_mobile.html not found: {e}</pre>", status_code=500)
    return HTMLResponse(html)

def _call_gpt(messages):
    if client is None:
        raise RuntimeError("OpenAI client not initialized")
    return client.chat.completions.create(
        model="gpt-4o",
        temperature=0.4,
        tools=_build_tools_schema(),
        tool_choice={"type":"function","function":{"name":"submit_analysis_v3"}},
        response_format={"type":"json_object"},
        messages=messages
    )

def _call_gpt_tool_with_image(data_url: str) -> Dict[str,Any]:
    messages = _prompt_for_image_v372()
    messages[-1]["content"] = [
        {"type":"text","text":messages[-1]["content"]},
        {"type":"image_url","image_url":{"url":data_url}}
    ]
    resp = _call_gpt(messages)
    choice = resp.choices[0]
    tool_calls = getattr(choice.message, "tool_calls", None)
    if tool_calls:
        args = json.loads(tool_calls[0].function.arguments)
    else:
        content = getattr(choice.message, "content", None)
        if isinstance(content, str) and content.strip().startswith("{"):
            args = json.loads(content)
        else:
            raise RuntimeError("Model did not return tool_calls.")
    return {"tool_args": args, "oai_raw": resp if DEBUG else None}

@app.post("/upload")
async def upload(file: UploadFile = File(...)):
    try:
        if not file: raise HTTPException(400,"No file")
        ct = file.content_type or ""
        if not ct.startswith("image/"): raise HTTPException(415,f"Unsupported content type: {ct}")
        raw = await file.read()
        if not raw: raise HTTPException(400,"Empty file")
        if len(raw) > 15*1024*1024: raise HTTPException(413,"File too large (>15MB)")

        data_url = _to_data_url(raw, ct)
        logger.info("[UPLOAD] %s %dB %s", file.filename, len(raw), ct)

        result = _call_gpt_tool_with_image(data_url)
        tool_args = result["tool_args"]
        final_out = _coerce_output_v372(tool_args)

        if DEBUG:
            meta = final_out.setdefault("meta",{}).setdefault("debug",{})
            meta["file_info"]={"filename":file.filename,"content_type":ct,"size":len(raw)}
            try:
                meta["oai_choice_finish_reason"]=result["oai_raw"].choices[0].finish_reason
            except Exception:
                meta["oai_choice_finish_reason"]="n/a"

        return JSONResponse(content=final_out, status_code=200)
    except HTTPException as he:
        if DEBUG:
            return JSONResponse(status_code=he.status_code, content={"error":he.detail,"debug":{"trace":traceback.format_exc()}})
        raise
    except Exception as e:
        logging.exception("upload failed: %s", e)
        body={"error":"Internal Server Error"}
        if DEBUG: body["debug"]={"message":str(e),"trace":traceback.format_exc()}
        return JSONResponse(status_code=500, content=body)
