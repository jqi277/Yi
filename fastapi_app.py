# fastapi_app.py  (runtime v3.8.3, analysis logic v3.7.2)
# 3.8.3: â€œæ˜“ç»å¯è§£é‡Šç‰ˆâ€
# - å¦è±¡ç»„åˆï¼šä¸»/è¾…/åŸºä¸“ä¸šå¼€å¤´ + å…³ç³»ä¸ä¸»é£æ ¼çš„â€œæ„è±¡â†’ç™½è¯è§£é‡Šâ€ä¸¤æ­¥è½åœ°
# - ä¸‰åˆ†è±¡ï¼šå¼€å¤´è½»é‡ä¸“ä¸šæç¤ºï¼ˆå¦‚ã€ä¹¾Â·ä¸»å¯¼ã€‘ï¼‰ï¼Œåæ¥ç™½è¯è§£é‡Šï¼›å¼ºå»é‡å¤
# - äº‹ä¸š/æ„Ÿæƒ…ï¼šçŠ¶æ€ä¸å»ºè®®é¿å…å£å·/å †è¯ï¼Œæ”¹ä¸ºâ€œå¯æ„ŸçŸ¥åœºæ™¯ + æ˜ç¡®åŠ¨ä½œâ€çš„å¥å­
# - æ–‡æœ¬åå¤„ç†ï¼š_dedupe_smart ä¿å¥è¯»ï¼Œæ¸…ç†â€œåœ¨â€¦æ–¹é¢/â€¦ä¸Šâ€æ®‹å½±ä¸ä»£è¯
import os, base64, json, logging, traceback, re, math
from typing import Dict, Any, List

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse, HTMLResponse, Response
from fastapi.middleware.cors import CORSMiddleware

from openai import OpenAI

RUNTIME_VERSION = "3.8.5-ux"
ANALYSIS_VERSION = os.getenv("ANALYSIS_VERSION", "372").strip()
SCHEMA_ID = "selfy.v3"
DEBUG = str(os.getenv("DEBUG","0")).strip() in ("1","true","True","YES","yes")

logging.basicConfig(level=logging.DEBUG if DEBUG else logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger("selfy-ai")

app = FastAPI(title="Selfy AI - YiJing Analysis API", version=RUNTIME_VERSION)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

try:
    client = OpenAI()
except Exception as e:
    logger.error("OpenAI client init failed: %s", e); client=None

BAGUA_SYMBOLS = {"è‰®":"å±±","ç¦»":"ç«","å…‘":"æ³½","ä¹¾":"å¤©","å¤":"åœ°","éœ‡":"é›·","å·½":"é£","å":"æ°´"}

def _to_data_url(content: bytes, content_type: str) -> str:
    return f"data:{content_type};base64,{base64.b64encode(content).decode('utf-8')}"

def _build_tools_schema() -> List[Dict[str, Any]]:
    return [{
      "type":"function",
      "function":{
        "name":"submit_analysis_v3",
        "description":"Return end-user facing JSON for Selfy AI YiJing analysis.",
        "parameters":{
          "type":"object",
          "properties":{
            "summary":{"type":"string"},
            "archetype":{"type":"string"},
            "confidence":{"type":"number"},
            "sections":{"type":"object","properties":{"å§¿æ€":{"type":"string"},"ç¥æƒ…":{"type":"string"},"é¢ç›¸":{"type":"string"}},"required":["å§¿æ€","ç¥æƒ…","é¢ç›¸"],"additionalProperties":False},
            "domains":{"type":"array","items":{"type":"string"}},
            "meta":{"type":"object","additionalProperties":True}
          },
          "required":["summary","archetype","confidence","sections","domains"],
          "additionalProperties":False
        }
      }
    }]

def _json_hint() -> str:
    return ("åªä»¥ JSON object è¿”å›ï¼ˆå¿…é¡» JSONï¼‰ã€‚ç¤ºä¾‹:{\"summary\":\"â€¦\",\"archetype\":\"â€¦\",\"confidence\":0.9,"
            "\"sections\":{\"å§¿æ€\":\"â€¦\",\"ç¥æƒ…\":\"â€¦\",\"é¢ç›¸\":\"â€¦\"},"
            "\"domains\":[\"é‡‘é’±ä¸äº‹ä¸š\",\"é…å¶ä¸æ„Ÿæƒ…\"],"
            "\"meta\":{\"triple_analysis\":{\"å§¿æ€\":{\"è¯´æ˜\":\"â€¦\",\"å¦è±¡\":\"è‰®\",\"è§£è¯»\":\"â€¦\",\"æ€§æ ¼å€¾å‘\":\"â€¦\"},\"ç¥æƒ…\":{â€¦},\"é¢å®¹\":{â€¦},\"ç»„åˆæ„å¢ƒ\":\"â€¦\",\"æ€»ç»“\":\"â€¦\"},"
            "\"face_parts\":{\"çœ‰\":{\"ç‰¹å¾\":\"â€¦\",\"å¦è±¡\":\"â€¦\",\"è§£è¯»\":\"â€¦\"},\"çœ¼\":{â€¦},\"é¼»\":{â€¦},\"å˜´\":{â€¦},\"é¢§/ä¸‹å·´\":{â€¦}},"
            "\"domains_detail\":{\"é‡‘é’±ä¸äº‹ä¸š\":\"â€¦(60â€“90å­—)\",\"é…å¶ä¸æ„Ÿæƒ…\":\"â€¦(60â€“90å­—)\"}}}")

def _prompt_for_image_v372():
    sys = (
      "ä½ æ˜¯ Selfy AI çš„æ˜“ç»è§‚ç›¸åŠ©æ‰‹ï¼ˆv3.7.2 é£æ ¼ï¼‰ã€‚"
      "ä¸¥æ ¼æŒ‰â€œä¸‰è±¡å››æ®µå¼â€åˆ†æï¼šã€å§¿æ€/ç¥æƒ…/é¢å®¹ã€‘ä¸‰éƒ¨åˆ†ã€‚æ¯éƒ¨åˆ†å¿…é¡»åŒ…å«ï¼š"
      "1) è¯´æ˜ï¼š1å¥ï¼Œå®¢è§‚æç»˜å¤–è§‚/åŠ¨ä½œ/æ°”è´¨ï¼›"
      "2) å¦è±¡ï¼šä»…å†™ä¸€ä¸ªå¦åï¼ˆè‰®/ç¦»/å…‘/ä¹¾/å¤/éœ‡/å·½/åï¼‰ï¼›"
      "3) è§£è¯»ï¼š1â€“2å¥ï¼ŒåŸºäºå¦è±¡ä¸è§‚å¯Ÿåšå«ä¹‰é˜é‡Šï¼›"
      "4) æ€§æ ¼å€¾å‘ï¼š1â€“2å¥ï¼Œç‹¬ç«‹æˆæ®µï¼Œä¸è¦ä¸â€œè§£è¯»â€é‡å¤æªè¾ã€‚"
      "ç„¶åç»™å‡ºï¼š"
      "5) å¦è±¡ç»„åˆï¼šæ ‡é¢˜=ä¸‰è±¡å¦åç›¸åŠ ï¼ˆå¦‚â€œè‰® + ç¦» + å…‘â€ï¼‰ï¼Œæ­£æ–‡ 90â€“150 å­—ã€‚"
      "6) æ€»ç»“æ€§æ ¼å°è±¡ï¼š20â€“40å­—ï¼Œé¿å…æ¨¡æ¿åŒ–ï¼›"
      "7) äººæ ¼æ ‡ç­¾ archetypeï¼š2â€“5å­—ä¸­æ–‡ï¼Œå¦‚â€œå¤–å†·å†…çƒ­/ä¸»å¯¼å‹/è°¨æ…å‹â€ã€‚"
      "é¢ç›¸éœ€æ‹†æˆäº”å®˜ï¼šåœ¨ meta.face_parts ä¸­ï¼Œç»™ã€çœ‰/çœ¼/é¼»/å˜´/é¢§/ä¸‹å·´ã€‘ï¼ˆä»»é€‰5é¡¹è¦†ç›–ï¼‰å„å†™â€œç‰¹å¾ï¼ˆå¤–è§‚ï¼‰â€ä¸â€œè§£è¯»ï¼ˆåŸºäºæ˜“ç»ï¼‰â€ã€‚"
      "domains ä»…ä» ['é‡‘é’±ä¸äº‹ä¸š','é…å¶ä¸æ„Ÿæƒ…'] é€‰æ‹©ï¼›åœ¨ meta.domains_detail ä¸­åˆ†åˆ«å†™ 60â€“90 å­—å»ºè®®æ–‡æœ¬ã€‚"
      "å°†ç»“æœé€šè¿‡ submit_analysis_v3 å·¥å…·è¿”å›ï¼Œå¹¶"+_json_hint()+"ã€‚è¯­è¨€ï¼šä¸­æ–‡ã€‚"
    )
    user = "è¯·æŒ‰ 3.7.2 é£æ ¼åˆ†æå›¾ç‰‡ï¼Œä¸¥æ ¼é€šè¿‡å‡½æ•°è¿”å› JSONï¼ˆä¸è¦è¾“å‡ºè‡ªç”±æ–‡æœ¬ï¼‰ã€‚"
    return [{"role":"system","content":sys},{"role":"user","content":user}]

DOMAIN_LEADS = r"(åœ¨(é‡‘é’±ä¸äº‹ä¸š|é…å¶ä¸æ„Ÿæƒ…|äº‹ä¸š|æ„Ÿæƒ…)(æ–¹é¢|ä¸­|é‡Œ)?|ç›®å‰|è¿‘æœŸ|å½“ä¸‹)"
_STOPWORDS = r"(å§¿æ€|ç¥æƒ…|é¢å®¹|æ•´ä½“|æ°”è´¨|å½¢è±¡|ç»™äººä»¥|ä¸€ç§|ä»¥åŠ|å¹¶ä¸”|è€Œä¸”|æ›´æ˜¾|æ˜¾å¾—|å±•ç°å‡º|æµéœ²å‡º|é€éœ²å‡º)"

def _depronoun(s: str) -> str:
    if not isinstance(s, str): return s
    s = s.strip()
    s = re.sub(r"^(ä»–|å¥¹|TA|ä½ |å¯¹æ–¹|å…¶)(çš„)?[ï¼Œã€ï¼š ]*", "", s)
    s = re.sub(r"^(åœ¨(äº‹ä¸š|æ„Ÿæƒ…|ç”Ÿæ´»)[ä¸Šä¸­]|ç›®å‰|è¿‘æœŸ)[ï¼Œã€ï¼š ]*", "", s)
    return s

def _neutralize(s: str) -> str:
    if not isinstance(s, str): return s
    s = s.strip()
    s = re.sub(r"(ä»–|å¥¹|TA|å¯¹æ–¹|å…¶)(çš„)?", "", s)
    s = re.sub(DOMAIN_LEADS + r"[ï¼Œã€ï¼š ]*", "", s)
    s = re.sub(r"(å¯èƒ½|æˆ–è®¸|ä¹Ÿè®¸)[ï¼Œã€ ]*", "", s)
    s = re.sub(r"[ï¼›;]+", "ï¼›", s)
    s = re.sub(r"[ï¼Œ,]{2,}", "ï¼Œ", s)
    return s.strip("ï¼›ï¼Œã€‚ ")

def _canon_key(s: str) -> str:
    if not isinstance(s, str): return ""
    k = re.sub(_STOPWORDS, "", s)
    k = re.sub(r"[çš„åœ°å¾—]", "", k)
    k = re.sub(r"\s+", "", k)
    return k

def _dedupe_smart(s: str) -> str:
    if not isinstance(s, str): return s
    s = s.strip("ã€‚ï¼›ï¼Œ,; ")
    sentences = re.split(r"[ã€‚ï¼ï¼Ÿ]", s)
    clean_sentences = []
    for sen in sentences:
        sen = sen.strip("ï¼Œ,;ï¼› ")
        if not sen: continue
        parts = re.split(r"[ï¼Œ,ï¼›;]", sen)
        seen_keys, kept = set(), []
        for p in parts:
            t = p.strip()
            if not t: continue
            ck = _canon_key(t)
            if ck and ck not in seen_keys:
                seen_keys.add(ck); kept.append(t)
        clean_sentences.append("ï¼Œ".join(kept))
    return "ã€‚".join(clean_sentences) + "ã€‚"

def _strip_domain_lead(s: str) -> str:
    if not isinstance(s, str): return s
    s = re.sub("^" + DOMAIN_LEADS + r"[ï¼Œã€ï¼š ]*", "", s.strip())
    s = re.sub(r"^ä¸Š[ï¼Œã€ï¼š ]*", "", s)  # è£æ‰â€œä¸Šï¼Œâ€ä¹‹ç±»æ®‹ç•™
    return s

# --- æ˜“ç»è¯­ä¹‰è¡¨ ---
HEX_SUMMARY = {
    "ä¹¾":"è‡ªä¿¡Â·ä¸»å¯¼Â·æœæ–­",
    "å¤":"åŒ…å®¹Â·ç¨³å®šÂ·æ‰¿è½½",
    "éœ‡":"è¡ŒåŠ¨Â·çªç ´Â·èµ·åŠ¿",
    "å·½":"åè°ƒÂ·æ¸—é€Â·è¯´æœ",
    "å":"è°¨æ…Â·æ¢æ·±Â·æ±‚è¯",
    "ç¦»":"æ¸…æ™°Â·è¡¨è¾¾Â·æ´å¯Ÿ",
    "è‰®":"æ­¢å®šÂ·è¾¹ç•ŒÂ·ç¨³å®ˆ",
    "å…‘":"äº²å’ŒÂ·äº¤æµÂ·æ‚¦äºº"
}
WUXING = {
    "ä¹¾":{"element":"é‡‘","polarity":"é˜³","virtue":"åˆšå¥è‡ªå¼ºã€å¾¡é¢†å±€é¢"},
    "å…‘":{"element":"é‡‘","polarity":"é˜´","virtue":"å’Œæ‚¦äº²å’Œã€ä»¥ä¹æ„Ÿäºº"},
    "ç¦»":{"element":"ç«","polarity":"é˜´","virtue":"æ˜è¾¨æ´å¯Ÿã€æ“…äºè¡¨è¾¾"},
    "éœ‡":{"element":"æœ¨","polarity":"é˜³","virtue":"å‘åŠ¨èµ·åŠ¿ã€æ•¢äºçªç ´"},
    "å·½":{"element":"æœ¨","polarity":"é˜´","virtue":"æ¸—é€åè°ƒã€å–„è°‹åˆä¼—"},
    "å":{"element":"æ°´","polarity":"é˜³","virtue":"å®¡æ…æ¢æ·±ã€å±…å®‰è¯†å±"},
    "è‰®":{"element":"åœŸ","polarity":"é˜³","virtue":"æ­¢å®šæœ‰åº¦ã€å®ˆæ­£ç«‹ç•Œ"},
    "å¤":{"element":"åœŸ","polarity":"é˜´","virtue":"åšå¾·è½½ç‰©ã€å†…æ•›æ‰¿è½½"}
}
SHENG = {"æœ¨":"ç«","ç«":"åœŸ","åœŸ":"é‡‘","é‡‘":"æ°´","æ°´":"æœ¨"}
KE    = {"æœ¨":"åœŸ","åœŸ":"æ°´","æ°´":"ç«","ç«":"é‡‘","é‡‘":"æœ¨"}

# â€”â€” ç»æ–‡æç¤ºï¼ˆç®€æ´å¯è¯»ï¼‰â€”â€”
CLASSIC_TIPS = {
    "ä¹¾": "ã€Šä¹¾å¦ã€‹äº¢é¾™æœ‰æ‚”ï¼šè¿‡å¼ºåˆ™æŠ˜ï¼Œå®œæ”¶é”‹æ•›åŠ¿",
    "å¤": "ã€Šå¤å¦ã€‹å«å¼˜å…‰å¤§ï¼šåšè½½ä¸äº‰ï¼Œå¿Œå› å¾ªä¸å†³",
    "ç¦»": "ã€Šç¦»å¦ã€‹æ˜ä¸¤ä½œï¼šæ´å¯Ÿé«˜ä¹Ÿæ˜“è‹›æ±‚ï¼Œå®œä»¥æ˜é©­æ˜",
    "å": "ã€Šåå¦ã€‹å±¥éœœåšå†°è‡³ï¼šå±…å®‰æ€å±ï¼Œå…ˆè¯åæ–­",
    "éœ‡": "ã€Šéœ‡å¦ã€‹éœ‡æ¥è™©è™©ï¼šèµ·åŠ¿è«ä¹±ï¼Œå®šç¥è€Œè¡Œ",
    "å·½": "ã€Šå·½å¦ã€‹å°äº¨ï¼šå…¥å¾®æ¸—é€ï¼Œå¿Œåå¤ä¸å†³",
    "è‰®": "ã€Šè‰®å¦ã€‹è‰®å…¶èƒŒï¼šå®ˆç•Œæœ‰åº¦ï¼Œå¿Œåƒµç¡¬ä¸åŒ–",
    "å…‘": "ã€Šå…‘å¦ã€‹å’Œè€Œä¸åªšï¼šæ‚¦äººä¸å¤±èŠ‚ï¼Œå¿Œé€¢è¿å¤±çœŸ",
}

# â€”â€” å…ƒç´  â†’ æ¨èè°ƒå’Œå¦ & è¯´æ˜ï¼ˆå½“â€œç›¸å…‹â€å‡ºç°æ—¶ç»™å‡ºè¡¥æ³•ï¼‰â€”â€”
ELEM_TO_HEX = {"é‡‘":"ä¹¾/å…‘", "ç«":"ç¦»", "æ°´":"å", "æœ¨":"éœ‡/å·½", "åœŸ":"è‰®/å¤"}
REMEDY_PHRASE = {
    # ç”¨ X åˆ¶ Y çš„ç›´è§‰è§£é‡Šï¼šåªç»™ä¸€å¥â€œæ€ä¹ˆåšâ€
    ("ç«","é‡‘"): "ä»¥æ°´åˆ¶ç«ï¼ˆå¢é…åï¼‰ï¼šå…ˆæ”¶ä¿¡æ¯é™æ¸©èŠ‚å¥ï¼Œå†å®šå¤º",
    ("æœ¨","åœŸ"): "ä»¥é‡‘ä¼æœ¨ï¼ˆå¢é…ä¹¾/å…‘ï¼‰ï¼šå®šè§„åˆ™ã€æ˜è¾¹ç•Œï¼Œå‡å°‘ç²˜è¿",
    ("åœŸ","æ°´"): "ä»¥æœ¨ç ´åœŸï¼ˆå¢é…éœ‡/å·½ï¼‰ï¼šå…ˆåŠ¨èµ·æ¥ã€æ‰“é€šé˜»æ»",
    ("æ°´","ç«"): "ä»¥åœŸæ³„æ°´ï¼ˆå¢é…è‰®/å¤ï¼‰ï¼šè½åˆ°æµç¨‹ï¼Œç¨³ä½èŠ‚å¾‹",
    ("é‡‘","æœ¨"): "ä»¥ç«ç‚¼é‡‘ï¼ˆå¢é…ç¦»ï¼‰ï¼šå…ˆè®²æ¸…ç†ç”±ï¼Œè®©æ‰§è¡Œæœ‰è¯´æœåŠ›",
}

# â€”â€” äº”å®˜é»˜è®¤å¦è±¡ï¼ˆç”¨äºæ›´å…·ä½“çš„é¢ç›¸è¯­è¨€ï¼‰â€”â€”
FACE_HEX_DEFAULT = {
    "çœ‰": "å·½",      # é£ï¼Œæ¡ç†/åˆ¤æ–­
    "çœ¼": "ç¦»",      # ç«ï¼Œæ˜å¯Ÿ/è¡¨è¾¾
    "é¼»": "è‰®",      # å±±ï¼Œå®šåŠ›/è¾¹ç•Œ
    "å˜´": "å…‘",      # æ³½ï¼Œæ²Ÿé€š/æƒ…ç»ª
    "é¢§/ä¸‹å·´": "å¤", # åœ°ï¼Œæ‰¿è½½/ç¨³é‡
}

def _rel(a: str, b: str) -> str:
    if not a or not b: return ""
    if a == b: return "åŒæ°”ç›¸æ±‚"
    if SHENG.get(a) == b: return "ç›¸ç”Ÿ"
    if KE.get(a) == b: return "ç›¸å…‹"
    return "ç›¸å¹¶"

def _harmony_suggestion(main_hex: str, other_hex: str) -> str:
    """å½“ other å…‹ main æ—¶ï¼Œç»™ä¸€ä¸ªâ€œè¡¥ç¬¬ä¸‰å…ƒç´ â€çš„è°ƒå’Œå»ºè®®ã€‚"""
    if not (main_hex and other_hex): return ""
    A = (WUXING.get(main_hex) or {}).get("element","")
    B = (WUXING.get(other_hex) or {}).get("element","")
    if not A or not B: return ""
    # è‹¥ B å…‹ Aï¼Œåˆ™æ‰¾ä¸€æ¡ â€œX åˆ¶ Bâ€ çš„å»ºè®®
    if KE.get(B) == A:
        # æ‰¾å¯¹ REMEDY_PHRASE çš„é”®ï¼š (B, A) æ˜¯â€œBå…‹Aâ€ï¼Œæˆ‘ä»¬éœ€è¦â€œXåˆ¶Bâ€ï¼ŒæŒ‰æ˜ å°„å†™æ­»å¸¸è§æ­é…
        for (x, y), phrase in REMEDY_PHRASE.items():
            if x == B:  # è¿™ä¸ªæ˜ å°„æ˜¯â€œç”¨ ELEM(x) å»èŠ‚åˆ¶ xï¼ˆ=Bï¼‰â€
                return f"è°ƒå’Œå»ºè®®ï¼š{phrase}"
    return ""

def _derive_archetype(main: str, mf_rel: str, bm_rel: str) -> str:
    """ä»ä¸»å¦ä¸å…³ç³»ç»™ä¸ªä¸ç”Ÿç¡¬çš„äººæ ¼æ ‡ç­¾ï¼ˆ2~5å­—ï¼‰ã€‚"""
    if main in ("ä¹¾","éœ‡"):
        if "ç›¸å…‹" in (mf_rel or "") or "ç›¸å…‹" in (bm_rel or ""): return "ç¨³ä¸­å¸¦é”‹"
        if "ç›¸ç”Ÿ" in (mf_rel or "") or "æ¯”å’Œ" in (mf_rel or ""): return "åˆšæŸ”ç›¸æµ"
        return "å¼ å¼›å¹¶è¿›"
    if main in ("å¤","è‰®"):
        if "ç›¸å…‹" in (mf_rel or "") or "ç›¸å…‹" in (bm_rel or ""): return "åšé‡è€Œè­¦"
        if "ç›¸ç”Ÿ" in (mf_rel or "") or "æ¯”å’Œ" in (mf_rel or ""): return "åšå®å¼€æœ—"
        return "æ²‰ç€ä¸­è¡Œ"
    if main == "ç¦»":
        return "æ˜æ–­æœå†³" if (mf_rel in ("ç›¸ç”Ÿ","æ¯”å’Œ")) else "æ˜æ…å¹¶ä¸¾"
    if main == "å…‘":
        return "å’Œè€Œä¸åªš"
    if main == "å":
        return "æ…æ€ç¬ƒè¡Œ"
    if main == "å·½":
        return "æ¶¦ç‰©æ— å£°"
    return "ä¸­å’Œä¹‹å§¿"

def _style_by_main_plain(h: str) -> str:
    # ä¸»é£æ ¼ â†’ ç™½è¯è§£é‡Š
    if h in ("ä¹¾","éœ‡"): return "æ•´ä½“åä¸»åŠ¨ï¼šçœ‹å‡†å°±å…ˆåšç¬¬ä¸€æ­¥"
    if h in ("å¤","è‰®"): return "æ•´ä½“åç¨³å¦¥ï¼šå…ˆæŠŠåŸºç¡€æ‰“ç‰¢ï¼Œå†æ…¢æ…¢æ”¾å¤§"
    if h in ("ç¦»",):     return "æ•´ä½“åæ¸…æ¥šè¡¨è¾¾ï¼šå…ˆæŠŠç†ç”±è®²æ˜ç™½"
    if h in ("å…‘",):     return "æ•´ä½“åäº²å’Œï¼šå…ˆæŠŠå…³ç³»å¤„å¥½ï¼Œäº‹å°±å¥½åŠ"
    if h in ("å·½",):     return "æ•´ä½“ååè°ƒï¼šå…ˆèè¿›å»ï¼Œå†å¸¦ç€å¾€å‰èµ°"
    if h in ("å",):     return "æ•´ä½“åè°¨æ…ï¼šå…ˆç¡®è®¤ä¿¡æ¯ï¼Œå†å†³å®š"
    return "æ•´ä½“é£æ ¼å¹³è¡¡"

def _relation_plain(rel: str, pos: str) -> str:
    # rel1 ä¸»-è¾…ï¼›rel2 åŸº-ä¸»
    if pos == "mf":  # main-fu
        if rel == "ç›¸ç”Ÿ": return "ä¸»ä¸è¾…èƒ½å¯¹ä¸Šå·ï¼šæƒ³æ³•å®¹æ˜“è¢«ç†è§£ä¸æ”¯æŒ"
        if rel == "ç›¸å…‹": return "ä¸»ä¸è¾…æœ‰æ‘©æ“¦ï¼šå…ˆæŠŠæœŸå¾…è®²æ¸…å†æ¨è¿›"
        if rel == "åŒæ°”ç›¸æ±‚": return "ä¸»ä¸è¾…é£æ ¼ä¸€è‡´ï¼šæ•ˆç‡é«˜ï¼Œä½†è¦ç•™æ„ä¸åŒæ„è§"
        return "ä¸»ä¸è¾…å„æœ‰ä¾§é‡ï¼šéœ€è¦ä½ æ¥å®šæ¬¡åºå’Œåˆ†å·¥"
    else:            # base-main
        if rel == "ç›¸ç”Ÿ": return "å†…åœ¨åŠ›é‡åœ¨æ”¯æŒä¸»çº¿ï¼šé‡åˆ°å˜åŒ–ä¹Ÿä¸å®¹æ˜“ä¹±"
        if rel == "ç›¸å…‹": return "å†…å¿ƒä¸ç›®æ ‡æœ‰æ‹‰æ‰¯ï¼šå…ˆæƒ³æ¸…åŸåˆ™å†å‡ºæ‰‹"
        if rel == "åŒæ°”ç›¸æ±‚": return "å†…å¤–ä¸€è‡´ï¼šæƒ³æ³•å’Œåšæ³•ä¸æ‰“æ¶"
        return "å†…åœ¨ä¸ç›®æ ‡å„èµ°å„çš„ï¼šç”¨ç®€å•è§„åˆ™æŠŠå®ƒä»¬æ‹¢åœ¨ä¸€èµ·"


def _pair_label(main_hex: str, other_hex: str, relation: str, which: str) -> str:
    """which: 'è¾…' or 'åŸº'ï¼›è¾“å‡ºâ€œä¸»Xï¼ˆé‡‘ï¼‰Ã—è¾…/åŸºYï¼ˆåœŸï¼‰ï¼šåœŸç”Ÿé‡‘ â†’ åŠ©åŠ›/åˆ¶è¡¡/æ¯”å’Œ/å¹¶è¡Œï¼ˆç™½è¯ï¼‰â€"""
    if not (main_hex and other_hex and relation): return ""
    A = (WUXING.get(main_hex) or {}).get("element","")
    B = (WUXING.get(other_hex) or {}).get("element","")
    if not (A and B): return ""
    # æ–¹å‘ï¼šother â†’ main
    if SHENG.get(B) == A: arrow, tag = "ç”Ÿ", "åŠ©åŠ›"
    elif KE.get(B) == A: arrow, tag = "å…‹", "åˆ¶è¡¡"
    elif A == B:         arrow, tag = "åŒ", "æ¯”å’Œ"
    else:                arrow, tag = "å¹¶", "å¹¶è¡Œ"
    zh = f"ä¸»{main_hex}ï¼ˆ{A}ï¼‰Ã—{which}{other_hex}ï¼ˆ{B}ï¼‰ï¼š{B}{arrow}{A} â†’ {tag}"
    note = {"åŠ©åŠ›":"é…åˆé¡ºç•…ï¼Œä¼˜åŠ¿äº’è¡¥", "åˆ¶è¡¡":"é£æ ¼æœ‰å¼ åŠ›ï¼Œæ¨è¿›éœ€æ›´å¤šåè°ƒ", "æ¯”å’Œ":"åŒé¢‘ååŒï¼Œæ‰§è¡Œå¹²è„†", "å¹¶è¡Œ":"å…³æ³¨ç‚¹ä¸åŒï¼Œå„æ“…å…¶é•¿"}[tag]
    return zh + f"ï¼ˆ{note}ï¼‰"

def _persona_line(h: str) -> str:
    if not h: return ""
    ele = (WUXING.get(h) or {}).get("element","")
    vir = (WUXING.get(h) or {}).get("virtue","")
    return f"{h}ï¼ˆ{ele}ï¼‰ï¼š{vir}"

def _synthesize_combo(hexes: List[str], ta: Dict[str,Any], traits: List[str]) -> str:
    zh, sh, bh = (hexes + ["", "", ""])[:3]
    if not any([zh, sh, bh]): return ""

    wx   = lambda h: (WUXING.get(h) or {}).get("element","")
    sym  = lambda h: BAGUA_SYMBOLS.get(h, "")
    virt = lambda h: (WUXING.get(h) or {}).get("virtue","")

    def rel_from_to(a: str, b: str):
        A, B = wx(a), wx(b)
        if not A or not B: return "", ""
        if SHENG.get(A) == B: return f"{A}ç”Ÿ{B}", "ç›¸ç”Ÿ"
        if KE.get(A)   == B: return f"{A}å…‹{B}", "ç›¸å…‹"
        if A == B:           return f"{A}åŒ{B}", "æ¯”å’Œ"
        return f"{A}å¹¶{B}", "ç›¸å¹¶"

    # å…³ç³»ï¼ˆæŒ‰å¯¹ä¸»æ–¹å‘ï¼šè¾…â†’ä¸» / åŸºâ†’ä¸»ï¼‰
    mf_pair, mf_rel = rel_from_to(sh, zh)
    bm_pair, bm_rel = rel_from_to(bh, zh)

    mf_note = {
        "ç›¸ç”Ÿ": "åŒé¢‘ååŒï¼Œæ‰§è¡Œå¹²è„†",
        "ç›¸å…‹": "é£æ ¼æœ‰å¼ åŠ›ï¼Œæ¨è¿›éœ€æ›´å¤šåè°ƒ",
        "æ¯”å’Œ": "åŒé¢‘ååŒï¼Œæ‰§è¡Œå¹²è„†",
        "ç›¸å¹¶": "å…³æ³¨ç‚¹ä¸åŒï¼Œå„æ“…å…¶é•¿",
    }.get(mf_rel, "")
    bm_note = {
        "ç›¸ç”Ÿ": "æ ¹åŸºåŠ©æ¨ï¼Œåº•ç›˜ç»™åŠ›",
        "ç›¸å…‹": "æ—§ç»éªŒç‰µæ‰¯ï¼Œå–èˆè¦ç¨³",
        "æ¯”å’Œ": "å†…å¤–ä¸€è‡´ï¼Œè¡¨è¾¾ä¸è¡ŒåŠ¨ä¸æ‰“æ¶",
        "ç›¸å¹¶": "èµ„æºä¸ç›®æ ‡å„æœ‰ä¾§é‡",
    }.get(bm_rel, "")

    lines = []
    # ä¸‰è¡Œä¸»è¾…åŸº
    if zh: lines.append(f"ä¸»{zh}ï¼ˆ{wx(zh)}Â·{sym(zh)}ï¼‰ï¼š{virt(zh)}")
    if sh: lines.append(f"è¾…{sh}ï¼ˆ{wx(sh)}Â·{sym(sh)}ï¼‰ï¼š{virt(sh)}")
    if bh: lines.append(f"åŸº{bh}ï¼ˆ{wx(bh)}Â·{sym(bh)}ï¼‰ï¼š{virt(bh)}")
    # ä¸¤æ¡å…³ç³»
    if mf_rel: lines.append(f"ä¸»ä¸è¾…ï¼ˆ{mf_pair}ï¼‰{mf_rel}ï¼š{mf_note}")
    if bm_rel: lines.append(f"åŸºä¸ä¸»ï¼ˆ{bm_pair}ï¼‰{bm_rel}ï¼š{bm_note}")
    # è‹¥å‡ºç°ç›¸å…‹ï¼Œç»™è°ƒå’Œå»ºè®®
    if mf_rel == "ç›¸å…‹":
        hs = _harmony_suggestion(zh, sh)
        if hs: lines.append(hs)
    if bm_rel == "ç›¸å…‹":
        hs = _harmony_suggestion(zh, bh)
        if hs: lines.append(hs)

    # â€”â€” ä¸‰æ®µå¼æ”¶æŸï¼šå¤–åœ¨æ°”è±¡ / å†…åœ¨åŸºè´¨ / è¿åŠ¿æ ¼å±€ â€”â€” 
    def kw(h: str):
        s = HEX_SUMMARY.get(h, "")
        return (s.split("Â·")[0] if s else "", s.split("Â·")[1] if ("Â·" in s) else "")
    main_kw, main_trait = kw(zh)
    fu_kw,   fu_trait   = kw(sh)
    # å¤–åœ¨æ°”è±¡ï¼ˆä¸»å¦ï¼‰
    if zh:
        lines.append(f"å¤–åœ¨æ°”è±¡ï¼šä»¥{zh}ä¸ºçº²ï¼ˆ{main_kw}ï¼‰ï¼Œå…ˆç«‹æ ‡å‡†å†å¸¦èŠ‚å¥ã€‚{CLASSIC_TIPS.get(zh,'').split('ï¼š')[0]}å¯ä½œé•œé‰´ã€‚")
    # å†…åœ¨åŸºè´¨ï¼ˆè¾…/åŸºä½œç”¨ï¼‰
    inner_frag = []
    if sh: inner_frag.append(f"è¾…{sh}åŠ©{zh}ï¼ˆ{mf_rel or 'å¹¶è¡Œ'}ï¼‰")
    if bh: inner_frag.append(f"åŸº{bh}æ‰˜åº•ï¼ˆ{bm_rel or 'å¹¶è¡Œ'}ï¼‰")
    if inner_frag: lines.append("å†…åœ¨åŸºè´¨ï¼š" + "ï¼›".join(inner_frag) + "ã€‚")
    # è¿åŠ¿æ ¼å±€ï¼ˆæ•´ä½“å®šæ€§â€”â€”ä¸å†™â€œçœ‹å‡†å°±å…ˆåšç¬¬ä¸€æ­¥â€è¿™ç§å£å·ï¼‰
    left  = main_trait or "ä¸»å¯¼åŠ›"
    right = fu_trait   or "äº²å’ŒåŠ›"
    soft  = "å¤–åˆšå†…æŸ”" if (mf_rel in ("ç›¸ç”Ÿ","æ¯”å’Œ") and bm_rel in ("ç›¸ç”Ÿ","æ¯”å’Œ")) else "å¼ å¼›æœ‰åº¦"
    lines.append(f"è¿åŠ¿æ ¼å±€ï¼š{soft}ï¼Œæ—¢æœ‰{left}ï¼Œåˆå…·{right}ã€‚ä»¥ç¨³æ¨è¿›ã€åˆ†å±‚å†³æ–­ä¸ºå®œã€‚")

    return "\n".join(lines)


def _human_status_sentence(s: set, domain: str) -> str:
    lines = []
    if domain == "äº‹ä¸š":
        if "ä¹¾" in s or "éœ‡" in s: lines.append("æœ‰è®¡åˆ’ä¹Ÿè‚¯åŠ¨æ‰‹ï¼Œé‡äº‹ä¸æ‹–")
        if "ç¦»" in s: lines.append("è¯´æ¸…æ¥šæƒ³æ³•ï¼Œèƒ½æŠŠåŸå› è®²æ˜ç™½")
        if "å…‘" in s or "å·½" in s: lines.append("ä¼šæŠŠäººæ‹‰è¿›æ¥ä¸€èµ·åšï¼Œæ°”æ°›ä¸ç´§å¼ ")
        if "å¤" in s or "è‰®" in s: lines.append("å…ˆç¨³ä½ï¼Œå†å†³å®šï¼Œäº‹æƒ…èƒ½è½åˆ°ç»“æœä¸Š")
        if "å" in s: lines.append("ä¼šå…ˆæŸ¥æ¸…ä¿¡æ¯ï¼Œç•™ä¸ªå¤‡é€‰æ–¹æ¡ˆ")
    else:
        if "å…‘" in s: lines.append("èŠå¤©è‡ªç„¶ï¼Œæ„¿æ„è¡¨è¾¾æ„Ÿå—")
        if "ç¦»" in s: lines.append("è®²é“ç†ä¹Ÿè®²åˆ†å¯¸")
        if "å¤" in s: lines.append("é‡æ‰¿è¯ºï¼Œæ„¿æ„èŠ±æ—¶é—´é™ªä¼´")
        if "å" in s: lines.append("åœ¨æ„å®‰å…¨æ„Ÿï¼Œå®¹æ˜“å¤šæƒ³")
        if "éœ‡" in s or "ä¹¾" in s: lines.append("å…³é”®æ—¶èƒ½ä¸»åŠ¨é è¿‘")
        if "è‰®" in s: lines.append("å°Šé‡å½¼æ­¤è¾¹ç•Œ")
    return "ï¼›".join(lines)

def _insight_for_domains(hexes: List[str]) -> Dict[str, str]:
    s = set([h for h in hexes if h])
    return {
        "äº‹ä¸š": _human_status_sentence(s, "äº‹ä¸š"),
        "æ„Ÿæƒ…": _human_status_sentence(s, "æ„Ÿæƒ…"),
    }

def _imperative_suggestion(detail: str, hexes: List[str], domain: str) -> str:
    s = set([h for h in hexes if h])
    tips = []
    if domain == "äº‹ä¸š":
        if "ä¹¾" in s or "éœ‡" in s: tips.append("å…ˆæŠŠæœ€é‡è¦çš„ä¸€ä»¶äº‹å®šä¸‹æ¥ï¼Œä»Šå¤©æ¨è¿›ä¸€å°æ­¥")
        if "ç¦»" in s: tips.append("å½“é¢è®²æ¸…ç†ç”±ï¼Œå†è½åˆ°å…·ä½“åšæ³•")
        if "å…‘" in s or "å·½" in s: tips.append("æ‰¾å…³é”®äººèŠä¸€èŠï¼Œå…ˆå¬å¯¹æ–¹çš„ï¼Œå†è¯´è‡ªå·±çš„")
        if "å¤" in s or "è‰®" in s: tips.append("æŠŠèŒƒå›´å’Œæ—¶é—´è¯´æ¸…æ¥šï¼Œåˆ«ä¸€å£åƒæˆèƒ–å­")
        if "å" in s: tips.append("åšäº‹å‰å…ˆæ ¸å¯¹ä¿¡æ¯ï¼Œå‡†å¤‡ä¸€ä¸ªå¤‡é€‰æ–¹æ¡ˆ")
    else:
        if "å…‘" in s: tips.append("ç”¨å¹³å¸¸è¯­æ°”èŠå¿ƒé‡Œçš„äº‹ï¼Œä¸ç”¨ç»•å¼¯å­")
        if "å¤" in s: tips.append("ç­”åº”çš„äº‹å°½é‡æŒ‰æ—¶åšåˆ°ï¼Œè®©å¯¹æ–¹æœ‰åº•")
        if "ç¦»" in s: tips.append("æŠŠç•Œé™è¯´æ¸…æ¥šï¼Œè®©å¯¹æ–¹çŸ¥é“ä½ çš„æƒ³æ³•")
        if "éœ‡" in s or "ä¹¾" in s: tips.append("åœ¨é‡è¦æ—¶åˆ»ä¸»åŠ¨ä¸€ç‚¹")
        if "å" in s: tips.append("å°‘é çŒœï¼Œå¤šç¡®è®¤")
        if "è‰®" in s: tips.append("ç»™å½¼æ­¤ä¸€äº›ç‹¬å¤„æ—¶é—´")
    add = "ï¼›".join(tips[:3])
    return (add + "ã€‚") if add else ""


def _imperative_suggestion_points(hexes: List[str], domain: str) -> List[str]:
    s = set([h for h in hexes if h])
    tips = []
    if domain == "äº‹ä¸š":
        if "ä¹¾" in s or "éœ‡" in s: tips.append("å…ˆæŠŠæœ€é‡è¦çš„ä¸€ä»¶äº‹å®šä¸‹æ¥ï¼Œä»Šå¤©æ¨è¿›ä¸€å°æ­¥")
        if "ç¦»" in s: tips.append("å½“é¢è®²æ¸…ç†ç”±ï¼Œå†è½åˆ°å…·ä½“åšæ³•")
        if "å…‘" in s or "å·½" in s: tips.append("æ‰¾å…³é”®äººèŠä¸€èŠï¼Œå…ˆå¬å¯¹æ–¹çš„ï¼Œå†è¯´è‡ªå·±çš„")
        if "å¤" in s or "è‰®" in s: tips.append("æŠŠèŒƒå›´å’Œæ—¶é—´è¯´æ¸…æ¥šï¼Œåˆ«ä¸€å£åƒæˆèƒ–å­")
        if "å" in s: tips.append("åšäº‹å‰å…ˆæ ¸å¯¹ä¿¡æ¯ï¼Œå‡†å¤‡ä¸€ä¸ªå¤‡é€‰æ–¹æ¡ˆ")
    else:
        if "å…‘" in s: tips.append("ç”¨å¹³å¸¸è¯­æ°”èŠå¿ƒé‡Œçš„äº‹ï¼Œä¸ç”¨ç»•å¼¯å­")
        if "å¤" in s: tips.append("ç­”åº”çš„äº‹å°½é‡æŒ‰æ—¶åšåˆ°ï¼Œè®©å¯¹æ–¹æœ‰åº•")
        if "ç¦»" in s: tips.append("æŠŠç•Œé™è¯´æ¸…æ¥šï¼Œè®©å¯¹æ–¹çŸ¥é“ä½ çš„æƒ³æ³•")
        if "éœ‡" in s or "ä¹¾" in s: tips.append("åœ¨é‡è¦æ—¶åˆ»ä¸»åŠ¨ä¸€ç‚¹")
        if "å" in s: tips.append("å°‘é çŒœï¼Œå¤šç¡®è®¤")
        if "è‰®" in s: tips.append("ç»™å½¼æ­¤ä¸€äº›ç‹¬å¤„æ—¶é—´")
    return tips[:3]

# ---- ä¸‰åˆ†è±¡åˆå¥ & ä¸“ä¸šæç¤º ----
def _combine_sentence(desc: str, interp: str) -> str:
    if not desc and not interp:
        return ""
    # åŸºç¡€æ¸…æ´—
    desc  = _neutralize(_depronoun((desc or "").strip()))
    interp = _neutralize(_depronoun((interp or "").strip()))
    # å»æ‰ç ´æŠ˜å·/å‰å¯¼è¯­
    interp = re.sub(r"^(â€”â€”|-+)\s*", "", interp)
    interp = re.sub(r"^(è¿™ç§|æ­¤ç±»|è¿™ç±»|å…¶|è¿™ç§å§¿æ€|è¿™ç§ç¥æƒ…|è¿™ç§é¢å®¹|è¿™ç§é¢ç›¸)[ï¼Œã€ï¼š ]*", "", interp)
    # åˆå¹¶
    s = f"{desc}ï¼Œ{interp}" if (desc and interp) else (desc or interp)
    # ç»Ÿä¸€æ ‡ç‚¹ & å»æ‰å¥é¦–å­¤ç«‹æ ‡ç‚¹
    s = re.sub(r"[ï¼›;]+", "ï¼›", s)
    s = re.sub(r"^[ï¼Œ,ã€‚ï¼›;ï¼š:]+", "", s)      # å…³é”®ï¼šå»æ‰â€œã€‚ï¼Œâ€ä¹‹ç±»
    s = re.sub(r"ï¼Œï¼Œ+", "ï¼Œ", s)
    return _dedupe_smart(s)

def _collect_traits_and_merge(ta: Dict[str,Any]) -> (List[str], Dict[str,Any]):
    traits, new_ta = [], {}
    for key in ["å§¿æ€","ç¥æƒ…","é¢å®¹"]:
        o = (ta.get(key) or {}).copy()
        tend = (o.get("æ€§æ ¼å€¾å‘") or "").strip().rstrip("ï¼›;ã€‚")
        if tend: traits.append(tend)

        desc   = (o.get("è¯´æ˜") or "")
        inter  = (o.get("è§£è¯»") or "")
        merged = _combine_sentence(desc, inter)

        hexname = re.sub(r"(å¦ï¼ˆ[^ï¼‰]*ï¼‰|å¦|[ã€‚\.ã€‚\s]+)$", "", (o.get("å¦è±¡") or "").strip())
        o["å¦è±¡"] = hexname

        pro = ""
        if hexname in HEX_SUMMARY:
            kw = HEX_SUMMARY[hexname].split("Â·")[1] if "Â·" in HEX_SUMMARY[hexname] else HEX_SUMMARY[hexname]
            pro = f""

        tip = CLASSIC_TIPS.get(hexname, "")
        if pro: merged = f"{pro} {merged}".strip()
        if tip: merged = f"{merged}ï¼ˆç»æ–‡æç¤ºï¼š{tip}ï¼‰".strip()

        o["è¯´æ˜"] = ""
        o["è§£è¯»"] = merged
        o["æ€§æ ¼å€¾å‘"] = ""
        new_ta[key] = o

    for k in ta.keys():
        if k not in new_ta: new_ta[k] = ta[k]
    return traits, new_ta

def _to_points(s: str, max_items: int = 4) -> List[str]:
    """Split a sentence by Chinese semicolons/commas into 2-4 concise bullet points."""
    if not s: return []
    s = _neutralize(s)
    s = re.sub(r"[ï¼›;]+", "ï¼›", s.strip("ï¼›ã€‚ \n\t"))
    parts = [p.strip("ï¼›ï¼Œã€‚ \n\t") for p in s.split("ï¼›") if p.strip()]
    if len(parts) <= 1:
        parts = [p.strip("ï¼›ï¼Œã€‚ \n\t") for p in re.split(r"[ï¼Œ,]", s) if p.strip()]
    seen, uniq = set(), []
    for p in parts:
        if p in seen: continue
        seen.add(p); uniq.append(p)
        if len(uniq) >= max_items: break
    return uniq
def _merge_status_and_detail(status: str, detail: str) -> str:
    detail_first = detail.split("ã€‚")[0].strip() if detail else ""
    detail_first = _neutralize(_strip_domain_lead(detail_first))
    status = _neutralize(_strip_domain_lead(status or ""))
    parts = [p for p in [status, detail_first] if p]
    text = "ï¼›".join(parts).rstrip("ï¼›")
    return _dedupe_smart(text)

def _confidence_breakdown(out: Dict[str,Any]) -> Dict[str,Any]:
    meta = out.get("meta") or {}
    ta   = meta.get("triple_analysis") or {}

    # 1) å›¾åƒæ¸…æ™°åº¦ï¼šæ— æ³•è¯»åƒç´ ï¼Œè¿™é‡Œç”¨â€œäº”å®˜æ¡ç›®é½å…¨åº¦â€ & æ–‡æœ¬å™ªå£°å æ¯”å½“ proxy
    face = meta.get("face_parts") or {}
    face_count = sum(1 for k,v in (face or {}).items() if isinstance(v,dict) and (v.get("ç‰¹å¾") or v.get("è§£è¯»")))
    clarity = min(1.0, 0.5 + 0.1 * face_count)  # 0.5~1.0 ä¹‹é—´

    # 2) å¦è±¡ä¸€è‡´æ€§ï¼šä¸‰è±¡æ˜¯å¦é½ã€æ˜¯å¦æœ‰æ¯”å’Œ/ç›¸ç”Ÿ
    hexes = [(ta.get("å§¿æ€") or {}).get("å¦è±¡",""), (ta.get("ç¥æƒ…") or {}).get("å¦è±¡",""), (ta.get("é¢å®¹") or {}).get("å¦è±¡","")]
    present = [h for h in hexes if h]
    uniq = len(set(present))
    if len(present) < 2:
        hx_cons = 0.5
    elif uniq == 1:
        hx_cons = 1.0  # å®Œå…¨æ¯”å’Œ
    else:
        hx_cons = 0.8  # æœ‰åˆ†å·¥ä½†ä¸å†²çªï¼Œç»™ 0.8

    # 3) ç‰¹å¾æ˜¾è‘—æ€§ï¼šä¸‰åˆ†è±¡â€œè§£è¯»â€é•¿åº¦ & å»é‡åå¯†åº¦
    texts = []
    for k in ("å§¿æ€","ç¥æƒ…","é¢å®¹"):
        t = (ta.get(k) or {}).get("è§£è¯»","")
        if t: texts.append(t)
    avg_len = sum(len(t) for t in texts)/max(1,len(texts))
    salience = 0.6 if avg_len < 30 else (0.8 if avg_len < 80 else 1.0)

    # é…ç½®æƒé‡ï¼ˆå¯å¾®è°ƒï¼‰
    w1, w2, w3 = 0.30, 0.40, 0.30
    score = w1*clarity + w2*hx_cons + w3*salience
    # ä¸è¦†ç›–æ¨¡å‹è‡ªå¸¦ confidenceï¼Œåªç»™â€œè§£é‡Šâ€
    return {
        "weights":{"å›¾åƒæ¸…æ™°åº¦":w1,"å¦è±¡ä¸€è‡´æ€§":w2,"ç‰¹å¾æ˜¾è‘—æ€§":w3},
        "scores":{"å›¾åƒæ¸…æ™°åº¦":round(clarity,2),"å¦è±¡ä¸€è‡´æ€§":round(hx_cons,2),"ç‰¹å¾æ˜¾è‘—æ€§":round(salience,2)},
        "explain":"åˆ†é¡¹æƒé‡ä¸ºç»éªŒå€¼ï¼Œç”¨äº”å®˜è¦†ç›–åº¦ã€ä¸‰è±¡é½æ•´åº¦/æ¯”å’Œåº¦ã€ä¸æ–‡æœ¬å¯†åº¦ä½œä¸ºè¿‘ä¼¼æŒ‡æ ‡ï¼Œä»…ç”¨äºå¸®åŠ©ç†è§£â€œ90%æŠŠæ¡â€çš„æ¥æºï¼Œä¸ä»£è¡¨ç»Ÿè®¡å­¦ç½®ä¿¡åŒºé—´ã€‚"
    }

def _coerce_output(data: Dict[str,Any]) -> Dict[str,Any]:
    out = dict(data)
    meta = out.get("meta") or {}
    if not isinstance(meta, dict):
        meta = {}
    out["meta"] = meta

    # 1) åˆå¹¶ä¸‰åˆ†è±¡ï¼ˆä¼šåšå»é‡/å»â€œå¦/ã€‚â€ç­‰æ¸…æ´—ï¼‰
    ta = meta.get("triple_analysis") or {}
    traits, ta = _collect_traits_and_merge(ta)
    meta["triple_analysis"] = ta

    # 2) ç”¨åˆå¹¶åçš„ä¸‰åˆ†è±¡å›å¡«é¡¶å±‚ sectionsï¼Œä¿®æ‰â€œã€‚ï¼Œ/ã€‚ï¼›â€ç­‰
    out["sections"] = {
        "å§¿æ€": (ta.get("å§¿æ€") or {}).get("è§£è¯»", ""),
        "ç¥æƒ…": (ta.get("ç¥æƒ…") or {}).get("è§£è¯»", ""),
        "é¢ç›¸": (ta.get("é¢å®¹") or {}).get("è§£è¯»", ""),
    }

    # 3) ç»„åˆæ ‡é¢˜ä¸æ€»è§ˆï¼ˆå¹¶è£æ‰é¦–è¡Œæ ‡é¢˜ï¼‰
    hexes = [
        (ta.get("å§¿æ€") or {}).get("å¦è±¡", ""),
        (ta.get("ç¥æƒ…") or {}).get("å¦è±¡", ""),
        (ta.get("é¢å®¹") or {}).get("å¦è±¡", "")
    ]
    combo_title = " + ".join([h for h in hexes if h])
    meta["combo_title"] = combo_title

    synthesized = _synthesize_combo(hexes, ta, traits)
    one = (ta.get("æ€»ç»“") or out.get("summary", ""))
    overview = (synthesized or one).strip().rstrip("ï¼›;")
    if overview.startswith("ğŸ”® å¦è±¡ç»„åˆ"):
        lines = overview.splitlines()
        if len(lines) > 1:
            overview = "\n".join(lines[1:]).strip()
    meta["overview_card"] = {
        "title": f"ğŸ”® å¦è±¡ç»„åˆï¼š{combo_title}" if combo_title else "ğŸ”® å¦è±¡ç»„åˆ",
        "summary": overview
    }

    # 4) å¯ä¿¡åº¦ä¸äººç‰©æ ‡ç­¾æŠ¬å¤´
    try:
        out["confidence"] = float(out.get("confidence", 0.0))
    except Exception:
        out["confidence"] = 0.0
    arch = (out.get("archetype") or "").strip()
    meta["headline"] = {"tag": arch, "confidence": out["confidence"]}

    # 5) äº‹ä¸š/æ„Ÿæƒ…ï¼šçŠ¶æ€ä¸å»ºè®®ï¼ˆåˆå¹¶ä¸ºæ›´äººè¯çš„è¦ç‚¹ + åˆ—è¡¨ï¼‰
    dd = meta.get("domains_detail") or {}
    status = _insight_for_domains(hexes)
    merged_status = {
        "äº‹ä¸š": _merge_status_and_detail(status.get("äº‹ä¸š", ""), dd.get("é‡‘é’±ä¸äº‹ä¸š", "")),
        "æ„Ÿæƒ…": _merge_status_and_detail(status.get("æ„Ÿæƒ…", ""), dd.get("é…å¶ä¸æ„Ÿæƒ…", "")),
    }
    meta["domains_status"] = merged_status
    meta["domains_status_list"] = {k: _to_points(v) for k, v in merged_status.items()}
    meta["domains_suggestion"] = {
        "äº‹ä¸š": _imperative_suggestion(dd.get("é‡‘é’±ä¸äº‹ä¸š", ""), hexes, "äº‹ä¸š"),
        "æ„Ÿæƒ…": _imperative_suggestion(dd.get("é…å¶ä¸æ„Ÿæƒ…", ""), hexes, "æ„Ÿæƒ…"),
    }
    meta["domains_suggestion_list"] = {
        "äº‹ä¸š": _imperative_suggestion_points(hexes, "äº‹ä¸š"),
        "æ„Ÿæƒ…": _imperative_suggestion_points(hexes, "æ„Ÿæƒ…"),
    }

    # 6) æ–‡æœ¬æ¸…æ´—å™¨
    def _clean(s):
        if not isinstance(s, str):
            return s
        s = s.replace("â€”â€”", "ï¼Œ")
        s = re.sub(r"[ï¼›;]+", "ï¼›", s)
        s = re.sub(r"ï¼›([ã€‚ï¼])", r"\1", s)
        s = re.sub(r"([ã€‚ï¼ï¼Ÿ])ï¼›", r"\1", s)
        s = _depronoun(s)
        s = _neutralize(s)
        return _dedupe_smart(s)

    out["summary"] = _clean(out.get("summary", ""))
    out["archetype"] = _clean(out.get("archetype", ""))

    def _deep_clean(x):
        if isinstance(x, dict):
            return {k: _deep_clean(v) for k, v in x.items()}
        if isinstance(x, list):
            return [_deep_clean(v) for v in x]
        return _clean(x)

    # 7) äº”å®˜ç»†èŠ‚ï¼šè‹¥â€œè§£è¯»â€é‡Œé‡å¤â€œç‰¹å¾â€ï¼Œåˆ™å»é‡ï¼Œç»Ÿä¸€æ ‡ç‚¹
    fps = meta.get("face_parts") or {}
    if isinstance(fps, dict):
        for k, v in list(fps.items()):
            if not isinstance(v, dict):
                continue
            feat = (v.get("ç‰¹å¾") or "").strip().strip("ã€‚ï¼›;ï¼Œ, ")
            expl = (v.get("è§£è¯»") or "").strip()
            if feat and expl and feat in expl:
                import re as _re
                expl = _re.sub(_re.escape(feat) + r"[ï¼Œ,ï¼›;]?", "", expl)
            v["ç‰¹å¾"] = feat
            v["è§£è¯»"] = re.sub(r"[ï¼›;]+", "ï¼›", expl).strip("ï¼›ã€‚ ")
    meta["face_parts"] = fps

    # 8) å…¨é‡æ·±åº¦æ¸…æ´—
    out["meta"] = _deep_clean(meta)
    return out


@app.get("/health")
def health(): return {"status":"ok"}

@app.get("/", include_in_schema=False)
def root():
    return HTMLResponse("<h3>Selfy AI</h3><a href='/docs'>/docs</a> Â· <a href='/mobile'>/mobile</a>")

@app.head("/", include_in_schema=False)
def root_head():
    return Response(status_code=200)

@app.get("/version")
def version(): return {"runtime":RUNTIME_VERSION,"analysis":ANALYSIS_VERSION,"schema":SCHEMA_ID,"debug":DEBUG}

@app.get("/mobile", include_in_schema=False)
def mobile():
    path = os.path.join(os.path.dirname(__file__), "index_mobile.html")
    try:
        html = open(path, "r", encoding="utf-8").read()
    except Exception as e:
        return HTMLResponse(f"<pre>index_mobile.html not found: {e}</pre>", status_code=500)
    return HTMLResponse(html)

def _call_openai(messages):
    if client is None:
        raise RuntimeError("OpenAI client not initialized")
    return client.chat.completions.create(
        model="gpt-4o",
        temperature=0.4,
        tools=_build_tools_schema(),
        tool_choice={"type":"function","function":{"name":"submit_analysis_v3"}},
        response_format={"type":"json_object"},
        messages=messages
    )

def _call_gpt_tool_with_image(data_url: str) -> Dict[str,Any]:
    messages = _prompt_for_image_v372()
    messages[-1]["content"] = [
        {"type":"text","text":messages[-1]["content"]},
        {"type":"image_url","image_url":{"url":data_url}}
    ]
    resp = _call_openai(messages)
    choice = resp.choices[0]
    tool_calls = getattr(choice.message, "tool_calls", None)
    if tool_calls:
        args = json.loads(tool_calls[0].function.arguments)
    else:
        content = getattr(choice.message, "content", None)
        if isinstance(content, str) and content.strip().startswith("{"):
            args = json.loads(content)
        else:
            raise RuntimeError("Model did not return tool_calls.")
    return {"tool_args": args, "oai_raw": resp if DEBUG else None}

@app.post("/upload")
async def upload(file: UploadFile = File(...)):
    try:
        if not file: raise HTTPException(400,"No file")
        ct = file.content_type or ""
        if not ct.startswith("image/"): raise HTTPException(415,f"Unsupported content type: {ct}")
        raw = await file.read()
        if not raw: raise HTTPException(400,"Empty file")
        if len(raw) > 15*1024*1024: raise HTTPException(413,"File too large (>15MB)")

        data_url = _to_data_url(raw, ct)
        logger.info("[UPLOAD] %s %dB %s", file.filename, len(raw), ct)

        result = _call_gpt_tool_with_image(data_url)
        tool_args = result["tool_args"]
        final_out = _coerce_output(tool_args)

        if DEBUG:
            meta = final_out.setdefault("meta",{}).setdefault("debug",{})
            meta["file_info"]={"filename":file.filename,"content_type":ct,"size":len(raw)}
            try:
                meta["oai_choice_finish_reason"]=result["oai_raw"].choices[0].finish_reason
            except Exception:
                meta["oai_choice_finish_reason"]="n/a"

        return JSONResponse(content=final_out, status_code=200)
    except HTTPException as he:
        if DEBUG:
            return JSONResponse(status_code=he.status_code, content={"error":he.detail,"debug":{"trace":traceback.format_exc()}})
        raise
    except Exception as e:
        logging.exception("upload failed: %s", e)
        body={"error":"Internal Server Error"}
        if DEBUG: body["debug"]={"message":str(e),"trace":traceback.format_exc()}
        return JSONResponse(status_code=500, content=body)
