# fastapi_app.py  (runtime v3.8.3, analysis logic v3.7.2)
# 3.8.3: â€œæ˜“ç»å¯è§£é‡Šç‰ˆâ€
# - å¦è±¡ç»„åˆï¼šä¸»/è¾…/åŸºä¸“ä¸šå¼€å¤´ + å…³ç³»ä¸ä¸»é£æ ¼çš„â€œæ„è±¡â†’ç™½è¯è§£é‡Šâ€ä¸¤æ­¥è½åœ°
# - ä¸‰åˆ†è±¡ï¼šå¼€å¤´è½»é‡ä¸“ä¸šæç¤ºï¼ˆå¦‚ã€ä¹¾Â·ä¸»å¯¼ã€‘ï¼‰ï¼Œåæ¥ç™½è¯è§£é‡Šï¼›å¼ºå»é‡å¤
# - äº‹ä¸š/æ„Ÿæƒ…ï¼šçŠ¶æ€ä¸å»ºè®®é¿å…å£å·/å †è¯ï¼Œæ”¹ä¸ºâ€œå¯æ„ŸçŸ¥åœºæ™¯ + æ˜ç¡®åŠ¨ä½œâ€çš„å¥å­
# - æ–‡æœ¬åå¤„ç†ï¼š_dedupe_smart ä¿å¥è¯»ï¼Œæ¸…ç†â€œåœ¨â€¦æ–¹é¢/â€¦ä¸Šâ€æ®‹å½±ä¸ä»£è¯
import os, base64, json, logging, traceback, re, math
from typing import Dict, Any, List

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.responses import JSONResponse, HTMLResponse, Response
from fastapi.middleware.cors import CORSMiddleware

from openai import OpenAI

RUNTIME_VERSION = "3.8.5-ux"
ANALYSIS_VERSION = os.getenv("ANALYSIS_VERSION", "372").strip()
SCHEMA_ID = "selfy.v3"
DEBUG = str(os.getenv("DEBUG","0")).strip() in ("1","true","True","YES","yes")

logging.basicConfig(level=logging.DEBUG if DEBUG else logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger("selfy-ai")

app = FastAPI(title="Selfy AI - YiJing Analysis API", version=RUNTIME_VERSION)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])

try:
    client = OpenAI()
except Exception as e:
    logger.error("OpenAI client init failed: %s", e); client=None

BAGUA_SYMBOLS = {"è‰®":"å±±","ç¦»":"ç«","å…‘":"æ³½","ä¹¾":"å¤©","å¤":"åœ°","éœ‡":"é›·","å·½":"é£","å":"æ°´"}

def _to_data_url(content: bytes, content_type: str) -> str:
    return f"data:{content_type};base64,{base64.b64encode(content).decode('utf-8')}"

def _build_tools_schema() -> List[Dict[str, Any]]:
    return [{
      "type":"function",
      "function":{
        "name":"submit_analysis_v3",
        "description":"Return end-user facing JSON for Selfy AI YiJing analysis.",
        "parameters":{
          "type":"object",
          "properties":{
            "summary":{"type":"string"},
            "archetype":{"type":"string"},
            "confidence":{"type":"number"},
            "sections":{"type":"object","properties":{"å§¿æ€":{"type":"string"},"ç¥æƒ…":{"type":"string"},"é¢ç›¸":{"type":"string"}},"required":["å§¿æ€","ç¥æƒ…","é¢ç›¸"],"additionalProperties":False},
            "domains":{"type":"array","items":{"type":"string"}},
            "meta":{"type":"object","additionalProperties":True}
          },
          "required":["summary","archetype","confidence","sections","domains"],
          "additionalProperties":False
        }
      }
    }]

def _json_hint() -> str:
    return ("åªä»¥ JSON object è¿”å›ï¼ˆå¿…é¡» JSONï¼‰ã€‚ç¤ºä¾‹:{\"summary\":\"â€¦\",\"archetype\":\"â€¦\",\"confidence\":0.9,"
            "\"sections\":{\"å§¿æ€\":\"â€¦\",\"ç¥æƒ…\":\"â€¦\",\"é¢ç›¸\":\"â€¦\"},"
            "\"domains\":[\"é‡‘é’±ä¸äº‹ä¸š\",\"é…å¶ä¸æ„Ÿæƒ…\"],"
            "\"meta\":{\"triple_analysis\":{\"å§¿æ€\":{\"è¯´æ˜\":\"â€¦\",\"å¦è±¡\":\"è‰®\",\"è§£è¯»\":\"â€¦\",\"æ€§æ ¼å€¾å‘\":\"â€¦\"},\"ç¥æƒ…\":{â€¦},\"é¢å®¹\":{â€¦},\"ç»„åˆæ„å¢ƒ\":\"â€¦\",\"æ€»ç»“\":\"â€¦\"},"
            "\"face_parts\":{\"çœ‰\":{\"ç‰¹å¾\":\"â€¦\",\"å¦è±¡\":\"â€¦\",\"è§£è¯»\":\"â€¦\"},\"çœ¼\":{â€¦},\"é¼»\":{â€¦},\"å˜´\":{â€¦},\"é¢§/ä¸‹å·´\":{â€¦}},"
            "\"domains_detail\":{\"é‡‘é’±ä¸äº‹ä¸š\":\"â€¦(60â€“90å­—)\",\"é…å¶ä¸æ„Ÿæƒ…\":\"â€¦(60â€“90å­—)\"}}}")

def _prompt_for_image_v372():
    sys = (
      "ä½ æ˜¯ Selfy AI çš„æ˜“ç»è§‚ç›¸åŠ©æ‰‹ï¼ˆv3.7.2 é£æ ¼ï¼‰ã€‚"
      "ä¸¥æ ¼æŒ‰â€œä¸‰è±¡å››æ®µå¼â€åˆ†æï¼šã€å§¿æ€/ç¥æƒ…/é¢å®¹ã€‘ä¸‰éƒ¨åˆ†ã€‚æ¯éƒ¨åˆ†å¿…é¡»åŒ…å«ï¼š"
      "1) è¯´æ˜ï¼š1å¥ï¼Œå®¢è§‚æç»˜å¤–è§‚/åŠ¨ä½œ/æ°”è´¨ï¼›"
      "2) å¦è±¡ï¼šä»…å†™ä¸€ä¸ªå¦åï¼ˆè‰®/ç¦»/å…‘/ä¹¾/å¤/éœ‡/å·½/åï¼‰ï¼›"
      "3) è§£è¯»ï¼š1â€“2å¥ï¼ŒåŸºäºå¦è±¡ä¸è§‚å¯Ÿåšå«ä¹‰é˜é‡Šï¼›"
      "4) æ€§æ ¼å€¾å‘ï¼š1â€“2å¥ï¼Œç‹¬ç«‹æˆæ®µï¼Œä¸è¦ä¸â€œè§£è¯»â€é‡å¤æªè¾ã€‚"
      "ç„¶åç»™å‡ºï¼š"
      "5) å¦è±¡ç»„åˆï¼šæ ‡é¢˜=ä¸‰è±¡å¦åç›¸åŠ ï¼ˆå¦‚â€œè‰® + ç¦» + å…‘â€ï¼‰ï¼Œæ­£æ–‡ 90â€“150 å­—ã€‚"
      "6) æ€»ç»“æ€§æ ¼å°è±¡ï¼š20â€“40å­—ï¼Œé¿å…æ¨¡æ¿åŒ–ï¼›"
      "7) äººæ ¼æ ‡ç­¾ archetypeï¼š2â€“5å­—ä¸­æ–‡ï¼Œå¦‚â€œå¤–å†·å†…çƒ­/ä¸»å¯¼å‹/è°¨æ…å‹â€ã€‚"
      "é¢ç›¸éœ€æ‹†æˆäº”å®˜ï¼šåœ¨ meta.face_parts ä¸­ï¼Œç»™ã€çœ‰/çœ¼/é¼»/å˜´/é¢§/ä¸‹å·´ã€‘ï¼ˆä»»é€‰5é¡¹è¦†ç›–ï¼‰å„å†™â€œç‰¹å¾ï¼ˆå¤–è§‚ï¼‰â€ä¸â€œè§£è¯»ï¼ˆåŸºäºæ˜“ç»ï¼‰â€ã€‚"
      "domains ä»…ä» ['é‡‘é’±ä¸äº‹ä¸š','é…å¶ä¸æ„Ÿæƒ…'] é€‰æ‹©ï¼›åœ¨ meta.domains_detail ä¸­åˆ†åˆ«å†™ 60â€“90 å­—å»ºè®®æ–‡æœ¬ã€‚"
      "å°†ç»“æœé€šè¿‡ submit_analysis_v3 å·¥å…·è¿”å›ï¼Œå¹¶"+_json_hint()+"ã€‚è¯­è¨€ï¼šä¸­æ–‡ã€‚"
    )
    user = "è¯·æŒ‰ 3.7.2 é£æ ¼åˆ†æå›¾ç‰‡ï¼Œä¸¥æ ¼é€šè¿‡å‡½æ•°è¿”å› JSONï¼ˆä¸è¦è¾“å‡ºè‡ªç”±æ–‡æœ¬ï¼‰ã€‚"
    return [{"role":"system","content":sys},{"role":"user","content":user}]

DOMAIN_LEADS = r"(åœ¨(é‡‘é’±ä¸äº‹ä¸š|é…å¶ä¸æ„Ÿæƒ…|äº‹ä¸š|æ„Ÿæƒ…)(æ–¹é¢|ä¸­|é‡Œ)?|ç›®å‰|è¿‘æœŸ|å½“ä¸‹)"
_STOPWORDS = r"(å§¿æ€|ç¥æƒ…|é¢å®¹|æ•´ä½“|æ°”è´¨|å½¢è±¡|ç»™äººä»¥|ä¸€ç§|ä»¥åŠ|å¹¶ä¸”|è€Œä¸”|æ›´æ˜¾|æ˜¾å¾—|å±•ç°å‡º|æµéœ²å‡º|é€éœ²å‡º)"

def _depronoun(s: str) -> str:
    if not isinstance(s, str): return s
    s = s.strip()
    s = re.sub(r"^(ä»–|å¥¹|TA|ä½ |å¯¹æ–¹|å…¶)(çš„)?[ï¼Œã€ï¼š ]*", "", s)
    s = re.sub(r"^(åœ¨(äº‹ä¸š|æ„Ÿæƒ…|ç”Ÿæ´»)[ä¸Šä¸­]|ç›®å‰|è¿‘æœŸ)[ï¼Œã€ï¼š ]*", "", s)
    return s

def _neutralize(s: str) -> str:
    if not isinstance(s, str): return s
    s = s.strip()
    s = re.sub(r"(ä»–|å¥¹|TA|å¯¹æ–¹|å…¶)(çš„)?", "", s)
    s = re.sub(DOMAIN_LEADS + r"[ï¼Œã€ï¼š ]*", "", s)
    s = re.sub(r"(å¯èƒ½|æˆ–è®¸|ä¹Ÿè®¸)[ï¼Œã€ ]*", "", s)
    s = re.sub(r"[ï¼›;]+", "ï¼›", s)
    s = re.sub(r"[ï¼Œ,]{2,}", "ï¼Œ", s)
    return s.strip("ï¼›ï¼Œã€‚ ")

def _canon_key(s: str) -> str:
    if not isinstance(s, str): return ""
    k = re.sub(_STOPWORDS, "", s)
    k = re.sub(r"[çš„åœ°å¾—]", "", k)
    k = re.sub(r"\s+", "", k)
    return k

def _dedupe_smart(s: str) -> str:
    if not isinstance(s, str): return s
    s = s.strip("ã€‚ï¼›ï¼Œ,; ")
    sentences = re.split(r"[ã€‚ï¼ï¼Ÿ]", s)
    clean_sentences = []
    for sen in sentences:
        sen = sen.strip("ï¼Œ,;ï¼› ")
        if not sen: continue
        parts = re.split(r"[ï¼Œ,ï¼›;]", sen)
        seen_keys, kept = set(), []
        for p in parts:
            t = p.strip()
            if not t: continue
            ck = _canon_key(t)
            if ck and ck not in seen_keys:
                seen_keys.add(ck); kept.append(t)
        clean_sentences.append("ï¼Œ".join(kept))
    return "ã€‚".join(clean_sentences) + "ã€‚"

def _strip_domain_lead(s: str) -> str:
    if not isinstance(s, str): return s
    s = re.sub("^" + DOMAIN_LEADS + r"[ï¼Œã€ï¼š ]*", "", s.strip())
    s = re.sub(r"^ä¸Š[ï¼Œã€ï¼š ]*", "", s)  # è£æ‰â€œä¸Šï¼Œâ€ä¹‹ç±»æ®‹ç•™
    return s

# --- æ˜“ç»è¯­ä¹‰è¡¨ ---
HEX_SUMMARY = {
    "ä¹¾":"è‡ªä¿¡Â·ä¸»å¯¼Â·æœæ–­",
    "å¤":"åŒ…å®¹Â·ç¨³å®šÂ·æ‰¿è½½",
    "éœ‡":"è¡ŒåŠ¨Â·çªç ´Â·èµ·åŠ¿",
    "å·½":"åè°ƒÂ·æ¸—é€Â·è¯´æœ",
    "å":"è°¨æ…Â·æ¢æ·±Â·æ±‚è¯",
    "ç¦»":"æ¸…æ™°Â·è¡¨è¾¾Â·æ´å¯Ÿ",
    "è‰®":"æ­¢å®šÂ·è¾¹ç•ŒÂ·ç¨³å®ˆ",
    "å…‘":"äº²å’ŒÂ·äº¤æµÂ·æ‚¦äºº"
}
WUXING = {
    "ä¹¾":{"element":"é‡‘","polarity":"é˜³","virtue":"åˆšå¥è‡ªå¼ºã€å¾¡é¢†å±€é¢"},
    "å…‘":{"element":"é‡‘","polarity":"é˜´","virtue":"å’Œæ‚¦äº²å’Œã€ä»¥ä¹æ„Ÿäºº"},
    "ç¦»":{"element":"ç«","polarity":"é˜´","virtue":"æ˜è¾¨æ´å¯Ÿã€æ“…äºè¡¨è¾¾"},
    "éœ‡":{"element":"æœ¨","polarity":"é˜³","virtue":"å‘åŠ¨èµ·åŠ¿ã€æ•¢äºçªç ´"},
    "å·½":{"element":"æœ¨","polarity":"é˜´","virtue":"æ¸—é€åè°ƒã€å–„è°‹åˆä¼—"},
    "å":{"element":"æ°´","polarity":"é˜³","virtue":"å®¡æ…æ¢æ·±ã€å±…å®‰è¯†å±"},
    "è‰®":{"element":"åœŸ","polarity":"é˜³","virtue":"æ­¢å®šæœ‰åº¦ã€å®ˆæ­£ç«‹ç•Œ"},
    "å¤":{"element":"åœŸ","polarity":"é˜´","virtue":"åšå¾·è½½ç‰©ã€å†…æ•›æ‰¿è½½"}
}
SHENG = {"æœ¨":"ç«","ç«":"åœŸ","åœŸ":"é‡‘","é‡‘":"æ°´","æ°´":"æœ¨"}
KE    = {"æœ¨":"åœŸ","åœŸ":"æ°´","æ°´":"ç«","ç«":"é‡‘","é‡‘":"æœ¨"}

def _rel(a: str, b: str) -> str:
    if not a or not b: return ""
    if a == b: return "åŒæ°”ç›¸æ±‚"
    if SHENG.get(a) == b: return "ç›¸ç”Ÿ"
    if KE.get(a) == b: return "ç›¸å…‹"
    return "ç›¸å¹¶"

def _style_by_main_plain(h: str) -> str:
    # ä¸»é£æ ¼ â†’ ç™½è¯è§£é‡Š
    if h in ("ä¹¾","éœ‡"): return "æ•´ä½“åä¸»åŠ¨ï¼šçœ‹å‡†å°±å…ˆåšç¬¬ä¸€æ­¥"
    if h in ("å¤","è‰®"): return "æ•´ä½“åç¨³å¦¥ï¼šå…ˆæŠŠåŸºç¡€æ‰“ç‰¢ï¼Œå†æ…¢æ…¢æ”¾å¤§"
    if h in ("ç¦»",):     return "æ•´ä½“åæ¸…æ¥šè¡¨è¾¾ï¼šå…ˆæŠŠç†ç”±è®²æ˜ç™½"
    if h in ("å…‘",):     return "æ•´ä½“åäº²å’Œï¼šå…ˆæŠŠå…³ç³»å¤„å¥½ï¼Œäº‹å°±å¥½åŠ"
    if h in ("å·½",):     return "æ•´ä½“ååè°ƒï¼šå…ˆèè¿›å»ï¼Œå†å¸¦ç€å¾€å‰èµ°"
    if h in ("å",):     return "æ•´ä½“åè°¨æ…ï¼šå…ˆç¡®è®¤ä¿¡æ¯ï¼Œå†å†³å®š"
    return "æ•´ä½“é£æ ¼å¹³è¡¡"

def _relation_plain(rel: str, pos: str) -> str:
    # rel1 ä¸»-è¾…ï¼›rel2 åŸº-ä¸»
    if pos == "mf":  # main-fu
        if rel == "ç›¸ç”Ÿ": return "ä¸»ä¸è¾…èƒ½å¯¹ä¸Šå·ï¼šæƒ³æ³•å®¹æ˜“è¢«ç†è§£ä¸æ”¯æŒ"
        if rel == "ç›¸å…‹": return "ä¸»ä¸è¾…æœ‰æ‘©æ“¦ï¼šå…ˆæŠŠæœŸå¾…è®²æ¸…å†æ¨è¿›"
        if rel == "åŒæ°”ç›¸æ±‚": return "ä¸»ä¸è¾…é£æ ¼ä¸€è‡´ï¼šæ•ˆç‡é«˜ï¼Œä½†è¦ç•™æ„ä¸åŒæ„è§"
        return "ä¸»ä¸è¾…å„æœ‰ä¾§é‡ï¼šéœ€è¦ä½ æ¥å®šæ¬¡åºå’Œåˆ†å·¥"
    else:            # base-main
        if rel == "ç›¸ç”Ÿ": return "å†…åœ¨åŠ›é‡åœ¨æ”¯æŒä¸»çº¿ï¼šé‡åˆ°å˜åŒ–ä¹Ÿä¸å®¹æ˜“ä¹±"
        if rel == "ç›¸å…‹": return "å†…å¿ƒä¸ç›®æ ‡æœ‰æ‹‰æ‰¯ï¼šå…ˆæƒ³æ¸…åŸåˆ™å†å‡ºæ‰‹"
        if rel == "åŒæ°”ç›¸æ±‚": return "å†…å¤–ä¸€è‡´ï¼šæƒ³æ³•å’Œåšæ³•ä¸æ‰“æ¶"
        return "å†…åœ¨ä¸ç›®æ ‡å„èµ°å„çš„ï¼šç”¨ç®€å•è§„åˆ™æŠŠå®ƒä»¬æ‹¢åœ¨ä¸€èµ·"


def _pair_label(main_hex: str, other_hex: str, relation: str, which: str) -> str:
    """which: 'è¾…' or 'åŸº'ï¼›è¾“å‡ºâ€œä¸»Xï¼ˆé‡‘ï¼‰Ã—è¾…/åŸºYï¼ˆåœŸï¼‰ï¼šåœŸç”Ÿé‡‘ â†’ åŠ©åŠ›/åˆ¶è¡¡/æ¯”å’Œ/å¹¶è¡Œï¼ˆç™½è¯ï¼‰â€"""
    if not (main_hex and other_hex and relation): return ""
    A = (WUXING.get(main_hex) or {}).get("element","")
    B = (WUXING.get(other_hex) or {}).get("element","")
    if not (A and B): return ""
    # æ–¹å‘ï¼šother â†’ main
    if SHENG.get(B) == A: arrow, tag = "ç”Ÿ", "åŠ©åŠ›"
    elif KE.get(B) == A: arrow, tag = "å…‹", "åˆ¶è¡¡"
    elif A == B:         arrow, tag = "åŒ", "æ¯”å’Œ"
    else:                arrow, tag = "å¹¶", "å¹¶è¡Œ"
    zh = f"ä¸»{main_hex}ï¼ˆ{A}ï¼‰Ã—{which}{other_hex}ï¼ˆ{B}ï¼‰ï¼š{B}{arrow}{A} â†’ {tag}"
    note = {"åŠ©åŠ›":"é…åˆé¡ºç•…ï¼Œä¼˜åŠ¿äº’è¡¥", "åˆ¶è¡¡":"é£æ ¼æœ‰å¼ åŠ›ï¼Œæ¨è¿›éœ€æ›´å¤šåè°ƒ", "æ¯”å’Œ":"åŒé¢‘ååŒï¼Œæ‰§è¡Œå¹²è„†", "å¹¶è¡Œ":"å…³æ³¨ç‚¹ä¸åŒï¼Œå„æ“…å…¶é•¿"}[tag]
    return zh + f"ï¼ˆ{note}ï¼‰"

def _persona_line(h: str) -> str:
    if not h: return ""
    ele = (WUXING.get(h) or {}).get("element","")
    vir = (WUXING.get(h) or {}).get("virtue","")
    return f"{h}ï¼ˆ{ele}ï¼‰ï¼š{vir}"



def _synthesize_combo(hexes: List[str], ta: Dict[str,Any], traits: List[str]) -> str:
    zh, sh, bh = (hexes + ["", "", ""])[:3]
    if not any([zh, sh, bh]):
        return ""

    def vw(h: str, key: str) -> str:
        return (WUXING.get(h) or {}).get(key, "")

    def persona_line(role: str, h: str) -> str:
        if not h:
            return ""
        ele = vw(h, "element")
        vir = vw(h, "virtue")
        sym = BAGUA_SYMBOLS.get(h, "")
        return f"{role}{h}ï¼ˆ{ele}Â·{sym}ï¼‰ï¼š{vir}"

    def rel_phrase(a: str, b: str):
        # é‡‡ç”¨â€œaâ†’bâ€çš„æ–¹å‘æè¿°äº”è¡Œå…³ç³»ï¼ˆä¸»ä¸è¾…ã€åŸºä¸ä¸»ï¼‰
        if not a or not b:
            return "", ""
        A = vw(a, "element")
        B = vw(b, "element")
        if not A or not B:
            return "", ""
        if SHENG.get(A) == B:
            return f"{A}ç”Ÿ{B}", "ç›¸ç”Ÿ"
        if KE.get(A) == B:
            return f"{A}å…‹{B}", "ç›¸å…‹"
        if A == B:
            return f"{A}åŒ{B}", "æ¯”å’Œ"
        return f"{A}å¹¶{B}", "ç›¸å¹¶"

    # 1) ä¸‰è±¡å®šä½ï¼ˆæ¯ä¸ªä¸€è¡Œï¼‰
    lines: list[str] = []
    for role, h in (("ä¸»", zh), ("è¾…", sh), ("åŸº", bh)):
        pl = persona_line(role, h)
        if pl:
            lines.append(pl)

    # 2) å…³ç³»ä¸¤è¡Œï¼ˆä¸»ä¸è¾… / åŸºä¸ä¸»ï¼‰
    mf_pair, mf_rel = rel_phrase(zh, sh)
    bm_pair, bm_rel = rel_phrase(bh, zh)
    if mf_rel:
        expl_mf = {
            "ç›¸ç”Ÿ": "é…åˆé¡ºç•…ï¼Œä¼˜åŠ¿äº’è¡¥",
            "ç›¸å…‹": "é£æ ¼æœ‰å¼ åŠ›ï¼Œç•™æ„èŠ‚å¥ä¸åˆ†å·¥",
            "æ¯”å’Œ": "åŒé¢‘ååŒï¼Œæ‰§è¡Œå¹²è„†",
            "ç›¸å¹¶": "å…³æ³¨ç‚¹ä¸åŒï¼Œéœ€å¹¶è¡Œå…¼å®¹",
        }[mf_rel]
        lines.append(f"ä¸»ä¸è¾…ï¼ˆ{mf_pair}ï¼‰{mf_rel}ï¼š{expl_mf}")
    if bm_rel:
        expl_bm = {
            "ç›¸ç”Ÿ": "æ ¹åŸºåŠ©æ¨ï¼Œåº•ç›˜ç»™åŠ›",
            "ç›¸å…‹": "æ—§ç»éªŒç‰µæ‰¯ï¼Œå½“ä¸‹å–èˆè¦ç¨³",
            "æ¯”å’Œ": "å†…å¤–ä¸€è‡´ï¼Œè¡¨è¾¾ä¸è¡ŒåŠ¨ä¸æ‰“æ¶",
            "ç›¸å¹¶": "èµ„æºä¸ç›®æ ‡å„æœ‰ä¾§é‡",
        }[bm_rel]
        lines.append(f"åŸºä¸ä¸»ï¼ˆ{bm_pair}ï¼‰{bm_rel}ï¼š{expl_bm}")

    # 3) æ”¶æŸå¥
    style = _style_by_main_plain(zh) if zh else "æ•´ä½“é£æ ¼å¹³è¡¡"

    def kw(h: str):
        s = HEX_SUMMARY.get(h, "")
        if not s:
            return ("", "")
        parts = s.split("Â·")
        if len(parts) == 2:
            return parts[0], parts[1]
        return parts[0], ""

    k1a, k1b = kw(zh)
    k2a, k2b = kw(sh)
    left = k1b or k1a or "ä¸»å¯¼åŠ›"
    right = k2b or k2a or "äº²å’ŒåŠ›"
    soft = "å¤–åˆšå†…æŸ”" if (mf_rel in ("ç›¸ç”Ÿ", "æ¯”å’Œ") and bm_rel in ("ç›¸ç”Ÿ", "æ¯”å’Œ")) else "å¼ å¼›æœ‰åº¦"
    summary = f"ä¸‰è€…ç»“åˆï¼Œå½¢æˆ{soft}çš„ç‰¹è´¨ï¼šæ—¢æœ‰{left}ï¼Œåˆå…·{right}ã€‚{style}ã€‚"

    # ---- çŠ¶æ€ & å»ºè®®ï¼ˆæ›´äººè¯ã€æ›´åœºæ™¯ï¼‰ ----
def _human_status_sentence(s: set, domain: str) -> str:
    lines = []
    if domain == "äº‹ä¸š":
        if "ä¹¾" in s or "éœ‡" in s: lines.append("æœ‰è®¡åˆ’ä¹Ÿè‚¯åŠ¨æ‰‹ï¼Œé‡äº‹ä¸æ‹–")
        if "ç¦»" in s: lines.append("è¯´æ¸…æ¥šæƒ³æ³•ï¼Œèƒ½æŠŠåŸå› è®²æ˜ç™½")
        if "å…‘" in s or "å·½" in s: lines.append("ä¼šæŠŠäººæ‹‰è¿›æ¥ä¸€èµ·åšï¼Œæ°”æ°›ä¸ç´§å¼ ")
        if "å¤" in s or "è‰®" in s: lines.append("å…ˆç¨³ä½ï¼Œå†å†³å®šï¼Œäº‹æƒ…èƒ½è½åˆ°ç»“æœä¸Š")
        if "å" in s: lines.append("ä¼šå…ˆæŸ¥æ¸…ä¿¡æ¯ï¼Œç•™ä¸ªå¤‡é€‰æ–¹æ¡ˆ")
    else:
        if "å…‘" in s: lines.append("èŠå¤©è‡ªç„¶ï¼Œæ„¿æ„è¡¨è¾¾æ„Ÿå—")
        if "ç¦»" in s: lines.append("è®²é“ç†ä¹Ÿè®²åˆ†å¯¸")
        if "å¤" in s: lines.append("é‡æ‰¿è¯ºï¼Œæ„¿æ„èŠ±æ—¶é—´é™ªä¼´")
        if "å" in s: lines.append("åœ¨æ„å®‰å…¨æ„Ÿï¼Œå®¹æ˜“å¤šæƒ³")
        if "éœ‡" in s or "ä¹¾" in s: lines.append("å…³é”®æ—¶èƒ½ä¸»åŠ¨é è¿‘")
        if "è‰®" in s: lines.append("å°Šé‡å½¼æ­¤è¾¹ç•Œ")
    return "ï¼›".join(lines)

def _insight_for_domains(hexes: List[str]) -> Dict[str, str]:
    s = set([h for h in hexes if h])
    return {
        "äº‹ä¸š": _human_status_sentence(s, "äº‹ä¸š"),
        "æ„Ÿæƒ…": _human_status_sentence(s, "æ„Ÿæƒ…"),
    }

def _imperative_suggestion(detail: str, hexes: List[str], domain: str) -> str:
    s = set([h for h in hexes if h])
    tips = []
    if domain == "äº‹ä¸š":
        if "ä¹¾" in s or "éœ‡" in s: tips.append("å…ˆæŠŠæœ€é‡è¦çš„ä¸€ä»¶äº‹å®šä¸‹æ¥ï¼Œä»Šå¤©æ¨è¿›ä¸€å°æ­¥")
        if "ç¦»" in s: tips.append("å½“é¢è®²æ¸…ç†ç”±ï¼Œå†è½åˆ°å…·ä½“åšæ³•")
        if "å…‘" in s or "å·½" in s: tips.append("æ‰¾å…³é”®äººèŠä¸€èŠï¼Œå…ˆå¬å¯¹æ–¹çš„ï¼Œå†è¯´è‡ªå·±çš„")
        if "å¤" in s or "è‰®" in s: tips.append("æŠŠèŒƒå›´å’Œæ—¶é—´è¯´æ¸…æ¥šï¼Œåˆ«ä¸€å£åƒæˆèƒ–å­")
        if "å" in s: tips.append("åšäº‹å‰å…ˆæ ¸å¯¹ä¿¡æ¯ï¼Œå‡†å¤‡ä¸€ä¸ªå¤‡é€‰æ–¹æ¡ˆ")
    else:
        if "å…‘" in s: tips.append("ç”¨å¹³å¸¸è¯­æ°”èŠå¿ƒé‡Œçš„äº‹ï¼Œä¸ç”¨ç»•å¼¯å­")
        if "å¤" in s: tips.append("ç­”åº”çš„äº‹å°½é‡æŒ‰æ—¶åšåˆ°ï¼Œè®©å¯¹æ–¹æœ‰åº•")
        if "ç¦»" in s: tips.append("æŠŠç•Œé™è¯´æ¸…æ¥šï¼Œè®©å¯¹æ–¹çŸ¥é“ä½ çš„æƒ³æ³•")
        if "éœ‡" in s or "ä¹¾" in s: tips.append("åœ¨é‡è¦æ—¶åˆ»ä¸»åŠ¨ä¸€ç‚¹")
        if "å" in s: tips.append("å°‘é çŒœï¼Œå¤šç¡®è®¤")
        if "è‰®" in s: tips.append("ç»™å½¼æ­¤ä¸€äº›ç‹¬å¤„æ—¶é—´")
    add = "ï¼›".join(tips[:3])
    return (add + "ã€‚") if add else ""


def _imperative_suggestion_points(hexes: List[str], domain: str) -> List[str]:
    s = set([h for h in hexes if h])
    tips = []
    if domain == "äº‹ä¸š":
        if "ä¹¾" in s or "éœ‡" in s: tips.append("å…ˆæŠŠæœ€é‡è¦çš„ä¸€ä»¶äº‹å®šä¸‹æ¥ï¼Œä»Šå¤©æ¨è¿›ä¸€å°æ­¥")
        if "ç¦»" in s: tips.append("å½“é¢è®²æ¸…ç†ç”±ï¼Œå†è½åˆ°å…·ä½“åšæ³•")
        if "å…‘" in s or "å·½" in s: tips.append("æ‰¾å…³é”®äººèŠä¸€èŠï¼Œå…ˆå¬å¯¹æ–¹çš„ï¼Œå†è¯´è‡ªå·±çš„")
        if "å¤" in s or "è‰®" in s: tips.append("æŠŠèŒƒå›´å’Œæ—¶é—´è¯´æ¸…æ¥šï¼Œåˆ«ä¸€å£åƒæˆèƒ–å­")
        if "å" in s: tips.append("åšäº‹å‰å…ˆæ ¸å¯¹ä¿¡æ¯ï¼Œå‡†å¤‡ä¸€ä¸ªå¤‡é€‰æ–¹æ¡ˆ")
    else:
        if "å…‘" in s: tips.append("ç”¨å¹³å¸¸è¯­æ°”èŠå¿ƒé‡Œçš„äº‹ï¼Œä¸ç”¨ç»•å¼¯å­")
        if "å¤" in s: tips.append("ç­”åº”çš„äº‹å°½é‡æŒ‰æ—¶åšåˆ°ï¼Œè®©å¯¹æ–¹æœ‰åº•")
        if "ç¦»" in s: tips.append("æŠŠç•Œé™è¯´æ¸…æ¥šï¼Œè®©å¯¹æ–¹çŸ¥é“ä½ çš„æƒ³æ³•")
        if "éœ‡" in s or "ä¹¾" in s: tips.append("åœ¨é‡è¦æ—¶åˆ»ä¸»åŠ¨ä¸€ç‚¹")
        if "å" in s: tips.append("å°‘é çŒœï¼Œå¤šç¡®è®¤")
        if "è‰®" in s: tips.append("ç»™å½¼æ­¤ä¸€äº›ç‹¬å¤„æ—¶é—´")
    return tips[:3]

# ---- ä¸‰åˆ†è±¡åˆå¥ & ä¸“ä¸šæç¤º ----
def _combine_sentence(desc: str, interp: str) -> str:
    if not desc and not interp: return ""
    desc  = _neutralize(_depronoun((desc or "").strip().rstrip("ï¼›;ã€‚")))
    interp = _neutralize(_depronoun((interp or "").strip().lstrip("â€”â€”").lstrip("- ").strip().rstrip("ï¼›;ã€‚")))
    interp = re.sub(r"^(è¿™ç§|æ­¤ç±»|è¿™ç±»|å…¶|è¿™ç§å§¿æ€|è¿™ç§ç¥æƒ…|è¿™ç§é¢å®¹)[ï¼Œã€ï¼š ]*", "", interp)
    s = f"{desc}ï¼Œ{interp}" if (desc and interp) else (desc or interp)
    s = re.sub(r"[ï¼›;]+", "ï¼›", s)
    s = re.sub(r"^\s*[ï¼Œ,ã€‚ï¼›;ï¼š:]+", "", s)  # æ¸…ç†å¥é¦–å¤šä½™æ ‡ç‚¹
    s = re.sub(r"ï¼Œï¼Œ+", "ï¼Œ", s)
    return _dedupe_smart(s)

def _collect_traits_and_merge(ta: Dict[str,Any]) -> (List[str], Dict[str,Any]):
    traits = []
    new_ta = {}
    for key in ["å§¿æ€","ç¥æƒ…","é¢å®¹"]:
        o = (ta.get(key) or {}).copy()
        tend = (o.get("æ€§æ ¼å€¾å‘") or "").strip().rstrip("ï¼›;ã€‚")
        if tend: traits.append(tend)
        desc = (o.get("è¯´æ˜") or "")
        inter = (o.get("è§£è¯»") or "")
        merged = _combine_sentence(desc, inter)
        hexname = (o.get("å¦è±¡") or "").strip()
        # å¦åæœ«å°¾æ¸…æ´—ï¼šå»æ‰å¥å·/ç‚¹å·ä¸â€œå¦â€å­—
        hexname = re.sub(r"[ã€‚\.\s]*(å¦)?$", "", hexname)
        o["å¦è±¡"] = hexname
        # å¦åæœ«å°¾æ¸…æ´—ï¼šå»æ‰å¥å·/ç‚¹å·ä¸â€œå¦â€å­—ï¼ˆå¦‚â€œä¹¾ã€‚å¦â€â†’â€œä¹¾â€ï¼‰
        hexname = re.sub(r"[ã€‚\.\s]*(å¦)?$", "", hexname)
        pro = ""
        if hexname in HEX_SUMMARY:
            # è½»é‡ä¸“ä¸šæç¤ºï¼šå¦‚ã€ä¹¾Â·ä¸»å¯¼ã€‘
            kw = HEX_SUMMARY[hexname].split("Â·")[1] if "Â·" in HEX_SUMMARY[hexname] else HEX_SUMMARY[hexname]
            pro = f"ã€{hexname}Â·{kw}ã€‘"
        if pro and merged:
            merged = f"{pro} {merged}"
        o["è¯´æ˜"] = ""  # åˆå¹¶è¿›â€œè§£è¯»â€åæ¸…ç©ºï¼Œé¿å… UI é‡å¤
        o["è§£è¯»"] = merged.strip()
        o["æ€§æ ¼å€¾å‘"] = ""
        new_ta[key] = o
    for k in ta.keys():
        if k not in new_ta:
            new_ta[k] = ta[k]
    return traits, new_ta


def _to_points(s: str, max_items: int = 4) -> List[str]:
    """Split a sentence by Chinese semicolons/commas into 2-4 concise bullet points."""
    if not s: return []
    s = _neutralize(s)
    s = re.sub(r"[ï¼›;]+", "ï¼›", s.strip("ï¼›ã€‚ \n\t"))
    parts = [p.strip("ï¼›ï¼Œã€‚ \n\t") for p in s.split("ï¼›") if p.strip()]
    if len(parts) <= 1:
        parts = [p.strip("ï¼›ï¼Œã€‚ \n\t") for p in re.split(r"[ï¼Œ,]", s) if p.strip()]
    seen, uniq = set(), []
    for p in parts:
        if p in seen: continue
        seen.add(p); uniq.append(p)
        if len(uniq) >= max_items: break
    return uniq
def _merge_status_and_detail(status: str, detail: str) -> str:
    detail_first = detail.split("ã€‚")[0].strip() if detail else ""
    detail_first = _neutralize(_strip_domain_lead(detail_first))
    status = _neutralize(_strip_domain_lead(status or ""))
    parts = [p for p in [status, detail_first] if p]
    text = "ï¼›".join(parts).rstrip("ï¼›")
    return _dedupe_smart(text)

def _coerce_output(data: Dict[str,Any]) -> Dict[str,Any]:
    out = dict(data)
    meta = out.get("meta") or {}
    if not isinstance(meta, dict): meta = {}
    out["meta"] = meta

    ta = meta.get("triple_analysis") or {}
    traits, ta = _collect_traits_and_merge(ta)
    meta["triple_analysis"] = ta

    hexes = [(ta.get("å§¿æ€") or {}).get("å¦è±¡",""),
             (ta.get("ç¥æƒ…") or {}).get("å¦è±¡",""),
             (ta.get("é¢å®¹") or {}).get("å¦è±¡","")]
    combo_title = " + ".join([h for h in hexes if h])
    meta["combo_title"] = combo_title

    synthesized = _synthesize_combo(hexes, ta, traits)
    one = (ta.get("æ€»ç»“") or out.get("summary",""))
    overview = (synthesized or one).strip().rstrip("ï¼›;")
    meta["overview_card"] = {"title": f"ğŸ”® å¦è±¡ç»„åˆï¼š{combo_title}" if combo_title else "ğŸ”® å¦è±¡ç»„åˆ",
                             "summary": overview}

    try:
        out["confidence"] = float(out.get("confidence",0.0))
    except Exception:
        out["confidence"] = 0.0
    arch = (out.get("archetype") or "").strip()
    meta["headline"] = {"tag": arch, "confidence": out["confidence"]}

    dd = meta.get("domains_detail") or {}
    status = _insight_for_domains(hexes)
    merged_status = {
        "äº‹ä¸š": _merge_status_and_detail(status.get("äº‹ä¸š",""), dd.get("é‡‘é’±ä¸äº‹ä¸š","")),
        "æ„Ÿæƒ…": _merge_status_and_detail(status.get("æ„Ÿæƒ…",""), dd.get("é…å¶ä¸æ„Ÿæƒ…","")),
    }
    meta["domains_status"] = merged_status
    meta["domains_status_list"] = {k:_to_points(v) for k,v in merged_status.items()}
    meta["domains_suggestion"] = {
        "äº‹ä¸š": _imperative_suggestion(dd.get("é‡‘é’±ä¸äº‹ä¸š",""), hexes, "äº‹ä¸š"),
        "æ„Ÿæƒ…": _imperative_suggestion(dd.get("é…å¶ä¸æ„Ÿæƒ…",""), hexes, "æ„Ÿæƒ…")
    }
    meta["domains_suggestion_list"] = {
        "äº‹ä¸š": _imperative_suggestion_points(hexes, "äº‹ä¸š"),
        "æ„Ÿæƒ…": _imperative_suggestion_points(hexes, "æ„Ÿæƒ…")
    }

    def _clean(s):
        if not isinstance(s, str): return s
        s = s.replace("â€”â€”", "ï¼Œ")
        s = re.sub(r"[ï¼›;]+", "ï¼›", s)
        s = re.sub(r"ï¼›([ã€‚ï¼])", r"\1", s)
        s = re.sub(r"([ã€‚ï¼ï¼Ÿ])ï¼›", r"\1", s)
        s = _depronoun(s)
        s = _neutralize(s)
        return _dedupe_smart(s)

    out["summary"] = _clean(out.get("summary",""))
    out["archetype"] = _clean(out.get("archetype",""))

    def _deep_clean(x):
        if isinstance(x, dict):
            return {k:_deep_clean(v) for k,v in x.items()}
        if isinstance(x, list):
            return [_deep_clean(v) for v in x]
        return _clean(x)

    # face_parts å»é‡ï¼šè‹¥â€œè§£è¯»â€åŒ…å«â€œç‰¹å¾â€ï¼Œå»æ‰é‡å¤ï¼›ç»Ÿä¸€æ ‡ç‚¹
    fps = meta.get("face_parts") or {}
    if isinstance(fps, dict):
        for k, v in list(fps.items()):
            if not isinstance(v, dict): continue
            feat = (v.get("ç‰¹å¾") or "").strip().strip("ã€‚ï¼›;ï¼Œ, ")
            expl = (v.get("è§£è¯»") or "").strip()
            if feat and expl and feat in expl:
                import re as _re
                expl = _re.sub(_re.escape(feat)+r"[ï¼Œ,ï¼›;]?", "", expl)
            v["ç‰¹å¾"] = feat
            v["è§£è¯»"] = re.sub(r"[ï¼›;]+", "ï¼›", expl).strip("ï¼›ã€‚ ")
    meta["face_parts"] = fps

    out["meta"] = _deep_clean(meta)
    return out

@app.get("/health")
def health(): return {"status":"ok"}

@app.get("/", include_in_schema=False)
def root():
    return HTMLResponse("<h3>Selfy AI</h3><a href='/docs'>/docs</a> Â· <a href='/mobile'>/mobile</a>")

@app.head("/", include_in_schema=False)
def root_head():
    return Response(status_code=200)

@app.get("/version")
def version(): return {"runtime":RUNTIME_VERSION,"analysis":ANALYSIS_VERSION,"schema":SCHEMA_ID,"debug":DEBUG}

@app.get("/mobile", include_in_schema=False)
def mobile():
    path = os.path.join(os.path.dirname(__file__), "index_mobile.html")
    try:
        html = open(path, "r", encoding="utf-8").read()
    except Exception as e:
        return HTMLResponse(f"<pre>index_mobile.html not found: {e}</pre>", status_code=500)
    return HTMLResponse(html)

def _call_openai(messages):
    if client is None:
        raise RuntimeError("OpenAI client not initialized")
    return client.chat.completions.create(
        model="gpt-4o",
        temperature=0.4,
        tools=_build_tools_schema(),
        tool_choice={"type":"function","function":{"name":"submit_analysis_v3"}},
        response_format={"type":"json_object"},
        messages=messages
    )

def _call_gpt_tool_with_image(data_url: str) -> Dict[str,Any]:
    messages = _prompt_for_image_v372()
    messages[-1]["content"] = [
        {"type":"text","text":messages[-1]["content"]},
        {"type":"image_url","image_url":{"url":data_url}}
    ]
    resp = _call_openai(messages)
    choice = resp.choices[0]
    tool_calls = getattr(choice.message, "tool_calls", None)
    if tool_calls:
        args = json.loads(tool_calls[0].function.arguments)
    else:
        content = getattr(choice.message, "content", None)
        if isinstance(content, str) and content.strip().startswith("{"):
            args = json.loads(content)
        else:
            raise RuntimeError("Model did not return tool_calls.")
    return {"tool_args": args, "oai_raw": resp if DEBUG else None}

@app.post("/upload")
async def upload(file: UploadFile = File(...)):
    try:
        if not file: raise HTTPException(400,"No file")
        ct = file.content_type or ""
        if not ct.startswith("image/"): raise HTTPException(415,f"Unsupported content type: {ct}")
        raw = await file.read()
        if not raw: raise HTTPException(400,"Empty file")
        if len(raw) > 15*1024*1024: raise HTTPException(413,"File too large (>15MB)")

        data_url = _to_data_url(raw, ct)
        logger.info("[UPLOAD] %s %dB %s", file.filename, len(raw), ct)

        result = _call_gpt_tool_with_image(data_url)
        tool_args = result["tool_args"]
        final_out = _coerce_output(tool_args)

        if DEBUG:
            meta = final_out.setdefault("meta",{}).setdefault("debug",{})
            meta["file_info"]={"filename":file.filename,"content_type":ct,"size":len(raw)}
            try:
                meta["oai_choice_finish_reason"]=result["oai_raw"].choices[0].finish_reason
            except Exception:
                meta["oai_choice_finish_reason"]="n/a"

        return JSONResponse(content=final_out, status_code=200)
    except HTTPException as he:
        if DEBUG:
            return JSONResponse(status_code=he.status_code, content={"error":he.detail,"debug":{"trace":traceback.format_exc()}})
        raise
    except Exception as e:
        logging.exception("upload failed: %s", e)
        body={"error":"Internal Server Error"}
        if DEBUG: body["debug"]={"message":str(e),"trace":traceback.format_exc()}
        return JSONResponse(status_code=500, content=body)
